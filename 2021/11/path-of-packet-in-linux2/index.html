<!DOCTYPE html>
<html lang="zh-tw">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="AEpTGuQAkxnTOlmfF4INDna3S660LxrkyZ4BQzVbRSw">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"littlebees.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":true,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="動機 算是packet在linux中走過的路的延伸">
<meta property="og:type" content="article">
<meta property="og:title" content="packet在linux中走過的路2">
<meta property="og:url" content="https://littlebees.github.io/2021/11/path-of-packet-in-linux2/index.html">
<meta property="og:site_name" content="記事本">
<meta property="og:description" content="動機 算是packet在linux中走過的路的延伸">
<meta property="og:locale" content="zh_TW">
<meta property="article:published_time" content="2021-11-04T15:38:50.000Z">
<meta property="article:modified_time" content="2022-03-29T15:59:01.296Z">
<meta property="article:author" content="zhengcf">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://littlebees.github.io/2021/11/path-of-packet-in-linux2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-tw'
  };
</script>

  <title>packet在linux中走過的路2 | 記事本</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">記事本</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">寫下來，不然會忘</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-tw">
    <link itemprop="mainEntityOfPage" href="https://littlebees.github.io/2021/11/path-of-packet-in-linux2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="zhengcf">
      <meta itemprop="description" content="想到什麼就寫什麼">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="記事本">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          packet在linux中走過的路2
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-04 23:38:50" itemprop="dateCreated datePublished" datetime="2021-11-04T23:38:50+08:00">2021-11-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-03-29 23:59:01" itemprop="dateModified" datetime="2022-03-29T23:59:01+08:00">2022-03-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Network/" itemprop="url" rel="index"><span itemprop="name">Network</span></a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Network/FAQ/" itemprop="url" rel="index"><span itemprop="name">FAQ</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="動機-619">動機</h2>
<p>算是<a href="/2021/09/path-of-packet-in-linux/">packet在linux中走過的路</a>的延伸</p>
<span id="more"></span>
<h2 id="in">in</h2>
<h3 id="device-init">device init</h3>
<p>module_init =&gt;<br>
當設備驅動編譯時，MODULE_DEVICE_TABLE會導出一個 PCI 設備 ID 列表，驅動據此識別它可以控制的設備，內核也會依據這個列表對不同設備加載相應驅動。</p>
<p>通過 PCI ID 識別設備後，內核就會為它選擇合適的驅動。<br>
probe()</p>
<ol>
<li>啟用 PCI 設備</li>
<li>請求（requesting）內存範圍和 IO 端口</li>
<li>設置 DMA 掩碼</li>
<li>註冊設備驅動支持的 ethtool 方法（後面介紹）</li>
<li>註冊所需的 watchdog（例如，e1000e 有一個檢測設備是否freeze的 watchdog）</li>
<li>其他和具體設備相關的事情，例如一些 workaround，或者特定硬件的非常規處理</li>
<li>創建、初始化和註冊一個 struct net_device_ops 類型變量，這個變量包含了用於設 備相關的回調函數，例如打開設備、發送數據到網絡、設置 MAC 地址等</li>
<li>創建、初始化和註冊一個更高層的 struct net_device 類型變量（一個變量就代表了 一個設備）</li>
</ol>
<p>驅動的硬中斷處理函數做的事情很少，但軟中斷將會在和硬中斷相同的 CPU 上執行。<br>
這就是為什麼給每個 CPU 一個特定的硬中斷非常重要：這個 CPU 不僅處理這個硬中斷，而且通過 NAPI 處理接下來的軟中斷來收包。</p>
<p>可以設定cpu affinity，讓localty上升，Set the IRQ affinity for IRQ 8 to CPU 0<br>
<code>echo 1 &gt; /proc/irq/8/smp_affinity</code></p>
<h3 id="device-bring-up">device bring up</h3>
<p>igb_open =&gt;</p>
<ul>
<li>分配 RX、TX 隊列內存
<ul>
<li>有些網卡支援Receive Side Scaling（RSS，接收端擴展）或者多隊列（ multiqueue）</li>
</ul>
</li>
<li>打開 NAPI 功能 (一次收多一點)
<ul>
<li>在一個單獨的線程裡，調用驅動註冊的 poll 方法收包</li>
<li>驅動禁止網卡產生新的硬件中斷。這樣做是為了 NAPI 能夠在收包的時候不會被新的中 斷打擾</li>
<li>一旦沒有包需要收了(或是收滿了,weight滿了)，NAPI 關閉，網卡的硬中斷重新開啟</li>
</ul>
</li>
<li>註冊中斷處理函數
<ul>
<li><code>/proc/softirqs</code></li>
<li>MSI-X
<ul>
<li>因為每個 RX 隊列有獨 立的MSI-X 中斷，因此可以被不同的 CPU 處理（通過 irqbalance 方式，或者修改 /proc/irq/IRQ_NUMBER/smp_affinity）</li>
</ul>
</li>
<li>MSI</li>
<li>legacy IRQ</li>
</ul>
</li>
<li>打開（enable）硬中斷
<ul>
<li>“Interrupt Throttling”（也叫 “Interrupt Coalescing”）的硬件 特性相關，這個特性可以平滑傳送到 CPU 的中斷數量</li>
<li>Interrupt coalescing 中斷合併會將多個中斷事件放到一起，累積到一定閾值後才向 CPU 發起中斷請求。
<ul>
<li>這可以防止中斷風暴，提升吞吐，降低 CPU 使用量，但延遲也變大；中斷數量過多則相反。</li>
<li><code>ethtool -C eth0 adaptive-rx on</code>: 自適應 RX IRQ 合併</li>
</ul>
</li>
<li><code>cat /proc/interrupts</code></li>
</ul>
</li>
<li>網絡設備子系統的初始化 (net_dev_init)
<ul>
<li>struct softnet_data 變量初始化
<ul>
<li>需要註冊到這個 CPU 的 NAPI 變量列表</li>
<li>數據處理 backlog</li>
<li>處理權重</li>
<li>receive offload 變量列表</li>
<li>receive packet steering 設置</li>
</ul>
</li>
<li>註冊SoftIRQ
<ul>
<li>NET_TX_SOFTIRQ =&gt; net_tx_action</li>
<li>NET_RX_SOFTIRQ =&gt; net_rx_action</li>
</ul>
</li>
</ul>
</li>
<li>monitor
<ul>
<li><code>ethtool -S</code></li>
<li><code>cat /sys/class/net/&lt;NIC&gt;/statistics/&lt;item&gt;</code></li>
<li><code>/proc/net/dev</code></li>
<li>所有顯示的項目都是由driver定義的，所以不同driver同樣item的意思可能不同</li>
</ul>
</li>
<li>tuning
<ul>
<li>調整queue
<ul>
<li>RX queue的數量</li>
<li>RX queue的大小</li>
<li>RX queue的權重</li>
<li>RX queue的hash (讓pkt到不同的queue)</li>
</ul>
</li>
<li>ntuple filtering
<ul>
<li>在NIC上做過濾，steer到指定的queue</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="inet開始收資料">inet開始收資料</h3>
<p>先irq(igb_msix_ring)去要求跑sortirq，之後會帶到對應的softirq<br>
<code>net_rx_action</code> 從包所在的內存開始處理，包是被設備通過 DMA 直接送到內存。</p>
<p>softirq透過NAPI(igb_poll)去讀，讀完就會解綁（unmap）這些內存，讀取數據，將數據送到napi_gro_receive，之後到下一層(tap或是ip，如果中間有RPS，會先過RPS)。</p>
<p>有趣的是這裡的NAPI的weight寫死成64</p>
<p>monitor:<br>
<code>/proc/net/softnet_stat</code><br>
沒有title，這要直接看kernel code，<code> net/core/net-procfs.c</code><br>
每一行代表一個 struct softnet_data 變量。因為每個 CPU 只有一個該變量，所以每行 其實代表一個 CPU<br>
每列用空格隔開，數值用 16 進製表示<br>
第一列 sd-&gt;processed，是處理的網絡幀的數量。如果你使用了 ethernet bonding， 那這個值會大於總的網絡幀的數量，因為 ethernet bonding 驅動有時會觸發網絡數據被 重新處理（re-processed）<br>
第二列，sd-&gt;dropped，是因為處理不過來而 drop 的網絡幀數量。後面會展開這一話題<br>
第三列，sd-&gt;time_squeeze，前面介紹過了，由於 budget 或 time limit 用完而退出 net_rx_action 循環的次數<br>
接下來的 5 列全是 0<br>
第九列，sd-&gt;cpu_collision，是為了發送包而獲取鎖的時候有衝突的次數<br>
第十列，sd-&gt;received_rps，是這個 CPU 被其他 CPU 喚醒去收包的次數<br>
最後一列，flow_limit_count，是達到 flow limit 的次數。 flow limit 是 RPS 的特性， 後面會稍微介紹一下<br>
tune: 調整 net_rx_action budget<br>
* <code>net.core.netdev_budget=600</code></p>
<h4 id="GRO（Generic-Receive-Offloading）">GRO（Generic Receive Offloading）</h4>
<p>Large Receive Offloading (LRO) 是一個硬件優化，GRO 是 LRO 的一種軟件實現。</p>
<p>如果用 tcpdump 抓包，有時會看到機器收到了看起來不現實的、非常大的包， 這很可能是你的系統開啟了 GRO。</p>
<p>tcpdump 的抓包點（捕獲包的 tap ）在整個棧的更後面一些，在GRO 之後。<br>
NIC(與linux的level2)到IP之間</p>
<p>dev_gro_receive 完成，napi_skb_finish 就會被調用，之後就是走<br>
<code>netif_receive_skb</code> 或者繼續將包送到協議棧，或者交給 RPS，後者會轉交給其他 CPU 處理。</p>
<h4 id="RPS-Receive-Packet-Steering">RPS (Receive Packet Steering)</h4>
<p>RPS （Receive Packet Steering，接收包控制，接收包引導）是 RSS 的一種軟件實現</p>
<p>因為它是軟件實現的，這意味著 RPS 只能在 packet 通過 DMA 進入內存後，RPS 才能開始工 作。</p>
<p>RPS 並不會減少 CPU 處理硬件中斷和 NAPI poll（軟中斷最重要的一部分）的時 間，但是可以在 packet 到達內存後，將 packet 分到其他 CPU，從其他 CPU 進入協議棧。</p>
<p>RPS 的工作原理是對個 packet 做 hash，以此決定分到哪個 CPU 處理。然後 packet 放到每個 CPU 獨占的接收後備隊列（backlog）等待處理。這個 CPU 會觸發一個進程間中斷（ IPI，Inter-processor Interrupt）向對端 CPU。如果當時對端 CPU 沒有在處理 backlog 隊列收包，這個進程間中斷會 觸發它開始從 backlog 收包。 /proc/net/softnet_stat 其中有一列是記錄 softnet_data 變量（也即這個 CPU）收到了多少 IPI（received_rps 列）。</p>
<p>tune: <code>/sys/class/net/DEVICE_NAME/queues/QUEUE/rps_cpus</code></p>
<h5 id="RFS-Receive-Flow-Steering">RFS (Receive Flow Steering)</h5>
<p>RFS 将属于相同 flow 的包送到相同的 CPU 进行处理，可以提高缓存命中率。</p>
<p><code>echo 2048 &gt; /sys/class/net/eth0/queues/rx-0/rps_flow_cnt</code><br>
<code>sysctl -w net.core.rps_sock_flow_entries=32768</code></p>
<h6 id="aRFS-Hardware-accelerated-RFS">aRFS (Hardware accelerated RFS)</h6>
<p>RFS 可以用硬件加速，網卡和內核協同工作，判斷哪個 flow 應該在哪個 CPU 上處理。<br>
這個要開RFS與kernel要編CONFIG_RFS_ACCEL，ntuple也要開，最後配置 IRQ（硬中斷）中每個 RX 和 CPU 的對應關係</p>
<h3 id="到linux的level2-协议栈">到linux的level2 (协议栈)</h3>
<p>從netif_receive_skb過來<br>
先當成有RPS，pkt就會到cpu的backlog(enqueue_to_backlog)，之後NAPI的poller會去拉(process_backlog)，可以把cpu當成另一種網卡<br>
另外有flow limit，避免cpu被打爆<br>
最後做gro(napi_gro_complete)</p>
<p>等到了cpu的backlog queue，NAPI poller就會去拉pkt<br>
backlog NAPI 變量和設備驅動 NAPI 變量的不同之處在於，它的 weight 是可以調節的，而設備 驅動是 hardcode 64。</p>
<p>monitor: 由於 input_pkt_queue 打滿或 flow limit 導致的丟包<br>
* /proc/net/softnet_stat 裡面的 dropped<br>
tune:<br>
* RX packet timestamping<br>
* net.core.netdev_tstamp_prequeue=0<br>
* 調cpu backlog:<br>
* net.core.netdev_max_backlog=3000<br>
* 讓NAPI能一次拿更多pkt(wieght)<br>
* net.core.dev_weight=600<br>
* 調flow limit的table size<br>
* net.core.flow_limit_table_len=8192<br>
* 打開flow limit<br>
* /proc/sys/net/core/flow_limit_cpu_bitmap</p>
<p>最後的最後，__netif_receive_skb_core把pkt送到抓包點（tap）或協議層</p>
<h3 id="到目前為止的流程">到目前為止的流程</h3>
<p>NIC -&gt;(DMA,irq) driver -&gt;(NAPI,sortirq) gro -&gt; RPS -&gt;(NAPI) tap/IP</p>
<h3 id="到IP">到IP</h3>
<p>ip_rcv之後就是netfilter(NF_HOOK)的prerouting，ip_rcv_finish(early_demux)<br>
就可以往上走了!!</p>
<p>early_demux<br>
如果這個優化打開了，但是並沒有命中緩存（例如，這是第一個包），這個包就會被送到內 核的路由子系統，在那裡將會計算出一個 dst_entry 並賦給相應的字段<br>
所以應該能猜，沒開就是每次都去查?</p>
<p>tune: 把early_demux關了(也許需要)</p>
<ul>
<li>net.ipv4.ip_early_demux=0</li>
</ul>
<p>如果是自己的pkt?<br>
ip_local_deliver處理，過netfilter，到ip_local_deliver_finish</p>
<p>monitor:</p>
<ul>
<li><code>/proc/net/snmp</code></li>
</ul>
<p>InReceives: ip_rcv收到多少pkt<br>
InHdrErrors: 多少ip header壞了<br>
InAddrErrors: 多少pkt的addr是到不了的<br>
ForwDatagrams: forwarded的ip pkt<br>
InUnknownProtos: 多少是protocol不明的<br>
InDiscards: 多少pkt被丟了(也許是mem alloc失敗，也許是checksum error)<br>
InDelivers: 多少pkt成功往上送<br>
InCsumErrors: 多少pkt是checksum error</p>
<h3 id="到udp">到udp</h3>
<p>udp_rcv</p>
<ul>
<li>這裡會看pkt與他的dst_entry(routing的結果)，送到socket的backlog</li>
</ul>
<p>tune:</p>
<ul>
<li>Socket receive queue memory
<ul>
<li>net.core.rmem_max=8388608</li>
<li>net.core.rmem_default=8388608</li>
<li>或是用setsockopt帶<br>
monitor:</li>
</ul>
</li>
<li>/proc/net/snmp</li>
<li>/proc/net/udp</li>
</ul>
<p>/proc/net/snmp<br>
InDatagrams: 總共有多少udp pkt進入或流出<br>
NoPorts: 總共有多少udp pkt的dport是沒有人在聽的<br>
InErrors: 多少udp pkt有錯誤(也許是mem alloc失敗，也許是checksum error)<br>
OutDatagrams: 多少pkt成功往下送<br>
RcvbufErrors: 有多少pkt是因為rcv buffer爆了而塞不進去<br>
SndbufErrors: 有多少pkt是因為send buffer爆了而塞不進去<br>
InCsumErrors: 多少pkt是checksum error</p>
<p>/proc/net/udp<br>
sl: Kernel hash slot for the socket<br>
local_address: local addr<br>
rem_address: remote addr<br>
st: socket的狀態<br>
tx_queue: tx queue的大小<br>
rx_queue: rx queue的大小<br>
tr, tm-&gt;when, retrnsmt: 這些應該retry與retransmit<br>
uid: 創這個socket的uid<br>
timeout: 應該是tcp的timeout<br>
inode: socket的inode<br>
ref: reference count<br>
pointer: socket的address<br>
drops: 這個socket的drop</p>
<h3 id="socket">socket</h3>
<p>sock_queue_rcv收pkt，sk_data_ready通知socket</p>
<h2 id="out">out</h2>
<h2 id="socket-2">socket</h2>
<p>sentto，會走到sock_sendmsg，之後看socket是AF_INET，走道inet_sendmsg<br>
之後就是走到udp_sendmsg</p>
<h2 id="udp">udp</h2>
<p>拿dip,dport，處理unicast或mutilcast，之後就是routing??，生skb(ip_make_skb)，丟給udp_send_skb</p>
<p>怎麼感覺有偷跑的感覺在??</p>
<h2 id="ip">ip</h2>
<p>ip_send_skb收到pkt，ip_local_out，netfilter(local_out)，routing(拿dst_entry)，ip_output，netfilter(post_routing)，ip_finish_output</p>
<p>ip_finish_output，通常是</p>
<ol>
<li>ip_fragment，之後ip_finish_output2</li>
<li>直接ip_finish_output2</li>
</ol>
<p>ip_finish_output2會去調arp(鄰居系統)</p>
<h3 id="Path-MTU-Discovery">Path MTU Discovery</h3>
<p>此功能允許內核自動確定 路由的最大傳輸單元（ MTU ）。</p>
<p>調用 setsockopt 帶 SOL_IP 和 IP_MTU_DISCOVER</p>
<h2 id="linux-level2-TC">linux level2 (TC)</h2>
<p>dev_queue_xmit，之後就是找TX queue的旅程(netdev_pick_tx)，拿到TX queue找對應的qdisc，之後到dev_hard_start_xmit，會去跑qdisc，再到sch_direct_xmit-&gt;dev_hard_start_xmit</p>
<p>任何無法發送的 skb 都重新入隊，將在 NET_TX softirq 中進行 發送。<br>
__netif_schedule發tx的irq</p>
<ol>
<li>sortirq: net_tx_action
<ul>
<li>completion queue: 待釋放 skb 隊列</li>
<li>output queue: 待發送 skb 隊列
<ul>
<li>之後到dev_hard_start_xmit
<ul>
<li>發送數據花費的總時間是下面二者之和
<ul>
<li>系統調用的系統時間(sys time)</li>
<li>NET_TX 類型的 softirq 時間（softirq time）</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>dev_hard_start_xmit
<ul>
<li>dev_queue_xmit_nit: copy skb到tap(pcap)</li>
<li>ops-&gt;ndo_start_xmit: 到device去</li>
</ul>
</li>
</ol>
<p>monitor: <code>tc -s qdisc show dev eth1</code><br>
bytes: driver總共傳了多少bytes<br>
pkt: driver總共傳了多少pkt<br>
dropped: qdisc drop多少pkt<br>
overlimits: 有多少pkt是因為這個qdisc的策略而被去掉的(queue爆了或是有pkt出queue時去清)<br>
requeues: 有多少pkt是重新入queue(像是driver丟不出去就會重新入queue)<br>
backlog: 現在queue多長</p>
<p>你可以調整前面看到的__qdisc_run 循環的權重（上面看到的 quota 變量），這將導致 __netif_schedule 更多的被調用執行。結果將是當前 qdisc 將被更多的添加到當前 CPU 的 output_queue，最終會使發包所佔的時間變多。</p>
<p>tune:</p>
<ul>
<li>調整__qdisc_run 處理權重
<ul>
<li><code>net.core.dev_weight=600</code></li>
</ul>
</li>
<li>增加發送隊列長度
<ul>
<li>ifconfig eth0 txqueuelen 10000</li>
</ul>
</li>
</ul>
<h3 id="Transmit-Packet-Steering-XPS">Transmit Packet Steering (XPS)</h3>
<p>發送數據包控制（XPS）是一項功能，允許系統管理員配置哪些 CPU 可以處理網卡的哪些發送 隊列。 XPS 的主要目的是避免處理髮送請求時的鎖競爭。使用 XPS 還可以減少緩存驅逐， 避免NUMA機器上的遠程 內存訪問等。</p>
<p>tune:<br>
<code>/sys/class/net/DEVICE_NAME/queues/QUEUE/xps_cpus</code></p>
<h2 id="NIC">NIC</h2>
<p>到了driver的igb_xmit_frame_ring，再到igb_tx_map將 skb 數據映射到 RAM 的 DMA 區域</p>
<p>完成傳送之後device送完pkt就會丟irq</p>
<p>對於 igb 驅動程序（及其關聯設備），發送完成和數據包接收所觸發的 IRQ 是相同的。這意味著 對於 igb 驅動程序，NET_RX 既用於處理髮送完成，又用於處理數據包接收。<br>
如果是這種情況，則 NET_RX softirq 會被用於 處理數據包接收和發送完成兩種情況。</p>
<p>在NAPI做poll(igb_poll)時會去清已完成的pkt(igb_clean_tx_irq)</p>
<h3 id="Dynamic-Queue-Limits-DQL">Dynamic Queue Limits (DQL)</h3>
<p>DQL 內部算法判斷何時數據已足夠多，達到此閾值後，DQL 將暫時禁用 TX Queue，從而對網絡系統產生背壓。當足夠的數據已發送完後，DQL 再自動重新啟用該隊列。</p>
<p>monitor:<br>
<code>/sys/class/net/&lt;NIC&gt;/queues/tx-&lt;QUEUE_ID&gt;/byte_queue_limits/*</code><br>
hold_time: 類似tcp的timeout，如果queue一直滿到一定時間(以HZ為單位)就去縮小queue的最大長度<br>
inflight: number of packets queued - number of packets completed<br>
limit_max: (寫死) DQL_MAX_LIMIT (1879048192 on my x86_64 system)<br>
limit_min: (寫死) 0<br>
limit: 介於limit_min與limit_max之間，代表queue的最大長度</p>
<h2 id="Ref-125">Ref</h2>
<p><span class="exturl" data-url="aHR0cDovL2FydGh1cmNoaWFvLmFydC9ibG9nL3R1bmluZy1zdGFjay1yeC16aA==">[译] Linux 网络栈监控和调优：接收数据（2016）<i class="fa fa-external-link-alt"></i></span><br>
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLnBhY2thZ2VjbG91ZC5pby9lbmcvMjAxNi8xMC8xMS9tb25pdG9yaW5nLXR1bmluZy1saW51eC1uZXR3b3JraW5nLXN0YWNrLXJlY2VpdmluZy1kYXRhLWlsbHVzdHJhdGVkLw==">Illustrated Guide to Monitoring and Tuning the Linux Networking Stack: Receiving Data<i class="fa fa-external-link-alt"></i></span><br>
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLnBhY2thZ2VjbG91ZC5pby9lbmcvMjAxNi8wNi8yMi9tb25pdG9yaW5nLXR1bmluZy1saW51eC1uZXR3b3JraW5nLXN0YWNrLXJlY2VpdmluZy1kYXRhLyNzb2Z0aXJxcw==">Monitoring and Tuning the Linux Networking Stack: Receiving Data<i class="fa fa-external-link-alt"></i></span><br>
<span class="exturl" data-url="aHR0cDovL2FydGh1cmNoaWFvLmFydC9ibG9nL3R1bmluZy1zdGFjay10eC16aC8=">[译] Linux 网络栈监控和调优：发送数据（2017）<i class="fa fa-external-link-alt"></i></span><br>
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLnBhY2thZ2VjbG91ZC5pby9lbmcvMjAxNy8wMi8wNi9tb25pdG9yaW5nLXR1bmluZy1saW51eC1uZXR3b3JraW5nLXN0YWNrLXNlbmRpbmctZGF0YS8jZ2VuZXJhbC1hZHZpY2Utb24tbW9uaXRvcmluZy1hbmQtdHVuaW5nLXRoZS1saW51eC1uZXR3b3JraW5nLXN0YWNr">Monitoring and Tuning the Linux Networking Stack: Sending Data<i class="fa fa-external-link-alt"></i></span><br>
<span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXIuYWxpeXVuLmNvbS9hcnRpY2xlLzExMDE4P3NwbT1hMmM2aC4xMzI2MjE4NS4wLjAuNjE1ZTFmYTRLSjlOTjI=">linux网络实现分析(1)——数据包的接收（从网卡到协议栈）<i class="fa fa-external-link-alt"></i></span><br>
<span class="exturl" data-url="aHR0cHM6Ly9kZXZlbG9wZXIuYWxpeXVuLmNvbS9hcnRpY2xlLzI2MjUzP3NwbT1hMmM2aC4xMzI2MjE4NS4wLjAuNjE1ZTFmYTRLSjlOTjI=">linux网络实现分析(3)——数据包的发送（IP层到链路层）<i class="fa fa-external-link-alt"></i></span><br>
<span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmxvdWllLmx1LzIwMTkvMDMvMDYvdGhlLXBhY2tldC1mbG93LWZyb20tdXNlcnNwYWNlLXRvLWtlcm5lbC1kcml2ZXItaW4tbGludXgtbmV0d29yay1zdGFjay8=">The packet flow, from userspace to kernel driver in Linux network stack<i class="fa fa-external-link-alt"></i></span></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/11/conntrack/" rel="prev" title="netfilter的conntrack">
      <i class="fa fa-chevron-left"></i> netfilter的conntrack
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/11/Disappearing-Pairs/" rel="next" title="Disappearing-Pairs">
      Disappearing-Pairs <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8B%95%E6%A9%9F-619"><span class="nav-number">1.</span> <span class="nav-text">動機</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#in"><span class="nav-number">2.</span> <span class="nav-text">in</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#device-init"><span class="nav-number">2.1.</span> <span class="nav-text">device init</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#device-bring-up"><span class="nav-number">2.2.</span> <span class="nav-text">device bring up</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#inet%E9%96%8B%E5%A7%8B%E6%94%B6%E8%B3%87%E6%96%99"><span class="nav-number">2.3.</span> <span class="nav-text">inet開始收資料</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#GRO%EF%BC%88Generic-Receive-Offloading%EF%BC%89"><span class="nav-number">2.3.1.</span> <span class="nav-text">GRO（Generic Receive Offloading）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RPS-Receive-Packet-Steering"><span class="nav-number">2.3.2.</span> <span class="nav-text">RPS (Receive Packet Steering)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#RFS-Receive-Flow-Steering"><span class="nav-number">2.3.2.1.</span> <span class="nav-text">RFS (Receive Flow Steering)</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#aRFS-Hardware-accelerated-RFS"><span class="nav-number">2.3.2.1.1.</span> <span class="nav-text">aRFS (Hardware accelerated RFS)</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%B0linux%E7%9A%84level2-%E5%8D%8F%E8%AE%AE%E6%A0%88"><span class="nav-number">2.4.</span> <span class="nav-text">到linux的level2 (协议栈)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%B0%E7%9B%AE%E5%89%8D%E7%82%BA%E6%AD%A2%E7%9A%84%E6%B5%81%E7%A8%8B"><span class="nav-number">2.5.</span> <span class="nav-text">到目前為止的流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%B0IP"><span class="nav-number">2.6.</span> <span class="nav-text">到IP</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%B0udp"><span class="nav-number">2.7.</span> <span class="nav-text">到udp</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#socket"><span class="nav-number">2.8.</span> <span class="nav-text">socket</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#out"><span class="nav-number">3.</span> <span class="nav-text">out</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#socket-2"><span class="nav-number">4.</span> <span class="nav-text">socket</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#udp"><span class="nav-number">5.</span> <span class="nav-text">udp</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ip"><span class="nav-number">6.</span> <span class="nav-text">ip</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Path-MTU-Discovery"><span class="nav-number">6.1.</span> <span class="nav-text">Path MTU Discovery</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#linux-level2-TC"><span class="nav-number">7.</span> <span class="nav-text">linux level2 (TC)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Transmit-Packet-Steering-XPS"><span class="nav-number">7.1.</span> <span class="nav-text">Transmit Packet Steering (XPS)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NIC"><span class="nav-number">8.</span> <span class="nav-text">NIC</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Dynamic-Queue-Limits-DQL"><span class="nav-number">8.1.</span> <span class="nav-text">Dynamic Queue Limits (DQL)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ref-125"><span class="nav-number">9.</span> <span class="nav-text">Ref</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">zhengcf</p>
  <div class="site-description" itemprop="description">想到什麼就寫什麼</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">690</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">79</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">64</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">zhengcf</span>
</div>
  <div class="powered-by">Powered by <span class="exturl theme-link" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & <span class="exturl theme-link" data-url="aHR0cHM6Ly9tdXNlLnRoZW1lLW5leHQub3Jn">NexT.Muse</span>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
  <script src="//cdn.jsdelivr.net/npm/quicklink@1/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink({
        timeout : 1000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'https://littlebees.github.io/2021/11/path-of-packet-in-linux2/',]
      });
      });
  </script>

</body>
</html>
