<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Parallel Thinking | 記事本</title>
<meta name=keywords content="Parallel"><meta name=description content="動機
平行的思考"><meta name=author content="zhengcf"><link rel=canonical href=https://littlebees.github.io/2022/01/parallel-thinking/><link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=https://littlebees.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://littlebees.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://littlebees.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://littlebees.github.io/apple-touch-icon.png><link rel=mask-icon href=https://littlebees.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://littlebees.github.io/2022/01/parallel-thinking/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Parallel Thinking"><meta property="og:description" content="動機
平行的思考"><meta property="og:type" content="article"><meta property="og:url" content="https://littlebees.github.io/2022/01/parallel-thinking/"><meta property="og:image" content="https://littlebees.github.io/images/papermod-cover.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-01-17T14:27:20+00:00"><meta property="article:modified_time" content="2022-01-17T14:27:20+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://littlebees.github.io/images/papermod-cover.png"><meta name=twitter:title content="Parallel Thinking"><meta name=twitter:description content="動機
平行的思考"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://littlebees.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Parallel Thinking","item":"https://littlebees.github.io/2022/01/parallel-thinking/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Parallel Thinking","name":"Parallel Thinking","description":"動機 平行的思考\n","keywords":["Parallel"],"articleBody":"動機 平行的思考\nConcurrent computing \u0026 Parallel computing \u0026 Distributed computing 其實就共用資源、與各個計算單元之間要溝通這件事而言，他們其實差不多\n但如果還是要分的話 Parallelism: 多個core同時跑 Concurrency: 因為同時/交互執行/節點失敗/節點不可靠所造成的效果(non-determinacy)\nParallel computing \u0026 Distributed computing 其實就共用資源、與各個計算單元之間要溝通這件事而言，他們其實差不多\n但Distributed computing更重視HA的部分\n平行化就是在 =\u003e Coordination Communication: 傳送結果到其他點 Load Balancing: 工作量不能差太大 Synchronization: 要等其他點到一定程度 用一般演算法的話來說就是D\u0026Q，所以怎麼divide?\nTask-parallelism \u0026 Data-parallelism Task-parallelism: 各種任務(像新年大掃除有掃廁所、清廚房…)，不同core做 Data-parallelism: 不同core做類似的任務，像多段下載\n目前沒有一個好的方式把serial prog轉成Parallel prog 所以要自己寫!!\n因為task吃題目，所以這裡只看Data-parallelism\n2 parallel models (+ 1 model) model的要素\nControl 如何達成平行化 operation之間的怎麼協調執行順序 Data 那些資料是 private 是 shared 怎麼存取shraed data Synchronization 有哪些operation 那些是atomic Cost 有什麼成本 Shared Memory Control 如何達成平行化 thread當成抽象的processor 可以自由地創thread operation之間的怎麼協調執行順序 讀寫shared memory 透過Synchronization做協調(coordinate) Data 那些資料是 private 是 shared private: 在thread內的 shared: 在thread外的 怎麼存取shraed data 直接讀寫mem Synchronization 有哪些operation Synchronization semaphore mutex busy-waiting 協調 condition variable 那些是atomic 就是atomic與thread操作 Cost 有什麼成本 創thread context switch Race Condition 每次跑，結果可能會不同\n優缺點 優點 溝通方式簡單 mem讀寫 缺點 需要Synchronization機制 不好scale 處理cache很頭痛 (cache Coherence) Message passing Control 如何達成平行化 有許多獨立的processor operation之間的怎麼協調執行順序 send/receive pairs 在溝通時剛好完成協調 Data 那些資料是 private 是 shared private: processor內的資料 怎麼存取shraed data 沒有shared data，要透過send/receive pairs Synchronization 有哪些operation 不用 那些是atomic 不用 Cost 有什麼成本 溝通時的delay 溝通的方式: 網路, mem 怎麼接收: poll, interrupt 優缺點 優點 明確溝通 不用特別為cache頭痛 缺點 溝通的成本很高 不好寫 (??) Shared Memory \u0026 Message passing communication Turing complete 彼此可以互相實作對方的模型 除了divide，還可以一次處理多一點: Data Parallel 像是array不是一次處理一個，或是讓thread分別處理每個段\n現在是直接處理整個array!!\nParallel Algo的手法 把任務切出來，讓其他人去做 (Divide-and-conquer) 讓一次處理的資料變多 (對array之類的collection操作) Contraction (縮點、上rank) Randomization Identifying independent sets Pointer jumping parallel primitives map reduce: reduce, min, max, sum, … scan: prefix sum filter 其實是由 map, scan, map構成 map給的predicate scan做prefix sum (counting sort) 利用 第1步與第2步的array去生array 從FP來的，在mapreduce後廣為人知\nParallel Algo的BigO work: sequential 跑的時間複雜度 (只有一顆core) span: 如果有很多core跑時，最長的時間複雜度 (有很多顆core) 怎麼樣叫Parallel，怎麼樣叫Sequential fully Sequential: for (i : range(A)) A[i] = f(A[i-1]) 這個span是A fully Parallel: for (i : range(A)) A[i] = f(A[i]) 這個span是1 partially Parallel: for (i : range(A)) A[i] = f(A[i-sqrt(len(A))]) 這個span是sqrt(A) speed-up: work/span 所以實際的時間複雜度 (Tp)，一定介於兩者之間 work \u003c= Tp \u003c= span\n之後有個神奇公式，可以求漸近的Tp，P是核心數， Tp = O(work/P + span)\n這邊可以配合後面的 測量指標(尤其是Amdahl’s Law) 一起看\n例子: findMin on Tree def findMin(root): if not root: return float('-inf') else: return min(root.data, findMin(root.left), findMin(root.right)) work:\nif n = 1 O(1) else O(left) + O(right) + O(1) span:\nif n = 1 O(1) else max(O(left) + O(right)) + O(1) 所以需要知道實際的input，才知道實際的複雜度\nDegenerate Tree(就是list) work: O(n) span: O(n) Perfect Tree(平衡樹) work: O(n) span: O(log n) 測量指標 speedup: serial執行的時間對上Parallel執行的時間 的比率 線性speedup speedup 剛好等於 核心數 也就是隨著核心數上去速度就會上去 efficiency: speedup的比率除上核心數，也就是每個核心促進了多少進步 scalability: 資料量上升，核心數上升就能讓efficiency不變 Strongly scalable 核心數上升，資料量不變，efficiency不變 Weakly scalable 核心數上升，資料量上升，efficiency不變 隨著核心數上升，performance可以提升到多少? performance有兩個面向，latency與throughput\ns是serial的比例(就是1-p)，p是parallel的比例(就是1-s)，n是核心數\n兩個面向關注的點不同，\n一個是關注能減少多少時間 Amdahl’s Law speedup = (s+p) / (s+(p/n)) = 1 / (s+(p/n)) n帶無限，可以得到1/s 假設 溝通時沒有成本 問題大小固定 一個看能多處理多少工作 Gustafson’s Law (s+n*p) / (s+p) = (1-p)+n*np = 1+(n-1)*p 假設 處理時間固定 到處都是平行化 processor Instruction Level Parallelism (ILP) 讓一次處理的資料變多 指令 (把指令想像成一條長帶子) 把帶子空隙填滿: 總throughput不變，latency變小 Out of Order Execution stall時跑指令 Out-of-order execution =\u003e out-of-order completion Pipelining 把工作流程分段 各種harzard Structural hazards: 同時想用同一個phase Data hazards Read-after-write: 完成寫入前讀值 沒辦法在不犧牲latency的方式下處理 處理方式 stall Write-after-read: 完成讀值前寫入 Write-after-write: 完成第一個寫入前寫入 處理方式 ooo execution register renaming forwarding (cps) Control hazards 猜、猜、猜 Speculative Execution 猜 分支、指令相依性、數字 猜錯要整個重來 把帶子疊到帶子上(1 x n到2 x 0.5n): 總throughput變大，latency變小 Superscalar: 可以同時跑很多指令 原本是scalar，也就是只操作一條指令/資料單位 VLIW (Very Long Instruction Word): 把沒有相關的指令包在一起跑 由compiler指定什麼指令可以一起跑 資料 Vector Processing/SIMD (Single Instruction Multiple Data) 以array為單位去操作了 (原本是一次一個，做好幾次) programmer要自己寫，compiler或是cpu不會自己轉換 Multimedia Extensions 把一部分的reg當成array 像把32 bits的reg當成兩個16 bits的array Superscalar \u0026 VLIW 與Superscalar差在compiler的輸出，一個是parralel，一個是serial code 所以VLIW compiler需要做binder把call分配到下面的執行單元，Superscalar是留給硬體 Thread-Level Parallelism (TLP) thread origin real processor Hardware illusion: 在某個時間後就切到另一個thread Hardware OS user-level (PL的library) thread creation cobegin/coend: 這塊(procedure)可以平行的跑 fork/join: fork出去的(procedure或function)會平行的跑 future: 這段code(expression)會平行的跑 thread Scheduling 底層決定他什麼時候跑 (thread switch) fine grain: thread主動交棒 coarse grain: thread stall, 其他thread可以跑 等等 可以指定affinity 或是用user-level thread too many thread! 創thread要$$ mem cpu的cycle Sol thread pool或是固定thread數量 復用main thread before: A.fork(); B.fork(); A.join(); B.join(); after: A.fork(); B.run(); A.join(); TLP + ILP 把TLP的thread當成填充ILP發生stall的指令來源\nsummary Memory System 與cpu相比，ram的速度太慢了!! Locality 8/2法則: 最常用到的只有一點點 從古至今，hardware依舊靠locality加速 Temporal Locality (Locality in Time) loop Spatial Locality (Locality in Space) array Memory Hierarchy 越慢的放越遠 hardware Architecture 把多個處理單元放在一起，一起處理問題\n重點是? 資源分配 有什麼處理單元? cpu? gpu? 多少mem communication的成本 Data access, Communication and Synchronization 從Power與latency (所有效能面向)，communication是最貴的部分 Architecture的分類 Single-instruction single-data (SISD) Single-instruction multiple-data (SIMD) Multiple-instruction multiple-data (MIMD) Multiple-instruction single-data (MISD) 總的來說 Shared-memory Architectures 任何cpu都能access到任何mem 透過mem操作溝通 兩種類型 Uniform Memory Access\nSymmetric Multiprocessor (SMP) Cache Coherence 每個cpu都有自己的cache 如果有人改到mem的值，其他cache怎麼辦 Sol: Coherence Protocol 去invalidate其他cache Non-Uniform Memory Access\nDistributed shared memory 所以access到mem的時間會有所不同!! Coherence not Enough!! 傳輸有delay的話… Sol: Memory Consistency Model 執行某個記憶體操作，誰看的到這個改動 Sequential Consistency 每個指令都是atomic!! 還有其他的，在介紹lockfree時再提 Coherence \u0026 Consistency Coherence read會拿到什麼值 別人寫了，(cache)拿到的值是不是對的 behavior to same location Consistency 什麼時候會拿到寫入的值 寫之後，要一直read到某一次或是第一次之後才會看到 behavior to other locations Distributed-memory Architectures processor有自己的mem mem不與其他人共享 software Programming Shared-Memory Model 變數(mem)分成shared與private Explicit v.s. Implicit Threads Programming Explicit: pthread 創thread 用API創 programmer創thread與管理thread 分配工作 programmer自己寫 sync (等待thread完成) 手動join Implicit: openMP 創thread 用directives runtime創thread與管理thread 分配工作 用directives sync (等待thread完成) 在區塊結束的地方等 Nondeterminism: race condition mutex busy-waiting semaphore Transactional memory Thread Safety serial function在multi-thread能不能正常跑 反例: strtok 他有static去存目前string處理到哪邊，如果被多thread call… Distributed-Memory Model process有自己的mem mem不與其他人共享 各個process之間有rank作類似addr的功用 有另外的超能力 Broadcast: 把val推到其他process Reduction: 把其他process的output整合 One-Sided Communication 只更新一個mem的值 跟新local，from remote process 跟新remote，from local process Programming Hybrid Systems Partitioned Global Address Space Languages allow the user to use some shared-memory techniques for programming distributed-memory hardware 跨local的mem access十分慢!! Shared memory programing 3 synchronization scenarios critical section 多thread共同修改某一變數，就是critical section\nBusy-waiting 可能是reorder的受害者 y = f(id) while (flag != id) ; x += y flag++ // order y = f(id) x += y while (flag != id) ; flag++ 會把一顆cpu吃掉 (spin lock) mutex 就是mutex，在real time可以有priority promotion處理priority inversion\nSemaphore acc就是還有多少個空位的意思\n原本: acc + queue posix: acc\nSemaphores vs Mutexes Semaphore不管ownership，只要有人call semaphore，semaphore就會變\nProducer-Consumer Synchronization (no critical section) 沒有critical section No competition synchronization 為了合作而synchronization (Cooperation synchronization) barrier 要所有thread在同一時間啟動(或是停在同一個點)\n像是debug或是計時會用到\n用busy-waiting與mutex mutex.lock() acc += 1 mutex.unlock() while acc \u003c cnt_of_threads: pass 吃爆cpu(busy waiting) 怎麼做第2個barrier? reset acc? 要考慮有沒有reset acc對 用Semaphores 一個數有幾個process (count_sem, 1) (其實應該可以用atomic)\n一個數負責停下process (barrier_sem, 0)\nsem_wait(\u0026count_sem); if (counter == thread_count−1) { counter = 0; sem_post(\u0026count_sem); for (j = 0; j \u003c thread_count−1; j++) sem_post(\u0026barrier_sem); } else { counter++; sem_post(\u0026count_sem); sem_wait(\u0026barrier_sem); } 用condition variable pthread_mutex_lock(\u0026mutex); counter++; if (counter == thread_count) { counter = 0; pthread_cond_broadcast(\u0026cond_var); } else { pthread_cond_wait(\u0026cond_var, \u0026mutex) ; } pthread_mutex_unlock(\u0026mutex); lock其實很貴 假設要做一個multi-thread的linked list\n所有動作用lock包 那根本就是serial 每個node放lock lock要空間!! 實作十分複雜 明明只要read卻還要跟別人搶lock!? 所以這個效能是所有case中最爛的 read-write lock 可以與1一樣但是read的成本變小了 write多，總體效果與1一樣 read多，效果比1好 還可以保護write 關於cache cache miss void *Pth_mat_vect(void* rank) { ... for (i = my_first_row; i \u003c= my_last_row; i++) { y[i] = 0.0; // HERE for (j = 0; j \u003c n; j++) y[i] += A[i*n+j]*x[j]; // HERE } return NULL; } /* Pth_mat_vect */ cacheline是64 bytes\n如果y的範圍太大，寫入會失敗 (write-misses) 如果x的範圍太大，寫入會失敗 (read-misses) false sharing 假設上面程式的y，剛好可以都放入cacheline，但是只要cacheline的值被某個thread改變，其他thread要access資料時cacheline就要重拿資料(Cache Coherence)，而這邊明明都是y，如果一直有thread寫資料…\n所以可以想見，cacheline會一直重拿，但是明明大部分的cache(y)是對的!!\n另一個情況是task上編號(rank)，這樣在shared mem中就不會衝突，但因為false sharing就算把thread加上去，效能也沒有隨之變好\n加padding把cacheline塞滿 sum[id][pad] 用syncronization包成一個變數!! atomic_int sum Reordering Memory 再一次，memory consistency model sequentially consistent program order == code order == commit order Relaxed Consistency 把指令分成 data(write, read) 不保證順序 sync (mem barrier, volatile, atomic, fork/join…) 保證順序!! S -\u003e S S -\u003e W/R W/R -\u003e S mem barrier (在openMP叫flush) flush之前的變數(在flush有用到的部分)會被commit (所以flush中一定看的到) read mem barrier flush之後的變數(在flush有用到的部分)，會看到在flush中做出的結果 write mem barrier flush中不會reorder 單論Synchronization 我們的敵人 race condition: 泛指跑好幾次可能出現不同的結果 data race: 對同一個變數修改 reorder (講義叫Bad interleavings) a = 1; b = 2 與 b = 2; a = 1; 在compiler或是cpu眼中是可以reorder的!! 看arch的規定 工具 保持Atomicity (critical section) mutex，但有很多細節 新的敵人 deadlock Dining Philosophers (為lock上順序!!) Time-Of-Check-To-Time-Of-Use if (checkA()) { execA(); } 有人在checkA成功後，到執行execA之前，被其他thread做到事的話… 換言之，if的block中，不能信任有確認過的條件了 解法 Thread-Local Memory 不用share的資料就copy Immutable Memory 沒有write，沒有race condition或是data race 但我真的需要改 (要用mutex了qq) Use Consistent Locking 用同樣的lock到所有有關的地方 好好記錄為什麼需要這個lock 用lock去割出 shared-and-mutable locations lock-oriented Start With Fewer Locks (Coarse-Grained) 除非contention太嚴重才讓lock變多 (Fine-Grained) Keep Critical Sections Small Critical Sections太長: 效能差 Critical Sections太短: race condition (可以看到中間狀態) Think About Atomicity 想想什麼動作應該放在一起 像Time-Of-Check-To-Time-Of-Use 不是data race也不是race condition 但就是出事，所以應該把if與動作綁在一起 Use Libraries Message passing programing Point to Point Communication Communication透過recv與send執行\nmessage會傳\nsender的rank receiver的rank communicator (MPI的網路) tag: 使用者指定的tag data 下面是standard 的傳送方式 sender buf --{copy}--\u003e system buffer --{network}--\u003e system buffer --{copy}--\u003e receiver buf\ncommunication modes blocking blocking的理由是\n等handshake\nSynchronous 等copy\n所有類型都要等 從sender buffer 到 receiver buffer Synchronous Ready 從sender buffer 到 對面的system buffer 到 receiver buffer Standard(資料小) 從sender buffer 到 自己指定的 buffer 到 receiver buffer Buffered 沒copy到別的buffer (sender buf =\u003e receiver buf)\nSynchronous: 一般的tcp ssend送msg到recv說我要傳，之後等 recv送msg，之後等 handshake好了，可以送了，兩邊等到完成 Ready: 類似reverse tunnel recv送msg，之後等 rsend看有沒有recv的msg，有，開送，兩邊等到完成；沒有，報錯退出 copy到別的buffer\nBuffered: 先copy到自己指定的mem (in sender) bsend把資料copy到自己指定的mem，copy完退出 收到recv的msg，開送 Standard: 資料小: copy到system buffer (in receiver) send把資料送到對面的system buffer，等到送完 recv直接從system buffer copy到receiver buf，等到copy完 資料大: 就變成Synchronous non-blocking isend會開始送，但是不會等，馬上return 用test看目前狀態，wait去等他完成 irecv如果好了就會收，但是不會等，馬上return 用test看目前狀態，wait去等他完成 剩下就是Standard(資料小)\n可以想像成傳資料時開thread!!\ndeadlock (對，還是有) send 與 recv 要成對出現\nif (rank == 0) { err = MPI_Send(sendbuf, count, datatype, 1, tag, comm); err = MPI_Recv(recvbuf, count, datatype, 1, tag, comm, \u0026status); }else { err = MPI_Send(sendbuf, count, datatype, 0, tag, comm); err = MPI_Recv(recvbuf, count, datatype, 0, tag, comm, \u0026status); } 解法: swap if (rank == 0) { err = MPI_Send(sendbuf, count, datatype, 1, tag, comm); err = MPI_Recv(recvbuf, count, datatype, 1, tag, comm, \u0026status); }else { err = MPI_Recv(recvbuf, count, datatype, 0, tag, comm, \u0026status); err = MPI_Send(sendbuf, count, datatype, 0, tag, comm); } 解法: non-blocking if (rank == 0) { err = MPI_Isend(sendbuf, count, datatype, 1, tag, comm, \u0026req); err = MPI_Irecv(recvbuf, count, datatype, 1, tag, comm); err = MPI_Wait(req, \u0026status); }else { err = MPI_Isend(sendbuf, count, datatype, 0, tag, comm, \u0026req); err = MPI_Irecv(recvbuf, count, datatype, 0, tag, comm); err = MPI_Wait(req, \u0026status); } CUDA cpu vs gpu cpu: Latency\n大cache 降低mem延遲 複雜的control邏輯 branch prediction data forwarding 計算能力(ALU)強 降低operation延遲 在sequential code快 gpu: Throughput\n小cache 增加mem的throughput 簡易的control邏輯 NO branch prediction NO data forwarding 計算能力(ALU)弱(省能源) 延遲高，但是可以pipeline達成高throughput 所以有很多thread 在parallel code快 gpu 下面是gpu在arch上的特點 CUDA: Parallel Computing Platform CUDA: Heterogeneous Programming gpu叫device 控制的cpu叫host 各自有自己的mem，而跑在device上的function(thread)叫kernel Thread Hierarchies grid有很多block，block(wrap)有很多thread，block中的thread可以共享資料，也同時啟動(迴避掉sync的問題)，跑同一個指令\n不同block的thread不能合作，同時以wrap為單位做schedule\nAt any time, only one of the warps is executed by SM\nThread Synchronization 可以用 __syncthreads創barrier atomic Memory Model lockfree 無論當前處於什麼狀態，只要運行足夠長的時間，至少有一個 process 能取得進展或完成其操作 像是Real-time的狀況，有mutex就有可能發生priority inversion\n或是說，絕對不可能會有deadlock程式，也就是沒有lock的程式\n作法 如果不能lock，就只能busy-waiting(或是cpu有特別指令)\n做test-and-set，fetch-and-add，compare-and-swap，來確認改之前與改之後的值一不一樣，一樣就寫，不一樣繼續等\n少了lock之後 lock有一個很重要的性質，他是memory model的sync指令，所以不會被reorder\n但現在不能用lock，所以要注意兩個東西\ncpu的memory model 不同架構的cpu在不同case會做reorder的case不一樣 Weak vs. Strong Memory Models 怎麼下memory barrier (Acquire and Release Semantics) 得自己把不能reorder的範圍畫出來 Acquire and Release Semantics 有些programming language有提供memory model!! 可以不用直接調用memory barrier，改用volatile 謝謝JAVA 我之前的記憶體模型筆記 The Happens-Before Relation The Synchronizes-With Relation ABA問題 前面是看值一樣就當成沒改，但這是把資料與時間兩件式混在一起，所以有了ABA，也就是看起來沒換，但其實被人換過，只是資料剛好長的一樣\n所以要多一個變數紀錄時間，有改就要遞增；與read-write lock一樣\nwait-free 就前面看到的，lockfree可能讓process無限的等(飢餓)，但是wait-free可以在有限的次數讓process動 但超難，pass\nWait-Free Queues With Multiple Enqueuers and Dequeuers\nTODO wait-free Algorithms Sequential \u0026 Parallel: A Unified Approach 這我不確定，但是因為他比較新就放這裡 An introduction to parallel algorithms (jaja) 這裡面提到 Parallel Algo的手法 中提到的與沒有提到的手法 Ref An Introduction to Parallel Programming, Morgan Kaufmann Parallel Thinking Analysis of Parallel Programs More Parallel Primitives and Parallel Sorting Synchronization Some Sequential Algorithms are Almost Always Parallel An Introduction to Lock-Free Programming\n","wordCount":"1536","inLanguage":"en","image":"https://littlebees.github.io/images/papermod-cover.png","datePublished":"2022-01-17T14:27:20Z","dateModified":"2022-01-17T14:27:20Z","author":{"@type":"Person","name":"zhengcf"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://littlebees.github.io/2022/01/parallel-thinking/"},"publisher":{"@type":"Organization","name":"記事本","logo":{"@type":"ImageObject","url":"https://littlebees.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://littlebees.github.io/ accesskey=h title="記事本 (Alt + H)">記事本</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://littlebees.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://littlebees.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://littlebees.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://littlebees.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://littlebees.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Parallel Thinking</h1><div class=post-meta><span title='2022-01-17 14:27:20 +0000 UTC'>January 17, 2022</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;zhengcf</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%8b%95%e6%a9%9f aria-label=動機>動機</a></li><li><a href=#concurrent-computing--parallel-computing--distributed-computing aria-label="Concurrent computing & Parallel computing & Distributed computing">Concurrent computing & Parallel computing & Distributed computing</a></li><li><a href=#parallel-computing--distributed-computing aria-label="Parallel computing & Distributed computing">Parallel computing & Distributed computing</a></li><li><a href=#%e5%b9%b3%e8%a1%8c%e5%8c%96%e5%b0%b1%e6%98%af%e5%9c%a8--coordination aria-label="平行化就是在 => Coordination">平行化就是在 => Coordination</a><ul><li><a href=#task-parallelism--data-parallelism aria-label="Task-parallelism & Data-parallelism">Task-parallelism & Data-parallelism</a></li></ul></li><li><a href=#2-parallel-models--1-model aria-label="2 parallel models (+ 1 model)">2 parallel models (+ 1 model)</a><ul><li><a href=#shared-memory aria-label="Shared Memory">Shared Memory</a><ul><li><a href=#race-condition aria-label="Race Condition">Race Condition</a></li><li><a href=#%e5%84%aa%e7%bc%ba%e9%bb%9e aria-label=優缺點>優缺點</a></li></ul></li><li><a href=#message-passing aria-label="Message passing">Message passing</a><ul><li><a href=#%e5%84%aa%e7%bc%ba%e9%bb%9e-1 aria-label=優缺點>優缺點</a></li></ul></li><li><a href=#shared-memory--message-passing aria-label="Shared Memory & Message passing">Shared Memory & Message passing</a></li><li><a href=#%e9%99%a4%e4%ba%86divide%e9%82%84%e5%8f%af%e4%bb%a5%e4%b8%80%e6%ac%a1%e8%99%95%e7%90%86%e5%a4%9a%e4%b8%80%e9%bb%9e-data-parallel aria-label="除了divide，還可以一次處理多一點: Data Parallel">除了divide，還可以一次處理多一點: Data Parallel</a></li></ul></li><li><a href=#parallel-algo%e7%9a%84%e6%89%8b%e6%b3%95 aria-label="Parallel Algo的手法">Parallel Algo的手法</a><ul><li><a href=#parallel-primitives aria-label="parallel primitives">parallel primitives</a></li><li><a href=#parallel-algo%e7%9a%84bigo aria-label="Parallel Algo的BigO">Parallel Algo的BigO</a><ul><li><a href=#%e4%be%8b%e5%ad%90-findmin-on-tree aria-label="例子: findMin on Tree">例子: findMin on Tree</a></li></ul></li></ul></li><li><a href=#%e6%b8%ac%e9%87%8f%e6%8c%87%e6%a8%99 aria-label=測量指標>測量指標</a><ul><li><a href=#%e9%9a%a8%e8%91%97%e6%a0%b8%e5%bf%83%e6%95%b8%e4%b8%8a%e5%8d%87performance%e5%8f%af%e4%bb%a5%e6%8f%90%e5%8d%87%e5%88%b0%e5%a4%9a%e5%b0%91 aria-label=隨著核心數上升，performance可以提升到多少?>隨著核心數上升，performance可以提升到多少?</a></li></ul></li><li><a href=#%e5%88%b0%e8%99%95%e9%83%bd%e6%98%af%e5%b9%b3%e8%a1%8c%e5%8c%96 aria-label=到處都是平行化>到處都是平行化</a><ul><li><a href=#processor aria-label=processor>processor</a><ul><li><a href=#instruction-level-parallelism-ilp aria-label="Instruction Level Parallelism (ILP)">Instruction Level Parallelism (ILP)</a><ul><li><a href=#superscalar--vliw aria-label="Superscalar & VLIW">Superscalar & VLIW</a></li></ul></li><li><a href=#thread-level-parallelism-tlp aria-label="Thread-Level Parallelism (TLP)">Thread-Level Parallelism (TLP)</a><ul><li><a href=#tlp--ilp aria-label="TLP + ILP">TLP + ILP</a></li></ul></li><li><a href=#summary aria-label=summary>summary</a></li><li><a href=#memory-system aria-label="Memory System">Memory System</a></li></ul></li><li><a href=#hardware-architecture aria-label="hardware Architecture">hardware Architecture</a><ul><li><a href=#shared-memory-architectures aria-label="Shared-memory Architectures">Shared-memory Architectures</a><ul><li><a href=#coherence--consistency aria-label="Coherence & Consistency">Coherence & Consistency</a></li></ul></li><li><a href=#distributed-memory-architectures aria-label="Distributed-memory Architectures">Distributed-memory Architectures</a></li></ul></li><li><a href=#software-programming aria-label="software Programming">software Programming</a><ul><li><a href=#shared-memory-model aria-label="Shared-Memory Model">Shared-Memory Model</a></li><li><a href=#distributed-memory-model aria-label="Distributed-Memory Model">Distributed-Memory Model</a></li><li><a href=#programming-hybrid-systems aria-label="Programming Hybrid Systems">Programming Hybrid Systems</a></li></ul></li></ul></li><li><a href=#shared-memory-programing aria-label="Shared memory programing">Shared memory programing</a><ul><li><a href=#3-synchronization-scenarios aria-label="3 synchronization scenarios">3 synchronization scenarios</a><ul><li><a href=#critical-section aria-label="critical section">critical section</a><ul><li><a href=#busy-waiting aria-label=Busy-waiting>Busy-waiting</a></li><li><a href=#mutex aria-label=mutex>mutex</a></li><li><a href=#semaphore aria-label=Semaphore>Semaphore</a></li></ul></li><li><a href=#semaphores-vs-mutexes aria-label="Semaphores vs Mutexes">Semaphores vs Mutexes</a></li><li><a href=#producer-consumer-synchronization-no-critical-section aria-label="Producer-Consumer Synchronization (no critical section)">Producer-Consumer Synchronization (no critical section)</a></li><li><a href=#barrier aria-label=barrier>barrier</a><ul><li><a href=#%e7%94%a8busy-waiting%e8%88%87mutex aria-label=用busy-waiting與mutex>用busy-waiting與mutex</a></li><li><a href=#%e7%94%a8semaphores aria-label=用Semaphores>用Semaphores</a></li><li><a href=#%e7%94%a8condition-variable aria-label="用condition variable">用condition variable</a></li></ul></li></ul></li><li><a href=#lock%e5%85%b6%e5%af%a6%e5%be%88%e8%b2%b4 aria-label=lock其實很貴>lock其實很貴</a></li><li><a href=#%e9%97%9c%e6%96%bccache aria-label=關於cache>關於cache</a><ul><li><a href=#cache-miss aria-label="cache miss">cache miss</a></li><li><a href=#false-sharing aria-label="false sharing">false sharing</a></li></ul></li><li><a href=#reordering-memory aria-label="Reordering Memory">Reordering Memory</a></li><li><a href=#%e5%96%ae%e8%ab%96synchronization aria-label=單論Synchronization>單論Synchronization</a></li></ul></li><li><a href=#message-passing-programing aria-label="Message passing programing">Message passing programing</a><ul><li><a href=#point-to-point-communication aria-label="Point to Point Communication">Point to Point Communication</a></li><li><a href=#communication-modes aria-label="communication modes">communication modes</a><ul><li><a href=#blocking aria-label=blocking>blocking</a></li><li><a href=#non-blocking aria-label=non-blocking>non-blocking</a></li></ul></li><li><a href=#deadlock-%e5%b0%8d%e9%82%84%e6%98%af%e6%9c%89 aria-label="deadlock (對，還是有)">deadlock (對，還是有)</a><ul><li><a href=#%e8%a7%a3%e6%b3%95-swap aria-label="解法: swap">解法: swap</a></li><li><a href=#%e8%a7%a3%e6%b3%95-non-blocking aria-label="解法: non-blocking">解法: non-blocking</a></li></ul></li></ul></li><li><a href=#cuda aria-label=CUDA>CUDA</a><ul><li><a href=#cpu-vs-gpu aria-label="cpu vs gpu">cpu vs gpu</a></li><li><a href=#gpu aria-label=gpu>gpu</a></li><li><a href=#cuda-parallel-computing-platform aria-label="CUDA: Parallel Computing Platform">CUDA: Parallel Computing Platform</a></li><li><a href=#cuda-heterogeneous-programming aria-label="CUDA: Heterogeneous Programming">CUDA: Heterogeneous Programming</a><ul><li><a href=#thread-hierarchies aria-label="Thread Hierarchies">Thread Hierarchies</a><ul><li><a href=#thread-synchronization aria-label="Thread Synchronization">Thread Synchronization</a></li></ul></li><li><a href=#memory-model aria-label="Memory Model">Memory Model</a></li></ul></li></ul></li><li><a href=#lockfree aria-label=lockfree>lockfree</a><ul><li><a href=#%e4%bd%9c%e6%b3%95 aria-label=作法>作法</a><ul><li><a href=#%e5%b0%91%e4%ba%86lock%e4%b9%8b%e5%be%8c aria-label=少了lock之後>少了lock之後</a></li></ul></li><li><a href=#aba%e5%95%8f%e9%a1%8c aria-label=ABA問題>ABA問題</a></li><li><a href=#wait-free aria-label=wait-free>wait-free</a></li></ul></li><li><a href=#todo aria-label=TODO>TODO</a></li><li><a href=#ref aria-label=Ref>Ref</a></li></ul></div></details></div><div class=post-content><h2 id=動機>動機<a hidden class=anchor aria-hidden=true href=#動機>#</a></h2><p>平行的思考</p><h2 id=concurrent-computing--parallel-computing--distributed-computing>Concurrent computing & Parallel computing & Distributed computing<a hidden class=anchor aria-hidden=true href=#concurrent-computing--parallel-computing--distributed-computing>#</a></h2><p>其實就共用資源、與各個計算單元之間要溝通這件事而言，他們其實差不多</p><p>但如果還是要分的話
Parallelism: 多個core同時跑
Concurrency: 因為同時/交互執行/節點失敗/節點不可靠所造成的效果(non-determinacy)</p><p><img loading=lazy src=https://i.imgur.com/xazEgFP.png alt></p><h2 id=parallel-computing--distributed-computing>Parallel computing & Distributed computing<a hidden class=anchor aria-hidden=true href=#parallel-computing--distributed-computing>#</a></h2><p>其實就共用資源、與各個計算單元之間要溝通這件事而言，他們其實差不多</p><p>但Distributed computing更重視HA的部分</p><h2 id=平行化就是在--coordination>平行化就是在 => Coordination<a hidden class=anchor aria-hidden=true href=#平行化就是在--coordination>#</a></h2><ul><li>Communication: 傳送結果到其他點</li><li>Load Balancing: 工作量不能差太大</li><li>Synchronization: 要等其他點到一定程度</li></ul><p>用一般演算法的話來說就是D&amp;Q，所以怎麼divide?</p><h3 id=task-parallelism--data-parallelism>Task-parallelism & Data-parallelism<a hidden class=anchor aria-hidden=true href=#task-parallelism--data-parallelism>#</a></h3><p>Task-parallelism: 各種任務(像新年大掃除有掃廁所、清廚房&mldr;)，不同core做
Data-parallelism: 不同core做類似的任務，像多段下載</p><p>目前沒有一個好的方式把serial prog轉成Parallel prog
所以要自己寫!!</p><p>因為task吃題目，所以這裡只看Data-parallelism</p><h2 id=2-parallel-models--1-model>2 parallel models (+ 1 model)<a hidden class=anchor aria-hidden=true href=#2-parallel-models--1-model>#</a></h2><p>model的要素</p><ul><li>Control<ul><li>如何達成平行化</li><li>operation之間的怎麼協調執行順序</li></ul></li><li>Data<ul><li>那些資料是 private 是 shared</li><li>怎麼存取shraed data</li></ul></li><li>Synchronization<ul><li>有哪些operation</li><li>那些是atomic</li></ul></li><li>Cost<ul><li>有什麼成本</li></ul></li></ul><h3 id=shared-memory>Shared Memory<a hidden class=anchor aria-hidden=true href=#shared-memory>#</a></h3><ul><li>Control<ul><li>如何達成平行化<ul><li>thread當成抽象的processor</li><li>可以自由地創thread</li></ul></li><li>operation之間的怎麼協調執行順序<ul><li>讀寫shared memory</li><li>透過Synchronization做協調(coordinate)</li></ul></li></ul></li><li>Data<ul><li>那些資料是 private 是 shared<ul><li>private: 在thread內的</li><li>shared: 在thread外的</li></ul></li><li>怎麼存取shraed data<ul><li>直接讀寫mem</li></ul></li></ul></li><li>Synchronization<ul><li>有哪些operation<ul><li>Synchronization<ul><li>semaphore</li><li>mutex</li><li>busy-waiting</li></ul></li><li>協調<ul><li>condition variable</li></ul></li></ul></li><li>那些是atomic<ul><li>就是atomic與thread操作</li></ul></li></ul></li><li>Cost<ul><li>有什麼成本<ul><li>創thread</li><li>context switch</li></ul></li></ul></li></ul><h4 id=race-condition>Race Condition<a hidden class=anchor aria-hidden=true href=#race-condition>#</a></h4><p>每次跑，結果可能會不同</p><h4 id=優缺點>優缺點<a hidden class=anchor aria-hidden=true href=#優缺點>#</a></h4><ul><li>優點<ul><li>溝通方式簡單<ul><li>mem讀寫</li></ul></li></ul></li><li>缺點<ul><li>需要Synchronization機制</li><li>不好scale</li><li>處理cache很頭痛 (cache Coherence)</li></ul></li></ul><h3 id=message-passing>Message passing<a hidden class=anchor aria-hidden=true href=#message-passing>#</a></h3><ul><li>Control<ul><li>如何達成平行化<ul><li>有許多獨立的processor</li></ul></li><li>operation之間的怎麼協調執行順序<ul><li>send/receive pairs<ul><li>在溝通時剛好完成協調</li></ul></li></ul></li></ul></li><li>Data<ul><li>那些資料是 private 是 shared<ul><li>private: processor內的資料</li></ul></li><li>怎麼存取shraed data<ul><li>沒有shared data，要透過send/receive pairs</li></ul></li></ul></li><li>Synchronization<ul><li>有哪些operation<ul><li>不用</li></ul></li><li>那些是atomic<ul><li>不用</li></ul></li></ul></li><li>Cost<ul><li>有什麼成本<ul><li>溝通時的delay<ul><li>溝通的方式: 網路, mem</li><li>怎麼接收: poll, interrupt</li></ul></li></ul></li></ul></li></ul><h4 id=優缺點-1>優缺點<a hidden class=anchor aria-hidden=true href=#優缺點-1>#</a></h4><ul><li>優點<ul><li>明確溝通</li><li>不用特別為cache頭痛</li></ul></li><li>缺點<ul><li>溝通的成本很高</li><li>不好寫 (??)</li></ul></li></ul><h3 id=shared-memory--message-passing>Shared Memory & Message passing<a hidden class=anchor aria-hidden=true href=#shared-memory--message-passing>#</a></h3><ul><li>communication Turing complete<ul><li>彼此可以互相實作對方的模型</li></ul></li></ul><h3 id=除了divide還可以一次處理多一點-data-parallel>除了divide，還可以一次處理多一點: Data Parallel<a hidden class=anchor aria-hidden=true href=#除了divide還可以一次處理多一點-data-parallel>#</a></h3><p>像是array不是一次處理一個，或是讓thread分別處理每個段</p><p>現在是直接處理整個array!!</p><h2 id=parallel-algo的手法>Parallel Algo的手法<a hidden class=anchor aria-hidden=true href=#parallel-algo的手法>#</a></h2><ol><li>把任務切出來，讓其他人去做 (Divide-and-conquer)</li><li>讓一次處理的資料變多 (對array之類的collection操作)</li><li>Contraction (縮點、上rank)</li><li>Randomization</li><li>Identifying independent sets</li><li>Pointer jumping</li></ol><h3 id=parallel-primitives>parallel primitives<a hidden class=anchor aria-hidden=true href=#parallel-primitives>#</a></h3><ul><li>map</li><li>reduce: reduce, min, max, sum, &mldr;</li><li>scan: prefix sum</li><li>filter<ul><li>其實是由 map, scan, map構成<ul><li>map給的predicate</li><li>scan做prefix sum (counting sort)</li><li>利用 第1步與第2步的array去生array</li></ul></li></ul></li></ul><p>從FP來的，在mapreduce後廣為人知</p><h3 id=parallel-algo的bigo>Parallel Algo的BigO<a hidden class=anchor aria-hidden=true href=#parallel-algo的bigo>#</a></h3><ul><li>work: sequential 跑的時間複雜度 (只有一顆core)</li><li>span: 如果有很多core跑時，<strong>最長的</strong>時間複雜度 (有很多顆core)<ul><li>怎麼樣叫Parallel，怎麼樣叫Sequential<ul><li>fully Sequential: <code>for (i : range(A)) A[i] = f(A[i-1])</code><ul><li>這個span是A</li></ul></li><li>fully Parallel: <code>for (i : range(A)) A[i] = f(A[i])</code><ul><li>這個span是1</li></ul></li><li>partially Parallel: <code>for (i : range(A)) A[i] = f(A[i-sqrt(len(A))])</code><ul><li>這個span是sqrt(A)</li></ul></li></ul></li></ul></li><li>speed-up: work/span</li></ul><p>所以實際的時間複雜度 (Tp)，一定介於兩者之間
<code>work &lt;= Tp &lt;= span</code></p><p>之後有個神奇公式，可以求漸近的Tp，P是核心數，
<code>Tp = O(work/P + span)</code></p><p>這邊可以配合後面的 測量指標(尤其是Amdahl’s Law) 一起看</p><h4 id=例子-findmin-on-tree>例子: findMin on Tree<a hidden class=anchor aria-hidden=true href=#例子-findmin-on-tree>#</a></h4><pre tabindex=0><code class="language-python=" data-lang="python=">def findMin(root):
    if not root:
        return float(&#39;-inf&#39;)
    else:
        return min(root.data, findMin(root.left), findMin(root.right))
</code></pre><p>work:</p><pre tabindex=0><code>if n = 1
    O(1)
else
    O(left) + O(right) + O(1)
</code></pre><p>span:</p><pre tabindex=0><code>if n = 1
    O(1)
else
    max(O(left) + O(right)) + O(1)
</code></pre><p>所以需要知道實際的input，才知道實際的複雜度</p><ul><li>Degenerate Tree(就是list)<ul><li>work: O(n)</li><li>span: O(n)</li></ul></li><li>Perfect Tree(平衡樹)<ul><li>work: O(n)</li><li>span: O(log n)</li></ul></li></ul><h2 id=測量指標>測量指標<a hidden class=anchor aria-hidden=true href=#測量指標>#</a></h2><ul><li>speedup: serial執行的時間對上Parallel執行的時間 的比率<ul><li>線性speedup<ul><li>speedup 剛好等於 核心數</li><li>也就是隨著核心數上去速度就會上去</li></ul></li></ul></li><li>efficiency: speedup的比率除上核心數，也就是每個核心促進了多少進步</li><li>scalability: 資料量上升，核心數上升就能讓efficiency不變<ul><li>Strongly scalable<ul><li>核心數上升，<strong>資料量不變</strong>，efficiency不變</li></ul></li><li>Weakly scalable<ul><li>核心數上升，<strong>資料量上升</strong>，efficiency不變</li></ul></li></ul></li></ul><h3 id=隨著核心數上升performance可以提升到多少>隨著核心數上升，performance可以提升到多少?<a hidden class=anchor aria-hidden=true href=#隨著核心數上升performance可以提升到多少>#</a></h3><p>performance有兩個面向，latency與throughput</p><p>s是serial的比例(就是1-p)，p是parallel的比例(就是1-s)，n是核心數</p><p>兩個面向關注的點不同，</p><ul><li>一個是關注能減少多少時間<ul><li>Amdahl’s Law<ul><li><code>speedup = (s+p) / (s+(p/n)) = 1 / (s+(p/n))</code><ul><li>n帶無限，可以得到1/s</li></ul></li><li>假設<ul><li>溝通時沒有成本</li><li>問題大小固定</li></ul></li></ul></li></ul></li><li>一個看能多處理多少工作<ul><li>Gustafson’s Law<ul><li><code>(s+n*p) / (s+p) = (1-p)+n*np = 1+(n-1)*p</code></li><li>假設<ul><li>處理時間固定</li></ul></li></ul></li></ul></li></ul><h2 id=到處都是平行化>到處都是平行化<a hidden class=anchor aria-hidden=true href=#到處都是平行化>#</a></h2><h3 id=processor>processor<a hidden class=anchor aria-hidden=true href=#processor>#</a></h3><h4 id=instruction-level-parallelism-ilp>Instruction Level Parallelism (ILP)<a hidden class=anchor aria-hidden=true href=#instruction-level-parallelism-ilp>#</a></h4><ul><li>讓一次處理的資料變多<ul><li>指令 (把指令想像成一條長帶子)<ul><li>把帶子空隙填滿: 總throughput不變，latency變小<ul><li>Out of Order Execution<ul><li>stall時跑指令</li><li>Out-of-order execution => out-of-order completion</li></ul></li><li>Pipelining<ul><li>把工作流程分段</li><li>各種harzard<ul><li>Structural hazards: 同時想用同一個phase</li><li>Data hazards<ul><li>Read-after-write: 完成寫入前讀值<ul><li>沒辦法在不犧牲latency的方式下處理</li><li>處理方式<ul><li>stall</li></ul></li></ul></li><li>Write-after-read: 完成讀值前寫入</li><li>Write-after-write: 完成第一個寫入前寫入<ul><li>處理方式<ul><li>ooo execution<ul><li>register renaming</li></ul></li><li>forwarding (cps)</li></ul></li></ul></li></ul></li><li>Control hazards</li><li>猜、猜、猜<ul><li>Speculative Execution</li><li>猜 分支、指令相依性、數字</li><li>猜錯要整個重來</li></ul></li></ul></li></ul></li></ul></li><li>把帶子疊到帶子上(1 x n到2 x 0.5n): 總throughput變大，latency變小<ul><li>Superscalar: 可以同時跑很多指令<ul><li>原本是scalar，也就是只操作一條指令/資料單位</li></ul></li><li>VLIW (Very Long Instruction Word): 把沒有相關的指令包在一起跑<ul><li>由compiler指定什麼指令可以一起跑</li></ul></li></ul></li></ul></li><li>資料<ul><li>Vector Processing/SIMD (Single Instruction Multiple Data)<ul><li>以array為單位去操作了 (原本是一次一個，做好幾次)</li><li>programmer要自己寫，compiler或是cpu不會自己轉換</li><li>Multimedia Extensions<ul><li>把一部分的reg當成array<ul><li>像把32 bits的reg當成兩個16 bits的array</li></ul></li></ul></li></ul></li></ul></li></ul></li></ul><h5 id=superscalar--vliw>Superscalar & VLIW<a hidden class=anchor aria-hidden=true href=#superscalar--vliw>#</a></h5><p>與Superscalar差在compiler的輸出，一個是parralel，一個是serial code
<img loading=lazy src=https://i.imgur.com/WtmxKpp.png alt></p><p><img loading=lazy src=https://i.imgur.com/1BAOOcI.png alt></p><p>所以VLIW compiler需要做binder把call分配到下面的執行單元，Superscalar是留給硬體
<img loading=lazy src=https://i.imgur.com/pSosatq.png alt></p><p><img loading=lazy src=https://i.imgur.com/KaxmLXe.png alt></p><h4 id=thread-level-parallelism-tlp>Thread-Level Parallelism (TLP)<a hidden class=anchor aria-hidden=true href=#thread-level-parallelism-tlp>#</a></h4><ul><li>thread origin<ul><li>real processor<ul><li>Hardware</li></ul></li><li>illusion: 在某個時間後就切到另一個thread<ul><li>Hardware</li><li>OS</li><li>user-level (PL的library)</li></ul></li></ul></li><li>thread creation<ul><li>cobegin/coend: 這塊(procedure)可以平行的跑</li><li>fork/join: fork出去的(procedure或function)會平行的跑</li><li>future: 這段code(expression)會平行的跑</li></ul></li><li>thread Scheduling<ul><li>底層決定他什麼時候跑 (thread switch)<ul><li>fine grain: thread主動交棒</li><li>coarse grain: thread stall, 其他thread可以跑 等等</li></ul></li><li>可以指定affinity</li><li>或是用user-level thread</li></ul></li><li>too many thread!<ul><li>創thread要$$<ul><li>mem</li><li>cpu的cycle</li></ul></li><li>Sol<ul><li>thread pool或是固定thread數量</li><li>復用main thread<ul><li>before: <code>A.fork(); B.fork(); A.join(); B.join();</code></li><li>after: <code>A.fork(); B.run(); A.join();</code></li></ul></li></ul></li></ul></li></ul><h5 id=tlp--ilp>TLP + ILP<a hidden class=anchor aria-hidden=true href=#tlp--ilp>#</a></h5><p>把TLP的thread當成填充ILP發生stall的指令來源</p><h4 id=summary>summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h4><p><img loading=lazy src=https://i.imgur.com/skEWOYm.png alt></p><h4 id=memory-system>Memory System<a hidden class=anchor aria-hidden=true href=#memory-system>#</a></h4><ul><li>與cpu相比，ram的速度太慢了!!<ul><li>Locality<ul><li>8/2法則: 最常用到的只有一點點<ul><li>從古至今，hardware依舊靠locality加速</li></ul></li><li>Temporal Locality (Locality in Time)<ul><li>loop</li></ul></li><li>Spatial Locality (Locality in Space)<ul><li>array</li></ul></li></ul></li><li>Memory Hierarchy<ul><li>越慢的放越遠</li></ul></li></ul></li></ul><h3 id=hardware-architecture>hardware Architecture<a hidden class=anchor aria-hidden=true href=#hardware-architecture>#</a></h3><p>把多個處理單元放在一起，一起處理問題</p><ul><li>重點是?<ul><li>資源分配<ul><li>有什麼處理單元? cpu? gpu?</li><li>多少mem</li></ul></li><li>communication的成本<ul><li>Data access, Communication and Synchronization</li><li>從Power與latency (所有效能面向)，communication是最貴的部分</li></ul></li></ul></li><li>Architecture的分類<ul><li>Single-instruction single-data (SISD)</li><li>Single-instruction multiple-data (SIMD)</li><li>Multiple-instruction multiple-data (MIMD)</li><li>Multiple-instruction single-data (MISD)</li><li><img loading=lazy src=https://i.imgur.com/WgkdkAY.png alt></li></ul></li><li>總的來說<ul><li><img loading=lazy src=https://i.imgur.com/bLqxzci.png alt></li></ul></li></ul><h4 id=shared-memory-architectures>Shared-memory Architectures<a hidden class=anchor aria-hidden=true href=#shared-memory-architectures>#</a></h4><ul><li>任何cpu都能access到任何mem</li><li>透過mem操作溝通</li><li>兩種類型<ul><li><p>Uniform Memory Access</p><ul><li>Symmetric Multiprocessor (SMP)</li><li>Cache Coherence<ul><li>每個cpu都有自己的cache</li><li>如果有人改到mem的值，其他cache怎麼辦<ul><li><img loading=lazy src=https://i.imgur.com/SAkjaRH.png alt></li></ul></li><li>Sol: Coherence Protocol<ul><li>去invalidate其他cache</li></ul></li></ul></li></ul></li><li><p>Non-Uniform Memory Access</p><ul><li>Distributed shared memory<ul><li>所以access到mem的時間會有所不同!!<ul><li>Coherence not Enough!!<ul><li>傳輸有delay的話&mldr;</li><li><img loading=lazy src=https://i.imgur.com/Xl2kT4r.png alt></li></ul></li><li>Sol: Memory Consistency Model<ul><li>執行某個記憶體操作，誰看的到這個改動</li><li>Sequential Consistency<ul><li>每個指令都是atomic!!</li></ul></li><li>還有其他的，在介紹lockfree時再提</li></ul></li></ul></li></ul></li></ul></li><li><p><img loading=lazy src=https://i.imgur.com/GgrGrNh.png alt></p></li></ul></li></ul><h5 id=coherence--consistency>Coherence & Consistency<a hidden class=anchor aria-hidden=true href=#coherence--consistency>#</a></h5><ul><li>Coherence<ul><li>read會拿到<strong>什麼值</strong><ul><li>別人寫了，(cache)拿到的值是不是對的</li></ul></li><li>behavior to same location</li></ul></li><li>Consistency<ul><li><strong>什麼時候</strong>會拿到寫入的值<ul><li>寫之後，要一直read到某一次或是第一次之後才會看到</li></ul></li><li>behavior to other locations</li></ul></li></ul><h4 id=distributed-memory-architectures>Distributed-memory Architectures<a hidden class=anchor aria-hidden=true href=#distributed-memory-architectures>#</a></h4><ul><li>processor有自己的mem</li><li>mem不與其他人共享</li><li><img loading=lazy src=https://i.imgur.com/2CkRyXO.png alt></li></ul><h3 id=software-programming>software Programming<a hidden class=anchor aria-hidden=true href=#software-programming>#</a></h3><h4 id=shared-memory-model>Shared-Memory Model<a hidden class=anchor aria-hidden=true href=#shared-memory-model>#</a></h4><ul><li>變數(mem)分成shared與private</li><li>Explicit v.s. Implicit Threads Programming<ul><li>Explicit: pthread<ul><li>創thread<ul><li>用API創</li><li>programmer創thread與管理thread</li></ul></li><li>分配工作<ul><li>programmer自己寫</li></ul></li><li>sync (等待thread完成)<ul><li>手動join</li></ul></li></ul></li><li>Implicit: openMP<ul><li>創thread<ul><li>用directives</li><li>runtime創thread與管理thread</li></ul></li><li>分配工作<ul><li>用directives</li></ul></li><li>sync (等待thread完成)<ul><li>在區塊結束的地方等</li></ul></li></ul></li></ul></li><li>Nondeterminism: race condition<ul><li>mutex</li><li>busy-waiting</li><li>semaphore</li><li>Transactional memory</li></ul></li><li>Thread Safety<ul><li>serial function在multi-thread能不能正常跑</li><li>反例: strtok<ul><li>他有static去存目前string處理到哪邊，如果被多thread call&mldr;</li></ul></li></ul></li></ul><h4 id=distributed-memory-model>Distributed-Memory Model<a hidden class=anchor aria-hidden=true href=#distributed-memory-model>#</a></h4><ul><li>process有自己的mem</li><li>mem不與其他人共享</li><li>各個process之間有rank作類似addr的功用</li><li>有另外的超能力<ul><li>Broadcast: 把val推到其他process</li><li>Reduction: 把其他process的output整合</li></ul></li><li>One-Sided Communication<ul><li>只更新一個mem的值<ul><li>跟新local，from remote process</li><li>跟新remote，from local process</li></ul></li></ul></li></ul><h4 id=programming-hybrid-systems>Programming Hybrid Systems<a hidden class=anchor aria-hidden=true href=#programming-hybrid-systems>#</a></h4><ul><li>Partitioned Global Address Space Languages<ul><li>allow the user to use some shared-memory techniques<ul><li>for programming distributed-memory hardware</li></ul></li><li>跨local的mem access十分慢!!</li></ul></li></ul><h2 id=shared-memory-programing>Shared memory programing<a hidden class=anchor aria-hidden=true href=#shared-memory-programing>#</a></h2><h3 id=3-synchronization-scenarios>3 synchronization scenarios<a hidden class=anchor aria-hidden=true href=#3-synchronization-scenarios>#</a></h3><h4 id=critical-section>critical section<a hidden class=anchor aria-hidden=true href=#critical-section>#</a></h4><p>多thread共同修改某一變數，就是critical section</p><h5 id=busy-waiting>Busy-waiting<a hidden class=anchor aria-hidden=true href=#busy-waiting>#</a></h5><ul><li>可能是reorder的受害者<div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl>    <span class=n>y</span> <span class=o>=</span> <span class=n>f</span><span class=p>(</span><span class=n>id</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=p>(</span><span class=n>flag</span> <span class=o>!=</span> <span class=n>id</span><span class=p>)</span> <span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>+=</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>    <span class=n>flag</span><span class=o>++</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>// order
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>y</span> <span class=o>=</span> <span class=n>f</span><span class=p>(</span><span class=n>id</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>+=</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=p>(</span><span class=n>flag</span> <span class=o>!=</span> <span class=n>id</span><span class=p>)</span> <span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>flag</span><span class=o>++</span>
</span></span></code></pre></div></li><li>會把一顆cpu吃掉 (spin lock)</li></ul><h5 id=mutex>mutex<a hidden class=anchor aria-hidden=true href=#mutex>#</a></h5><p>就是mutex，在real time可以有priority promotion處理priority inversion</p><h5 id=semaphore>Semaphore<a hidden class=anchor aria-hidden=true href=#semaphore>#</a></h5><p>acc就是還有多少個空位的意思</p><p>原本: acc + queue
posix: acc</p><h4 id=semaphores-vs-mutexes>Semaphores vs Mutexes<a hidden class=anchor aria-hidden=true href=#semaphores-vs-mutexes>#</a></h4><p>Semaphore不管ownership，只要有人call semaphore，semaphore就會變</p><h4 id=producer-consumer-synchronization-no-critical-section>Producer-Consumer Synchronization (no critical section)<a hidden class=anchor aria-hidden=true href=#producer-consumer-synchronization-no-critical-section>#</a></h4><ul><li>沒有critical section<ul><li>No competition synchronization</li><li>為了合作而synchronization (Cooperation synchronization)</li></ul></li></ul><h4 id=barrier>barrier<a hidden class=anchor aria-hidden=true href=#barrier>#</a></h4><p>要所有thread在同一時間啟動(或是停在同一個點)</p><p>像是debug或是計時會用到</p><h5 id=用busy-waiting與mutex>用busy-waiting與mutex<a hidden class=anchor aria-hidden=true href=#用busy-waiting與mutex>#</a></h5><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>mutex</span><span class=o>.</span><span class=n>lock</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>acc</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=n>mutex</span><span class=o>.</span><span class=n>unlock</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=k>while</span> <span class=n>acc</span> <span class=o>&lt;</span> <span class=n>cnt_of_threads</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>pass</span>
</span></span></code></pre></div><ul><li>吃爆cpu(busy waiting)</li><li>怎麼做第2個barrier?<ul><li>reset acc? 要考慮有沒有reset acc對</li></ul></li></ul><h5 id=用semaphores>用Semaphores<a hidden class=anchor aria-hidden=true href=#用semaphores>#</a></h5><p>一個數有幾個process (count_sem, 1) (其實應該可以用atomic)</p><p>一個數負責停下process (barrier_sem, 0)</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>sem_wait</span><span class=p>(</span><span class=o>&amp;</span><span class=n>count_sem</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=n>counter</span> <span class=o>==</span> <span class=n>thread_count</span><span class=err>−</span><span class=mi>1</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>counter</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>sem_post</span><span class=p>(</span><span class=o>&amp;</span><span class=n>count_sem</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>thread_count</span><span class=err>−</span><span class=mi>1</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>sem_post</span><span class=p>(</span><span class=o>&amp;</span><span class=n>barrier_sem</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>counter</span><span class=o>++</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>sem_post</span><span class=p>(</span><span class=o>&amp;</span><span class=n>count_sem</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>sem_wait</span><span class=p>(</span><span class=o>&amp;</span><span class=n>barrier_sem</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span> 
</span></span></code></pre></div><h5 id=用condition-variable>用condition variable<a hidden class=anchor aria-hidden=true href=#用condition-variable>#</a></h5><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=n>pthread_mutex_lock</span><span class=p>(</span><span class=o>&amp;</span><span class=n>mutex</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=n>counter</span><span class=o>++</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=n>counter</span> <span class=o>==</span> <span class=n>thread_count</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>counter</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=n>pthread_cond_broadcast</span><span class=p>(</span><span class=o>&amp;</span><span class=n>cond_var</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>pthread_cond_wait</span><span class=p>(</span><span class=o>&amp;</span><span class=n>cond_var</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>mutex</span><span class=p>)</span> <span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>pthread_mutex_unlock</span><span class=p>(</span><span class=o>&amp;</span><span class=n>mutex</span><span class=p>);</span>
</span></span></code></pre></div><h3 id=lock其實很貴>lock其實很貴<a hidden class=anchor aria-hidden=true href=#lock其實很貴>#</a></h3><p>假設要做一個multi-thread的linked list</p><ol><li>所有動作用lock包<ul><li>那根本就是serial</li></ul></li><li>每個node放lock<ul><li>lock要空間!!</li><li>實作十分複雜</li><li>明明只要read卻還要跟別人搶lock!?<ul><li>所以這個效能是所有case中最爛的</li></ul></li></ul></li><li>read-write lock<ul><li>可以與1一樣但是read的成本變小了<ul><li>write多，總體效果與1一樣</li><li>read多，效果比1好</li></ul></li><li>還可以保護write</li></ul></li></ol><h3 id=關於cache>關於cache<a hidden class=anchor aria-hidden=true href=#關於cache>#</a></h3><h4 id=cache-miss>cache miss<a hidden class=anchor aria-hidden=true href=#cache-miss>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=kt>void</span> <span class=o>*</span><span class=nf>Pth_mat_vect</span><span class=p>(</span><span class=kt>void</span><span class=o>*</span> <span class=n>rank</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=p>...</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=n>i</span> <span class=o>=</span> <span class=n>my_first_row</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;=</span> <span class=n>my_last_row</span><span class=p>;</span> <span class=n>i</span><span class=o>++</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=mf>0.0</span><span class=p>;</span> <span class=c1>// HERE
</span></span></span><span class=line><span class=cl><span class=c1></span>        <span class=k>for</span> <span class=p>(</span><span class=n>j</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>j</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=n>j</span><span class=o>++</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>+=</span> <span class=n>A</span><span class=p>[</span><span class=n>i</span><span class=o>*</span><span class=n>n</span><span class=o>+</span><span class=n>j</span><span class=p>]</span><span class=o>*</span><span class=n>x</span><span class=p>[</span><span class=n>j</span><span class=p>];</span> <span class=c1>// HERE
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=nb>NULL</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=cm>/* Pth_mat_vect */</span>
</span></span></code></pre></div><p>cacheline是64 bytes</p><ol><li>如果y的範圍太大，寫入會失敗 (write-misses)</li><li>如果x的範圍太大，寫入會失敗 (read-misses)</li></ol><h4 id=false-sharing>false sharing<a hidden class=anchor aria-hidden=true href=#false-sharing>#</a></h4><p>假設上面程式的y，剛好可以都放入cacheline，但是只要cacheline的值被某個thread改變，其他thread要access資料時cacheline就要重拿資料(Cache Coherence)，而這邊明明都是y，如果一直有thread寫資料&mldr;</p><p>所以可以想見，cacheline會一直重拿，但是明明大部分的cache(y)是對的!!</p><p>另一個情況是task上編號(rank)，這樣在shared mem中就不會衝突，但因為false sharing就算把thread加上去，效能也沒有隨之變好</p><ol><li>加padding把cacheline塞滿<ul><li><code>sum[id][pad]</code></li></ul></li><li>用syncronization包成一個變數!!<ul><li><code>atomic_int sum</code></li></ul></li></ol><h3 id=reordering-memory>Reordering Memory<a hidden class=anchor aria-hidden=true href=#reordering-memory>#</a></h3><p><img loading=lazy src=https://i.imgur.com/GmbmSlh.png alt></p><ul><li>再一次，memory consistency model<ul><li>sequentially consistent<ul><li>program order == code order == commit order</li></ul></li><li>Relaxed Consistency<ul><li>把指令分成<ul><li>data(write, read)<ul><li>不保證順序</li></ul></li><li>sync (mem barrier, volatile, atomic, fork/join&mldr;)<ul><li>保證順序!!<ul><li>S -> S</li><li>S -> W/R</li><li>W/R -> S</li></ul></li><li>mem barrier (在openMP叫flush)<ul><li>flush之前的變數(在flush有用到的部分)會被commit (所以flush中一定看的到)<ul><li>read mem barrier</li></ul></li><li>flush之後的變數(在flush有用到的部分)，會看到在flush中做出的結果<ul><li>write mem barrier</li></ul></li><li>flush中不會reorder</li></ul></li></ul></li></ul></li></ul></li></ul></li></ul><h3 id=單論synchronization>單論Synchronization<a hidden class=anchor aria-hidden=true href=#單論synchronization>#</a></h3><ul><li>我們的敵人<ul><li>race condition: 泛指跑好幾次可能出現不同的結果<ul><li>data race: 對同一個變數修改</li></ul></li><li>reorder (講義叫Bad interleavings)<ul><li><code>a = 1; b = 2</code> 與 <code>b = 2; a = 1;</code><ul><li>在compiler或是cpu眼中是可以reorder的!!<ul><li>看arch的規定</li></ul></li></ul></li></ul></li></ul></li><li>工具<ul><li>保持Atomicity (critical section)<ul><li>mutex，但有很多細節</li></ul></li></ul></li><li>新的敵人<ul><li>deadlock<ul><li>Dining Philosophers (為lock上順序!!)</li></ul></li><li>Time-Of-Check-To-Time-Of-Use<ul><li><code>if (checkA()) { execA(); }</code></li><li>有人在checkA成功後，到執行execA之前，被其他thread做到事的話&mldr;<ul><li>換言之，if的block中，不能信任有確認過的條件了</li></ul></li></ul></li></ul></li><li>解法<ul><li>Thread-Local Memory<ul><li>不用share的資料就copy</li></ul></li><li>Immutable Memory<ul><li>沒有write，沒有race condition或是data race</li></ul></li><li>但我真的需要改 (要用mutex了qq)<ul><li>Use Consistent Locking<ul><li>用同樣的lock到所有有關的地方</li><li>好好記錄為什麼需要這個lock</li><li>用lock去割出 shared-and-mutable locations<ul><li>lock-oriented</li></ul></li></ul></li><li>Start With Fewer Locks (Coarse-Grained)<ul><li>除非contention太嚴重才讓lock變多 (Fine-Grained)</li></ul></li><li>Keep Critical Sections Small<ul><li>Critical Sections太長: 效能差</li><li>Critical Sections太短: race condition (可以看到中間狀態)</li></ul></li><li>Think About Atomicity<ul><li>想想什麼動作應該放在一起<ul><li>像Time-Of-Check-To-Time-Of-Use</li><li>不是data race也不是race condition</li><li>但就是出事，所以應該把if與動作綁在一起</li></ul></li></ul></li><li>Use Libraries</li></ul></li></ul></li></ul><p><img loading=lazy src=https://i.imgur.com/rgjIZBu.png alt></p><h2 id=message-passing-programing>Message passing programing<a hidden class=anchor aria-hidden=true href=#message-passing-programing>#</a></h2><h3 id=point-to-point-communication>Point to Point Communication<a hidden class=anchor aria-hidden=true href=#point-to-point-communication>#</a></h3><p>Communication透過recv與send執行</p><p>message會傳</p><ul><li>sender的rank</li><li>receiver的rank</li><li>communicator (MPI的網路)</li><li>tag: 使用者指定的tag</li><li>data</li></ul><p>下面是standard 的傳送方式
<code>sender buf --{copy}--> system buffer --{network}--> system buffer --{copy}--> receiver buf</code></p><h3 id=communication-modes>communication modes<a hidden class=anchor aria-hidden=true href=#communication-modes>#</a></h3><h4 id=blocking>blocking<a hidden class=anchor aria-hidden=true href=#blocking>#</a></h4><p>blocking的理由是</p><ul><li><p>等handshake</p><ul><li>Synchronous</li></ul></li><li><p>等copy</p><ul><li>所有類型都要等</li><li>從sender buffer 到 receiver buffer<ul><li>Synchronous</li><li>Ready</li></ul></li><li>從sender buffer 到 對面的system buffer 到 receiver buffer<ul><li>Standard(資料小)</li></ul></li><li>從sender buffer 到 自己指定的 buffer 到 receiver buffer<ul><li>Buffered</li></ul></li></ul></li><li><p>沒copy到別的buffer (sender buf => receiver buf)</p><ul><li>Synchronous: 一般的tcp<ol><li>ssend送msg到recv說我要傳，之後等</li><li>recv送msg，之後等</li><li>handshake好了，可以送了，兩邊等到完成</li></ol><ul><li><img loading=lazy src=https://i.imgur.com/rFcemmQ.png alt></li></ul></li><li>Ready: 類似reverse tunnel<ol><li>recv送msg，之後等</li><li>rsend看有沒有recv的msg，有，開送，兩邊等到完成；沒有，報錯退出</li></ol><ul><li><img loading=lazy src=https://i.imgur.com/9m990mT.png alt></li></ul></li></ul></li><li><p>copy到別的buffer</p><ul><li>Buffered: 先copy到自己指定的mem (in sender)<ol><li>bsend把資料copy到自己指定的mem，copy完退出</li><li>收到recv的msg，開送</li></ol><ul><li><img loading=lazy src=https://i.imgur.com/3MltPh3.png alt></li></ul></li><li>Standard:<ul><li>資料小: copy到system buffer (in receiver)<ol><li>send把資料送到對面的system buffer，等到送完</li><li>recv直接從system buffer copy到receiver buf，等到copy完</li></ol><ul><li><img loading=lazy src=https://i.imgur.com/Y4sfx64.png alt></li></ul></li><li>資料大: 就變成Synchronous<ul><li><img loading=lazy src=https://i.imgur.com/lCEPIqv.png alt></li></ul></li></ul></li></ul></li></ul><h4 id=non-blocking>non-blocking<a hidden class=anchor aria-hidden=true href=#non-blocking>#</a></h4><ol><li>isend會開始送，但是不會等，馬上return<ul><li>用test看目前狀態，wait去等他完成</li></ul></li><li>irecv如果好了就會收，但是不會等，馬上return<ul><li>用test看目前狀態，wait去等他完成</li></ul></li></ol><p>剩下就是Standard(資料小)</p><p><img loading=lazy src=https://i.imgur.com/qfmOova.png alt></p><p>可以想像成傳資料時開thread!!</p><h3 id=deadlock-對還是有>deadlock (對，還是有)<a hidden class=anchor aria-hidden=true href=#deadlock-對還是有>#</a></h3><p>send 與 recv 要成對出現</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>err</span> <span class=o>=</span> <span class=n>MPI_Send</span><span class=p>(</span><span class=n>sendbuf</span><span class=p>,</span> <span class=n>count</span><span class=p>,</span> <span class=n>datatype</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>tag</span><span class=p>,</span> <span class=n>comm</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>err</span> <span class=o>=</span> <span class=n>MPI_Recv</span><span class=p>(</span><span class=n>recvbuf</span><span class=p>,</span> <span class=n>count</span><span class=p>,</span> <span class=n>datatype</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>tag</span><span class=p>,</span> <span class=n>comm</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>status</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span><span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>err</span> <span class=o>=</span> <span class=n>MPI_Send</span><span class=p>(</span><span class=n>sendbuf</span><span class=p>,</span> <span class=n>count</span><span class=p>,</span> <span class=n>datatype</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>tag</span><span class=p>,</span> <span class=n>comm</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>err</span> <span class=o>=</span> <span class=n>MPI_Recv</span><span class=p>(</span><span class=n>recvbuf</span><span class=p>,</span> <span class=n>count</span><span class=p>,</span> <span class=n>datatype</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>tag</span><span class=p>,</span> <span class=n>comm</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>status</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h4 id=解法-swap>解法: swap<a hidden class=anchor aria-hidden=true href=#解法-swap>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>err</span> <span class=o>=</span> <span class=n>MPI_Send</span><span class=p>(</span><span class=n>sendbuf</span><span class=p>,</span> <span class=n>count</span><span class=p>,</span> <span class=n>datatype</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>tag</span><span class=p>,</span> <span class=n>comm</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>err</span> <span class=o>=</span> <span class=n>MPI_Recv</span><span class=p>(</span><span class=n>recvbuf</span><span class=p>,</span> <span class=n>count</span><span class=p>,</span> <span class=n>datatype</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>tag</span><span class=p>,</span> <span class=n>comm</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>status</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span><span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>err</span> <span class=o>=</span> <span class=n>MPI_Recv</span><span class=p>(</span><span class=n>recvbuf</span><span class=p>,</span> <span class=n>count</span><span class=p>,</span> <span class=n>datatype</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>tag</span><span class=p>,</span> <span class=n>comm</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>status</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>err</span> <span class=o>=</span> <span class=n>MPI_Send</span><span class=p>(</span><span class=n>sendbuf</span><span class=p>,</span> <span class=n>count</span><span class=p>,</span> <span class=n>datatype</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>tag</span><span class=p>,</span> <span class=n>comm</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h4 id=解法-non-blocking>解法: non-blocking<a hidden class=anchor aria-hidden=true href=#解法-non-blocking>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>if</span> <span class=p>(</span><span class=n>rank</span> <span class=o>==</span> <span class=mi>0</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>err</span> <span class=o>=</span> <span class=n>MPI_Isend</span><span class=p>(</span><span class=n>sendbuf</span><span class=p>,</span> <span class=n>count</span><span class=p>,</span> <span class=n>datatype</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>tag</span><span class=p>,</span> <span class=n>comm</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>req</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>err</span> <span class=o>=</span> <span class=n>MPI_Irecv</span><span class=p>(</span><span class=n>recvbuf</span><span class=p>,</span> <span class=n>count</span><span class=p>,</span> <span class=n>datatype</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>tag</span><span class=p>,</span> <span class=n>comm</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>err</span> <span class=o>=</span> <span class=n>MPI_Wait</span><span class=p>(</span><span class=n>req</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>status</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span><span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>err</span> <span class=o>=</span> <span class=n>MPI_Isend</span><span class=p>(</span><span class=n>sendbuf</span><span class=p>,</span> <span class=n>count</span><span class=p>,</span> <span class=n>datatype</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>tag</span><span class=p>,</span> <span class=n>comm</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>req</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>err</span> <span class=o>=</span> <span class=n>MPI_Irecv</span><span class=p>(</span><span class=n>recvbuf</span><span class=p>,</span> <span class=n>count</span><span class=p>,</span> <span class=n>datatype</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=n>tag</span><span class=p>,</span> <span class=n>comm</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=n>err</span> <span class=o>=</span> <span class=n>MPI_Wait</span><span class=p>(</span><span class=n>req</span><span class=p>,</span> <span class=o>&amp;</span><span class=n>status</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h2 id=cuda>CUDA<a hidden class=anchor aria-hidden=true href=#cuda>#</a></h2><p><img loading=lazy src=https://i.imgur.com/bipcZbK.png alt></p><h3 id=cpu-vs-gpu>cpu vs gpu<a hidden class=anchor aria-hidden=true href=#cpu-vs-gpu>#</a></h3><p><img loading=lazy src=https://i.imgur.com/Y5FnG7O.png alt>
<img loading=lazy src=https://i.imgur.com/uDo2iLn.png alt>
<img loading=lazy src=https://i.imgur.com/6DBNRtv.png alt>
<img loading=lazy src=https://i.imgur.com/LnmsK9k.png alt></p><ul><li><p>cpu: Latency</p><ul><li>大cache<ul><li>降低mem延遲</li></ul></li><li>複雜的control邏輯<ul><li>branch prediction</li><li>data forwarding</li></ul></li><li>計算能力(ALU)強<ul><li>降低operation延遲</li></ul></li><li>在sequential code快</li><li><img loading=lazy src=https://i.imgur.com/QY4cmRT.png alt></li></ul></li><li><p>gpu: Throughput</p><ul><li>小cache<ul><li>增加mem的throughput</li></ul></li><li>簡易的control邏輯<ul><li>NO branch prediction</li><li>NO data forwarding</li></ul></li><li>計算能力(ALU)弱(省能源)<ul><li>延遲高，但是可以pipeline達成高throughput<ul><li>所以有<strong>很多</strong>thread</li></ul></li></ul></li><li>在parallel code快</li><li><img loading=lazy src=https://i.imgur.com/R6Zrm30.png alt></li></ul></li></ul><h3 id=gpu>gpu<a hidden class=anchor aria-hidden=true href=#gpu>#</a></h3><p>下面是gpu在arch上的特點
<img loading=lazy src=https://i.imgur.com/AV6N7D6.png alt>
<img loading=lazy src=https://i.imgur.com/GlP5OdH.png alt>
<img loading=lazy src=https://i.imgur.com/TsxBVxy.png alt>
<img loading=lazy src=https://i.imgur.com/bXAa9X7.png alt>
<img loading=lazy src=https://i.imgur.com/yGe3fV2.png alt></p><h3 id=cuda-parallel-computing-platform>CUDA: Parallel Computing Platform<a hidden class=anchor aria-hidden=true href=#cuda-parallel-computing-platform>#</a></h3><p><img loading=lazy src=https://i.imgur.com/aOsDBlb.png alt>
<img loading=lazy src=https://i.imgur.com/LbbSIdZ.png alt></p><h3 id=cuda-heterogeneous-programming>CUDA: Heterogeneous Programming<a hidden class=anchor aria-hidden=true href=#cuda-heterogeneous-programming>#</a></h3><p><img loading=lazy src=https://i.imgur.com/lJYJwVy.png alt>
<img loading=lazy src=https://i.imgur.com/GkIRVVs.png alt>
<img loading=lazy src=https://i.imgur.com/baa7m0s.png alt></p><p>gpu叫<em>device</em>
控制的cpu叫<em>host</em>
各自有自己的mem，而跑在device上的function(thread)叫kernel
<img loading=lazy src=https://i.imgur.com/YsT0FTF.png alt>
<img loading=lazy src=https://i.imgur.com/xUbOepe.png alt>
<img loading=lazy src=https://i.imgur.com/BYlb6Lx.png alt></p><h4 id=thread-hierarchies>Thread Hierarchies<a hidden class=anchor aria-hidden=true href=#thread-hierarchies>#</a></h4><p>grid有很多block，block(wrap)有很多thread，block中的thread可以共享資料，也同時啟動(迴避掉sync的問題)，跑同一個指令</p><p>不同block的thread不能合作，同時以wrap為單位做schedule</p><p>At any time, only one of the warps is executed by SM</p><p><img loading=lazy src=https://i.imgur.com/0CxmgBp.png alt></p><h5 id=thread-synchronization>Thread Synchronization<a hidden class=anchor aria-hidden=true href=#thread-synchronization>#</a></h5><ul><li>可以用<code> __syncthreads</code>創barrier</li><li>atomic</li></ul><h4 id=memory-model>Memory Model<a hidden class=anchor aria-hidden=true href=#memory-model>#</a></h4><p><img loading=lazy src=https://i.imgur.com/GLS2dNJ.png alt>
<img loading=lazy src=https://i.imgur.com/uVyBfhp.png alt></p><h2 id=lockfree>lockfree<a hidden class=anchor aria-hidden=true href=#lockfree>#</a></h2><p>無論當前處於什麼狀態，只要運行足夠長的時間，至少有一個 process 能取得進展或完成其操作
像是Real-time的狀況，有mutex就有可能發生priority inversion</p><p>或是說，絕對不可能會有deadlock程式，也就是沒有lock的程式</p><h3 id=作法>作法<a hidden class=anchor aria-hidden=true href=#作法>#</a></h3><p>如果不能lock，就只能busy-waiting(或是cpu有特別指令)</p><p>做test-and-set，fetch-and-add，compare-and-swap，來確認改之前與改之後的值一不一樣，一樣就寫，不一樣繼續等</p><h4 id=少了lock之後>少了lock之後<a hidden class=anchor aria-hidden=true href=#少了lock之後>#</a></h4><p>lock有一個很重要的性質，他是memory model的sync指令，所以不會被reorder</p><p>但現在不能用lock，所以要注意兩個東西</p><ol><li>cpu的memory model<ul><li>不同架構的cpu在不同case會做reorder的case不一樣</li><li><a href=https://preshing.com/20120930/weak-vs-strong-memory-models/>Weak vs. Strong Memory Models</a></li></ul></li><li>怎麼下memory barrier (Acquire and Release Semantics)<ul><li>得自己把不能reorder的範圍畫出來</li><li><a href=https://preshing.com/20120913/acquire-and-release-semantics/>Acquire and Release Semantics</a><ul><li>有些programming language有提供memory model!!<ul><li>可以不用直接調用memory barrier，改用volatile<ul><li>謝謝JAVA</li><li><a href=/2021/06/mem-models/>我之前的記憶體模型筆記</a></li></ul></li><li><a href=https://preshing.com/20130702/the-happens-before-relation/>The Happens-Before Relation</a></li><li><a href=https://preshing.com/20130823/the-synchronizes-with-relation/>The Synchronizes-With Relation</a></li></ul></li></ul></li></ul></li></ol><h3 id=aba問題>ABA問題<a hidden class=anchor aria-hidden=true href=#aba問題>#</a></h3><p>前面是看值一樣就當成沒改，但這是把資料與時間兩件式混在一起，所以有了ABA，也就是看起來沒換，但其實被人換過，只是資料剛好長的一樣</p><p>所以要多一個變數紀錄時間，有改就要遞增；與read-write lock一樣</p><h3 id=wait-free>wait-free<a hidden class=anchor aria-hidden=true href=#wait-free>#</a></h3><p>就前面看到的，lockfree可能讓process無限的等(飢餓)，但是wait-free可以在有限的次數讓process動
但超難，pass</p><p><a href=http://www.cs.technion.ac.il/~erez/Papers/wfquque-ppopp.pdf>Wait-Free Queues With Multiple Enqueuers and Dequeuers</a></p><h2 id=todo>TODO<a hidden class=anchor aria-hidden=true href=#todo>#</a></h2><ul><li>wait-free</li><li>Algorithms Sequential & Parallel: A Unified Approach<ul><li>這我不確定，但是因為他比較新就放這裡</li></ul></li><li>An introduction to parallel algorithms (jaja)<ul><li>這裡面提到 Parallel Algo的手法 中提到的與沒有提到的手法</li></ul></li></ul><h2 id=ref>Ref<a hidden class=anchor aria-hidden=true href=#ref>#</a></h2><p>An Introduction to Parallel Programming, Morgan Kaufmann
<a href=https://www.cs.cmu.edu/~blelloch/papers/PPoPP09.pdf>Parallel Thinking</a>
<a href=https://courses.cs.washington.edu/courses/cse332/17wi/lectures/forkjoin-2/forkjoin-analysis.pdf>Analysis of Parallel Programs</a>
<a href=https://courses.cs.washington.edu/courses/cse332/17wi/lectures/forkjoin-3/scan-pack-sorting.pdf>More Parallel Primitives and Parallel Sorting</a>
<a href=https://courses.cs.washington.edu/courses/cse332/17wi/lectures/synchronization-1/synchronization.pdf>Synchronization</a>
<a href=https://www.cs.cmu.edu/~guyb/papers/spaapodc17.pdf>Some Sequential Algorithms are Almost Always Parallel</a>
<a href=https://preshing.com/20120612/an-introduction-to-lock-free-programming/>An Introduction to Lock-Free Programming</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://littlebees.github.io/tags/parallel/>Parallel</a></li></ul><nav class=paginav><a class=prev href=https://littlebees.github.io/2022/01/%E5%8F%AA%E8%AE%80%E4%B8%80%E9%83%A8%E4%BB%BD%E7%9A%84yaml%E6%88%90ansible%E7%9A%84%E8%AE%8A%E6%95%B8/><span class=title>« Prev</span><br><span>只讀一部份的yaml成ansible的變數</span>
</a><a class=next href=https://littlebees.github.io/2022/01/%E5%9C%A8bash%E4%B8%AD%E5%AF%ABrucursive/><span class=title>Next »</span><br><span>在bash中寫rucursive</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://littlebees.github.io/>記事本</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>