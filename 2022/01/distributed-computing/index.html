<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Distributed computing | 記事本</title>
<meta name=keywords content="Distributed"><meta name=description content="動機
論選好課、書的重要性"><meta name=author content="zhengcf"><link rel=canonical href=https://littlebees.github.io/2022/01/distributed-computing/><link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=https://littlebees.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://littlebees.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://littlebees.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://littlebees.github.io/apple-touch-icon.png><link rel=mask-icon href=https://littlebees.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://littlebees.github.io/2022/01/distributed-computing/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Distributed computing"><meta property="og:description" content="動機
論選好課、書的重要性"><meta property="og:type" content="article"><meta property="og:url" content="https://littlebees.github.io/2022/01/distributed-computing/"><meta property="og:image" content="https://littlebees.github.io/images/papermod-cover.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-01-17T16:17:07+00:00"><meta property="article:modified_time" content="2022-01-17T16:17:07+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://littlebees.github.io/images/papermod-cover.png"><meta name=twitter:title content="Distributed computing"><meta name=twitter:description content="動機
論選好課、書的重要性"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://littlebees.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Distributed computing","item":"https://littlebees.github.io/2022/01/distributed-computing/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Distributed computing","name":"Distributed computing","description":"動機 論選好課、書的重要性\n","keywords":["Distributed"],"articleBody":"動機 論選好課、書的重要性\nModels of distributed systems 怎麼處理上面會騙人的狀況? 只有兩人無法判斷對錯，所以如果有一個壞人，就要有兩個好人，才能抵銷\n剛剛的兩個問題都有兩項東西\nNode Network 還有另一個要考慮的是 latency!!\nsystem model是由下面3項組成\nNetwork behaviour (e.g. message loss) (assume) bidirectional point-to-point communication Reliable (perfect) links 如果收到msg，就代表他有被送出 可能重排 Fair-loss links + retry\u0026消除重複 Fair-loss links msg可能消失、重複、重排 但一直try總會到對面去 Arbitrary links + TLS Arbitrary links 中間有不懷好意的人 Network partition 線路會drop或是delay Node behaviour (e.g. crashes) 如果出事(faulty)的話 Crash-stop (fail-stop) crash就直接停下來 Crash-recovery (fail-recovery) crash會失去mem中的狀態 在一段時間之後會自己復原 Byzantine (fail-arbitrary) crash後什麼事都有可能發生 Timing behaviour (e.g. latency) Node \u0026 Network Synchronous latency有上限 node的執行速度是可以預估的 Partially synchronous 有些部分是async的(會結束，但不知道多久) 其他都是sync Asynchronous latency不確定 node也不確定會不會隨時停下 Node的意外 Operating system scheduling (priority inversion) Stop-the-world garbage collection Page faults, swap, thrashing Network的意外 Message loss requiring retry Congestion/contention causing queueing Network/route reconfiguration Failure: system完全不動\nFault: 一部份不動了\nnode crash (crash-stop/crash-recovery) deviating from algorithm (Byzantine) Network dropping or significantly delaying messages Failure detectors\nPerfect failure detector labels a node as faulty iff it has crashed Typical implementation send message await response label node as crashed if no reply within some timeout Problem 不能區分是 crashed node temporarily unresponsive node lost message delayed message 所以只能在 synchronous crash-stop system with reliable links Eventually perfect failure detector temporarily 標成 crashed, 就算是 correct 標成 correct, 就算是 crashed eventually 標成 crashed, iff crashed Time, clocks, and ordering of events two types of clock physical clocks 就是時間，數花掉的時間 會有誤差 (可能在調整時變大變小) Coordinated Universal Time (UTC) UTC 是由 TAI 配合地球自轉速度做修正 International Atomic Time(TAI): from 原子鐘 地球自轉速度不是常數 Leap seconds 在 on 30 June and 31 December at 23:59:59 UTC negative leap second: 直接跳過去 positive leap second: 跳到 23:59:60等1秒，再跳過去 timestamps格式 Unix time: number of seconds since 1 January 1970 00:00:00 UTC 不算Leap seconds ISO 8601: year, month, day, hour, minute, second, and timezone offset relative to UTC 2020-11-09T09:50:17+00:00 NTP Clock skew: 兩個clock的差 作法 對多台server取樣，跑算式\n開始調時間\nTime-of-day clock 從某個時間點開始 適合比較 可能會變動 (NTP矯正) Monotonic clock 隨便一個點開始 適合計算花費的時間 logical clocks 用來數有多少事件 單調遞增 happens-before relation (就是有向圖的path) 定義 同一個process a發生在b之前 不同process a送msg，b收到 遞移律 a -\u003e b -\u003e c: a -\u003e c 就是有向圖的path partial order concurrent: 不是 a -\u003e b 或 b -\u003e a a || b Causality concurrent代表兩者一定沒有因果關係 有hb就是可能有因果關係 定義 a strict total order on events Broadcast protocols and logical time capture causal dependencies\nNo Physical timestamps!! two types of logical clocks Lamport clocks 作法 每個process init自己的時間 t = 0 有event發生 t++ send某個m t++ send((t, m)) recv到某個(tx, m) t = max(tx, t) + 1 對m做事 Properties L是取t的函數、N是取在哪個process的函數 the pair (L(e), N(e)) uniquely identifies event e 用Lamport clocks定義total order BUT 不同process的timestamp可能會一樣!! 不能分辨 a -\u003e b 或是 a || b 因為把自己與其他人的時間混在一起了 Vector clocks 作法 每個process(假設叫i) init自己的時間 t = [0] * len(procs) 有event發生 t[i]++ send某個m t[i]++ send((t, m)) recv到某個(tx, m) t = [max(t[j], tx[j]) if j != i else t[i]+1 for j in range(len(procs))] 對m做事 用Vector clocks定義total order Broadcast protocols\nbroadcast的種類 FIFO 從同一個process出來的msg，收到msg的順序，與送的順序一樣 vaild order: (m1一定在m3前面，其他隨便) (m2, m1, m3) (m1, m2, m3) (m1, m3, m2) Causal broadcast(m1) -\u003e broadcast(m2)，那m1一定先收到，之後才是m2 vaild order: (m1 -\u003e m2，m1 -\u003e m3) (m1, m2, m3) (m1, m3, m2) Total order 只要m1先收到，在所有process都會是m1先收到 vaild order: (m1, m2, m3) vaild order: (m1, m3, m2) FIFO-total order FIFO + Total order Broadcast algorithms 要處理兩個部分 把best-effort broadcast變成reliable 利用retransmitting 保持送(收)的順序 Not Reliable (Naive) 直接送到process去 Problem 送的process中間掛掉怎麼辦 Reliable Eager reliable broadcast process只要是第一次收到時就re-boardcast!! 到所有process Problem 總共有O(n^2)個msg在流動!! Gossip protocols process只要是第一次收到時就re-boardcast!! 到3個process (隨機選) 適合用在process很多時 Problem Eventually reaches all nodes (with high probability) FIFO broadcast algorithm 重點是在buffer中找有對到自己的delivered的msg才做下一步動作 這樣就是FIFO，從同一個process出來的msg的順序是對的 Causal broadcast algorithm 延伸FIFO broadcast algorithm，但 改成vector clock!! (這樣就完成CO了!!) Total order broadcast algorithms Single leader (轉成BFS tree) 會有單點失敗!! Lamport clocks (用Lamport clocks排序) 怎麼確定我現在拿到的msg是最小的? 要等到每個process的msg的stamp都比較大才會知道 Replication Use best-effort broadcast Idempotence\n多次操作後不用dedup的操作 retry behaviour At-most-once 不retry At-least-once Retry到收到ack Exactly-once Retry + idempotence deduplication 就算是idempotence，還是有影響到狀態 所以下面的client2到了最後沒辦法看到移除了client1 add的資料的狀態 Timestamps and tombstones (soft delete)\n資料會放一個flag，標有沒有被刪過\nReconciling replicas\n還會放一個timestamp標什麼時候被寫入 Concurrent writes by different clients\n兩種做法 Last writer wins 取timestamp最大的 total order (e.g. Lamport clock) 注意 data loss Multi-value register 如果可以比，取最大；不能比，都存 partial order (e.g. vector clock) Quorum (Byzantine problem)\nRead-after-write consistency 只要read(read quorum)/write(write quorum)的response有到達指定人數就取這個結果 read quorum + write quorum \u003e nodes 一般取，(nodes+1)/2 可以在write時忍受nodes-write quorum壞掉，read是nodes-read quorum Use Total order broadcast (在每個process中msg的順序都是一樣的!!) State machine replication FIFO total order broadcast送update msg: 一定會到、順序一樣 能不能用更弱的broadcast 這樣就沒有順序的保證了!! 但是update的順序如果不影響最後的結果的話 commutative: f(g(x)) = g(f(x)) 所以可以把update msg當成map，Replica當成DFA same input, same output: deterministic 限制 不能馬上更新，要等msg傳遞 需要 fault-tolerant total order broadcast Consensus Fault-tolerant total order broadcast total order broadcast一定要leader!! leader壞了怎麼辦? 自己選一個 用failure detector (timeout)看leader壞了沒 壞了就選下一個 確保只有一個leader 用term區份這個任期中誰是該區市民與leader 需要定義Quorum (過 半+1) 每個process在每個任期中最多只能投一次票 這樣可以確保一個任期只有一個leader leader在傳msg之前都要ack，不然怕場面尷尬 Consensus and total order broadcast Consensus: 大家都同意某一個值 total order broadcast: 大家都同意下一個msg要送什麼 Consensus and total order broadcast are formally equivalent Common consensus algorithms: Paxos, Multi-Paxos, Raft Distributed mutual exclusion 作法 central lock server leader是bottleneck 單點失敗 怎麼重選leader token passing 用一個token去傳(整個要連成一個ring)，拿到就當成拿到lock 單點失敗 怎麼rebuild ring 怎麼重生token Totally ordered multicast 只有一個人held，所以讓所有人投票，要拿到N-1 raft是(N+1)/2 concurrent requests (兩個以上want lock) 看pid比大小 哲學家用餐問題!! Consensus system models 假設system model是 partially synchronous not asynchronous (FLP result) in an asynchronous crash-stop system model no deterministic consensus algorithm that is guaranteed to terminate use clocks only used for timeouts/failure detector to ensure progress not Safety (correctness) crash-recovery Raft state(腳色) 變化 元件 log: leader傳過的msg 或是 follower收到的msg (array) msg term: 傳msg當時的term term: 任期 主要是看這個 sentLength: leader傳了多長的log給follower ackedLength: follower回報他們的log多長 commitLength: 真的有deliver的有多少 class Node: def __init__(self): self.init_runtime_state() self.Term = 0 self.votedFor = None self.log = [] self.commitLength = 0 self.id = \"whatever\" def init_runtime_state(self): self.Role = \"follower\" self.Leader = None self.votesReceived = set() self.sentLength = [0]*len(nodes) self.ackedLength = [0]*len(nodes) def when_leader_fail_OR_election_timeout(self): self.Term += 1 self.Role = \"candidate\" self.votedFor = self.id self.votesReceived.add(self.id) lastTerm = self.log[-1].term if len(self.log) \u003e 0 else 0 for n in nodes: __send(n, (\"VoteRequest\", self.id, self.Term, len(self.log), lastTerm)) __startElectionTimer() def when_recv_VoteRequest(self, cId, cTerm, CLogLen, cLogTerm): myLogTerm = self.log[-1].term isLargeLogTerm = cLogTerm \u003e myLogTerm inSameLogTerm = myLogTerm == myLogTerm hasMoreLog = CLogLen \u003e= len(self.log) logOK = isLargeLogTerm or (inSameLogTerm and hasMoreLog) isLargeTerm = cTerm \u003e self.Term inSameTerm = cTerm == self.Term notVoted_OR_voteSame = self.votedFor in {cId, None} termOK = isLargeTerm or (inSameTerm and notVoted_OR_voteSame) if logOK and termOK: self.Term, self.Role, self.votedFor = cTerm, \"follower\", cId __send(cId, (\"VoteResponse\", self.id, self.Term, True)) else: __send(cId, (\"VoteResponse\", self.id, self.Term, False)) def when_recv_VoteResponse(self, vId, vTerm, isAgree): if self.Role == \"candidate\" and self.Term == vTerm and isAgree: self.votesReceived.add(vId) if len(self.votesReceived) \u003e= (len(nodes)+1)//2: self.Role, self.Leader = \"leader\", self.id __stopElectionTimer() for n in {n in nodes if n is not self}: self.sentLength[n], self.ackedLength[n] = len(self.log), 0 self.copyLogTo(n) elif vTerm \u003e self.term: self.Role, self.Term, self.votedFor = \"follower\", vTerm, None __stopElectionTimer() def broadcast(self, msg): if self.Role == \"leader\": self.log.append((msg, self.Term)) self.ackedLength[self.id] = len(self.log) for n in {n in nodes if n is not self}: self.copyLogTo(n) else: __forwarding_to_leader(msg) def periodically_do(self): if self.Role == \"leader\": for n in {n in nodes if n is not self}: self.copyLogTo(n) def copyLogTo(self, n): i = self.sentLength[n] diffLogs = self.log[i:] prevFollowerTerm = self.log[max(0, i-1)] __send(n, (\"LogRequest\", self.id, self.Term, i, prevFollowerTerm, self.commitLength, diffLogs)) def when_recv_LogRequest(self, lId, lTerm, followerLogStart, followerLogTerm, lCommitLen, diffLogs): isLargeTerm = lTerm \u003e self.Term isCandidate = lTerm == self.Term and self.Role == \"candidate\" if isLargeTerm or isCandidate: self.Role, self.Leader = \"follower\", lId if isCandidate: self.Term, self.votedFor = lTerm, None largerLog = len(self.log) \u003e= followerLogStart # follower不能比較大，不然就沒有更新的意義了 isFreshStart = followerLogStart == 0 isLogStartInSameTerm = followerLogTerm == self.log[followerLogStart-1].term logOK = largerLog and (isFreshStart or isLogStartInSameTerm) if self.Term == lTerm and logOK: self.patchDiff(followerLogStart, lCommitLen, diffLogs) ack = len(diffLogs) + followerLogStart __send(lId, (\"LogResponse\", self.id, self.Term, ack, True)) else: __send(lId, (\"LogResponse\", self.id, self.Term, 0, False)) def patchDiff(self, start, lCommitLen, diff): #shrink log if diff and len(self.log) \u003e start and self.log[start].term != diff[0].term: self.log = self.log[:start] if start+len(diff) \u003e len(self.log): self.log += diff[len(self.log)-start:] if lCommitLen \u003e self.commitLength: for msg,_ in self.log[self.commitLength:lCommitLen]: __deliver(msg) self.commitLength = lCommitLen def when_recv_LogResponse(self, fId, fTerm, ack, good): if self.Term == fTerm and self.Role == \"leader\": if good and ack \u003e= self.ackedLength[fId]: self.sentLength[fId] = self.ackedLength[Utils, CId] = ack self.commitLogEntries() elif self.sentLength[fId] \u003e 0: # 太長啦 self.sentLength[fId] -= 1 self.copyLogTo(fId) elif fTerm \u003e self.Term: self.Term, self.Role, self.votedFor = fTerm, \"follower\", None def commitLogEntries(self): acks = lambda l: len(n for n in nodes if self.ackedLength[n] \u003e= l) newCommitLen = max({l for l in range(1,len(self.log+1)) if acks(l) \u003e= (len(nodes)+1)//2}, default=-1) if newCommitLen \u003e self.commitLength and self.log[newCommitLen-1].term == self.Term: for msg,_ in self.log[self.commitLength:newCommitLen]: __deliver(msg) self.commitLength = newCommitLen Replica consistency 各種情境下的Consistency ACID DB在跑完transaction後會從consistent state到另一個consistent state consistent: satisfying application-specific invariants Read-after-write consistency Replication: 每個replica都要consistent 同樣狀態? 從什麼時候開始算 read都要return一樣的結果 Atomic commit ACID的transaction是 either commits or aborts commit: 是持久的(後面都看的到) abort: 沒有可見的side-effect 所以如果很多DB，也是either commits or aborts Two-phase commit 如果process在等coordinator回commit或是abort之前掛了? 就是等coordinator回來 Fault-tolerant two-phase commit 就是傳commit時會帶所有有關的replica 這樣只要有人發現在有關的replica掛了就可以發abort total order broadcast 之後就是等return ok 都ok，commit 出事，abort Linearizability (strong consistency) 多node的atomic operation 每個operation的return都是最新的結果 not happens-before set之後的get都要能看到set的結果 (Linearizability) client1與client2沒有send/recv (not happens-before) 所以不能用Lamport clocks!! 只能用phy clock!! Operations overlapping in time order沒差，這裡的重點是看的到 Serializability \u0026 Linearizability Linearizability: 都拿到最新的結果 (cache coherense) Serializability: 多個transaction同時跑就像是transaction按照某個順序去跑 (mem model) client2與client3拿到的結果不對 (not Linearizability)!! 手法 get的linearizability quorum read set的linearizability blind write to quorum Linearizable compare-and-swap (CAS) total order broadcast advantages 分散式不像分散式 使用上就變簡單了 Downsides Performance: 很多msg與一直在等 Scalability: 需要leader Availability: 連不到quorum什麼事都不用做 Eventual consistency The CAP theorem 在網路會gg的情況下 (network Partition)，只能保證一個 Consistent (linearizable) 等 Available 不等，直接傳自己的舊資料 只要沒有進一步的update，所有replica都會變成一樣的state Strong eventual consistency Convergence: 只要是同一個state跑同一集合的update(order沒差)，最後會是一樣的狀態 Eventual delivery: 只要有人被update到，最後所有人都會被update到 Properties 不用等 只要Causal broadcast或以下就可以update Concurrent updates =\u003e 只要能處理conflict就沒事 Concurrency control in applications (152) Conflicts due to concurrent updates\n解法 Conflict-free Replicated Data Types (CRDTs)\n就是有timestamp的dict 作法 Operation-based 傳的是action (set) reliable broadcast (一定要到，但可以是任何順序) action(set)一定要commutative typically has smaller messages 例子: Operation-based text CRDT init \u0026 read elementAt就是array[i]，實作在set上 有一段區間，insert就是二分 delete就是把tuple從state中去掉 causal broadcast insert要在delete之前先到 State-based 傳的是state (整個values) best-effort broadcast (不到沒關係) 原本reliable確保一定會到，但現在沒有 Idempotent 剩下就是原本reliable的事 Commutative 不過現在一次多個 (opration based是一次兩個) Associative can tolerate message loss/duplication Operational Transformation (OT)\n把operation記錄下來，之後需要重組可以重新組合 Consistent snapshots\n前面做的事(包含transcation)，後面(包含transcation)看的到 consistent with causality transcation都要consistent with causality linearizability depends on real-time order 把誤差補上 作法: multi-version concurrency control (MVCC) 每一次write都會產生新的版本與相對應的timestamp read-only transcation就是一個時間 read時就是取比自己早且最靠近自己的資料 snapshot of system-wide state\nConsistent cuts 區分event的cut 在cut中的event都是happens-before 除了起點:-)，所以是consistent 也就是，在左手邊的event只有兩種 send recv，同時他的send也在cut(左手邊)中 f只有recv，沒有send在cut中，就不是consistent 收集snapshot of system-wide state 把Consistent cut推到的地方才收集local state local state之後可以集合起來變成global state 怎麼代表cut? 用marker msg以causal order去發 收到marker就收集state 但是當初收到marker的channel不用 因為前一個process已經收集過了 Ref Linearizability versus Serializability Distributed Systems (好地方，如果slide不懂還有lecture note) Lecture 13: Vector clocks, consistent cuts, process groups, and distributed mutual exclusion\n","wordCount":"1550","inLanguage":"en","image":"https://littlebees.github.io/images/papermod-cover.png","datePublished":"2022-01-17T16:17:07Z","dateModified":"2022-01-17T16:17:07Z","author":{"@type":"Person","name":"zhengcf"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://littlebees.github.io/2022/01/distributed-computing/"},"publisher":{"@type":"Organization","name":"記事本","logo":{"@type":"ImageObject","url":"https://littlebees.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://littlebees.github.io/ accesskey=h title="記事本 (Alt + H)">記事本</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://littlebees.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://littlebees.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://littlebees.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://littlebees.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://littlebees.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Distributed computing</h1><div class=post-meta><span title='2022-01-17 16:17:07 +0000 UTC'>January 17, 2022</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;zhengcf</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%8b%95%e6%a9%9f aria-label=動機>動機</a></li><li><a href=#models-of-distributed-systems aria-label="Models of distributed systems">Models of distributed systems</a></li><li><a href=#time-clocks-and-ordering-of-events aria-label="Time, clocks, and ordering of events">Time, clocks, and ordering of events</a></li><li><a href=#broadcast-protocols-and-logical-time aria-label="Broadcast protocols and logical time">Broadcast protocols and logical time</a></li><li><a href=#replication aria-label=Replication>Replication</a></li><li><a href=#consensus aria-label=Consensus>Consensus</a></li><li><a href=#replica-consistency aria-label="Replica consistency">Replica consistency</a></li><li><a href=#concurrency-control-in-applications-152 aria-label="Concurrency control in applications (152)">Concurrency control in applications (152)</a></li><li><a href=#ref aria-label=Ref>Ref</a></li></ul></div></details></div><div class=post-content><h2 id=動機>動機<a hidden class=anchor aria-hidden=true href=#動機>#</a></h2><p>論選好課、書的重要性</p><h2 id=models-of-distributed-systems>Models of distributed systems<a hidden class=anchor aria-hidden=true href=#models-of-distributed-systems>#</a></h2><p><img loading=lazy src=https://i.imgur.com/DOj27J0.png alt></p><p><img loading=lazy src=https://i.imgur.com/kAes3y7.png alt>
<img loading=lazy src=https://i.imgur.com/pUbx57g.png alt></p><p>怎麼處理上面會騙人的狀況?
只有兩人無法判斷對錯，所以如果有一個壞人，就要有兩個好人，才能抵銷</p><p><img loading=lazy src=https://i.imgur.com/ZgnstJj.png alt></p><p>剛剛的兩個問題都有兩項東西</p><ul><li>Node</li><li>Network</li></ul><p>還有另一個要考慮的是 latency!!</p><ul><li><p>system model是由下面3項組成</p><ul><li>Network behaviour (e.g. message loss)<ul><li>(assume) bidirectional point-to-point communication<ul><li>Reliable (perfect) links<ul><li>如果收到msg，就代表他有被送出</li><li>可能重排<ul><li>Fair-loss links + retry&消除重複</li></ul></li></ul></li><li>Fair-loss links<ul><li>msg可能消失、重複、重排</li><li>但一直try總會到對面去<ul><li>Arbitrary links + TLS</li></ul></li></ul></li><li>Arbitrary links<ul><li>中間有不懷好意的人</li></ul></li></ul></li><li>Network partition<ul><li>線路會drop或是delay</li></ul></li></ul></li><li>Node behaviour (e.g. crashes)<ul><li>如果出事(faulty)的話<ul><li>Crash-stop (fail-stop)<ul><li>crash就直接停下來</li></ul></li><li>Crash-recovery (fail-recovery)<ul><li>crash會失去mem中的狀態</li><li>在一段時間之後會自己復原</li></ul></li><li>Byzantine (fail-arbitrary)<ul><li>crash後什麼事都有可能發生</li></ul></li></ul></li></ul></li><li>Timing behaviour (e.g. latency)<ul><li>Node & Network<ul><li>Synchronous<ul><li>latency有上限</li><li>node的執行速度是可以預估的</li></ul></li><li>Partially synchronous<ul><li>有些部分是async的(會結束，但不知道多久)</li><li>其他都是sync</li></ul></li><li>Asynchronous<ul><li>latency不確定</li><li>node也不確定會不會隨時停下</li></ul></li></ul></li><li>Node的意外<ul><li>Operating system scheduling (priority inversion)</li><li>Stop-the-world garbage collection</li><li>Page faults, swap, thrashing</li></ul></li><li>Network的意外<ul><li>Message loss requiring retry</li><li>Congestion/contention causing queueing</li><li>Network/route reconfiguration</li></ul></li></ul></li></ul></li><li><p>Failure: system完全不動</p></li><li><p>Fault: 一部份不動了</p><ul><li>node<ul><li>crash (crash-stop/crash-recovery)</li><li>deviating from algorithm (Byzantine)</li></ul></li><li>Network<ul><li>dropping or significantly delaying messages</li></ul></li></ul></li><li><p>Failure detectors</p><ul><li>Perfect failure detector<ul><li>labels a node as faulty iff it has crashed</li></ul></li><li>Typical implementation<ul><li>send message</li><li>await response</li><li>label node as crashed if no reply within some timeout</li></ul></li><li>Problem<ul><li>不能區分是<ul><li>crashed node</li><li>temporarily unresponsive node</li><li>lost message</li><li>delayed message</li></ul></li><li>所以只能在<ul><li>synchronous crash-stop system with reliable links</li><li>Eventually perfect failure detector<ul><li>temporarily<ul><li>標成 crashed, 就算是 correct</li><li>標成 correct, 就算是 crashed</li></ul></li><li>eventually<ul><li>標成 crashed, iff crashed</li></ul></li></ul></li></ul></li></ul></li></ul></li></ul><h2 id=time-clocks-and-ordering-of-events>Time, clocks, and ordering of events<a hidden class=anchor aria-hidden=true href=#time-clocks-and-ordering-of-events>#</a></h2><ul><li>two types of clock<ul><li>physical clocks<ul><li>就是時間，數花掉的時間</li><li>會有誤差 (可能在調整時變大變小)<ul><li>Coordinated Universal Time (UTC)<ul><li>UTC 是由 TAI 配合地球自轉速度做修正<ul><li>International Atomic Time(TAI): from 原子鐘</li><li>地球自轉速度不是常數</li></ul></li></ul></li><li>Leap seconds<ul><li>在 on 30 June and 31 December at 23:59:59 UTC<ul><li>negative leap second: 直接跳過去</li><li>positive leap second: 跳到 23:59:60等1秒，再跳過去</li></ul></li></ul></li><li>timestamps格式<ul><li>Unix time: number of seconds since 1 January 1970 00:00:00 UTC<ul><li>不算Leap seconds</li></ul></li><li>ISO 8601: year, month, day, hour, minute, second, and timezone offset relative to UTC<ul><li>2020-11-09T09:50:17+00:00</li></ul></li></ul></li><li>NTP<ul><li>Clock skew: 兩個clock的差</li><li>作法<ul><li><p>對多台server取樣，跑算式</p><ul><li><img loading=lazy src=https://i.imgur.com/WPhU2qf.png alt></li></ul></li><li><p>開始調時間</p><ul><li><img loading=lazy src=https://i.imgur.com/5Z3HV4j.png alt></li></ul></li></ul></li><li><img loading=lazy src=https://i.imgur.com/KVvBgul.png alt><ul><li>Time-of-day clock<ul><li>從某個時間點開始</li><li>適合比較</li><li>可能會變動 (NTP矯正)</li></ul></li><li>Monotonic clock<ul><li>隨便一個點開始</li><li>適合計算花費的時間</li></ul></li></ul></li></ul></li></ul></li></ul></li><li>logical clocks<ul><li>用來數有多少事件</li><li>單調遞增</li><li>happens-before relation (就是有向圖的path)<ul><li>定義<ul><li>同一個process<ul><li>a發生在b之前</li></ul></li><li>不同process<ul><li>a送msg，b收到</li></ul></li><li>遞移律<ul><li>a -> b -> c: a -> c</li></ul></li></ul></li><li>就是有向圖的path<ul><li>partial order</li></ul></li><li>concurrent: 不是 a -> b 或 b -> a<ul><li>a || b</li></ul></li><li><img loading=lazy src=https://i.imgur.com/VNv4Siy.png alt></li></ul></li><li>Causality<ul><li>concurrent代表兩者一定沒有因果關係</li><li>有hb就是可能有因果關係</li><li>定義<ul><li>a strict total order on events</li><li><img loading=lazy src=https://i.imgur.com/lKzhUJo.png alt></li></ul></li></ul></li></ul></li></ul></li></ul><h2 id=broadcast-protocols-and-logical-time>Broadcast protocols and logical time<a hidden class=anchor aria-hidden=true href=#broadcast-protocols-and-logical-time>#</a></h2><ul><li><p>capture causal dependencies</p><ul><li>No Physical timestamps!!</li><li>two types of logical clocks<ul><li>Lamport clocks<ul><li>作法<ul><li>每個process init自己的時間<ul><li><code>t = 0</code></li></ul></li><li>有event發生<ul><li><code>t++</code></li></ul></li><li>send某個m<ul><li><code>t++</code></li><li><code>send((t, m))</code></li></ul></li><li>recv到某個(tx, m)<ul><li><code>t = max(tx, t) + 1</code></li><li>對m做事</li></ul></li></ul></li><li>Properties<ul><li>L是取t的函數、N是取在哪個process的函數</li><li><img loading=lazy src=https://i.imgur.com/6J6DSfc.png alt></li></ul></li><li>the pair (L(e), N(e)) <strong>uniquely identifies</strong> event e</li><li>用Lamport clocks定義total order<ul><li><img loading=lazy src=https://i.imgur.com/kAfy95j.png alt></li></ul></li><li>BUT<ul><li>不同process的timestamp可能會一樣!!<ul><li>不能分辨 a -> b 或是 a || b<ul><li>因為把自己與其他人的時間混在一起了</li></ul></li></ul></li></ul></li></ul></li><li>Vector clocks<ul><li>作法<ul><li>每個process(假設叫i) init自己的時間<ul><li><code>t = [0] * len(procs)</code></li></ul></li><li>有event發生<ul><li><code>t[i]++</code></li></ul></li><li>send某個m<ul><li><code>t[i]++</code></li><li><code>send((t, m))</code></li></ul></li><li>recv到某個(tx, m)<ul><li><code>t = [max(t[j], tx[j]) if j != i else t[i]+1 for j in range(len(procs))]</code></li><li>對m做事</li></ul></li></ul></li><li>用Vector clocks定義total order<ul><li><img loading=lazy src=https://i.imgur.com/2fE1ov3.png alt></li><li><img loading=lazy src=https://i.imgur.com/5gQOlhZ.png alt></li></ul></li></ul></li></ul></li></ul></li><li><p>Broadcast protocols</p><ul><li><img loading=lazy src=https://i.imgur.com/qWH3jVT.png alt></li><li>broadcast的種類<ul><li>FIFO<ul><li>從同一個process出來的msg，收到msg的順序，與送的順序一樣</li><li><img loading=lazy src=https://i.imgur.com/EWegnFj.png alt><ul><li>vaild order: (m1一定在m3前面，其他隨便)<ul><li>(m2, m1, m3)</li><li>(m1, m2, m3)</li><li>(m1, m3, m2)</li></ul></li></ul></li></ul></li><li>Causal<ul><li>broadcast(m1) -> broadcast(m2)，那m1一定先收到，之後才是m2</li><li><img loading=lazy src=https://i.imgur.com/DG1yL5w.png alt><ul><li>vaild order: (m1 -> m2，m1 -> m3)<ul><li>(m1, m2, m3)</li><li>(m1, m3, m2)</li></ul></li></ul></li></ul></li><li>Total order<ul><li>只要m1先收到，在所有process都會是m1先收到</li><li><img loading=lazy src=https://i.imgur.com/kZF7Dxa.png alt><ul><li>vaild order:<ul><li>(m1, m2, m3)</li></ul></li></ul></li><li><img loading=lazy src=https://i.imgur.com/MNYzEK2.png alt><ul><li>vaild order:<ul><li>(m1, m3, m2)</li></ul></li></ul></li></ul></li><li>FIFO-total order<ul><li>FIFO + Total order</li></ul></li><li><img loading=lazy src=https://i.imgur.com/pJPOCMv.png alt></li></ul></li><li>Broadcast algorithms<ul><li>要處理兩個部分<ul><li>把best-effort broadcast變成reliable<ul><li>利用retransmitting</li></ul></li><li>保持送(收)的順序</li></ul></li><li>Not Reliable (Naive)<ul><li>直接送到process去</li><li>Problem<ul><li>送的process中間掛掉怎麼辦</li></ul></li></ul></li><li>Reliable<ul><li>Eager reliable broadcast<ul><li>process只要是<strong>第一次</strong>收到時就re-boardcast!!<ul><li>到所有process</li></ul></li><li>Problem<ul><li>總共有O(n^2)個msg在流動!!</li></ul></li></ul></li><li>Gossip protocols<ul><li>process只要是<strong>第一次</strong>收到時就re-boardcast!!<ul><li>到3個process (隨機選)</li></ul></li><li>適合用在process很多時</li><li>Problem<ul><li><strong>Eventually</strong> reaches all nodes (with high probability)</li></ul></li></ul></li></ul></li></ul></li><li>FIFO broadcast algorithm<ul><li><img loading=lazy src=https://i.imgur.com/pLv6p3a.png alt></li><li>重點是在buffer中找有對到自己的delivered的msg才做下一步動作<ul><li>這樣就是FIFO，從同一個process出來的msg的順序是對的</li></ul></li></ul></li><li>Causal broadcast algorithm<ul><li><img loading=lazy src=https://i.imgur.com/AKXTyBE.png alt></li><li>延伸FIFO broadcast algorithm，但<ul><li>改成vector clock!! (這樣就完成CO了!!)</li></ul></li></ul></li><li>Total order broadcast algorithms<ol><li>Single leader (轉成BFS tree)<ul><li><img loading=lazy src=https://i.imgur.com/jBbn5rf.png alt></li><li>會有單點失敗!!</li></ul></li><li>Lamport clocks (用Lamport clocks排序)<ul><li><img loading=lazy src=https://i.imgur.com/BqJpIWH.png alt></li><li>怎麼確定我現在拿到的msg是最小的?<ul><li>要等到<strong>每個</strong>process的msg的stamp都比較大才會知道</li></ul></li></ul></li></ol></li></ul></li></ul><h2 id=replication>Replication<a hidden class=anchor aria-hidden=true href=#replication>#</a></h2><ul><li>Use best-effort broadcast<ul><li><p>Idempotence</p><ul><li>多次操作後不用dedup的操作</li><li>retry behaviour<ul><li>At-most-once<ul><li>不retry</li></ul></li><li>At-least-once<ul><li>Retry到收到ack</li></ul></li><li>Exactly-once<ul><li>Retry +<ul><li>idempotence</li><li>deduplication</li></ul></li></ul></li></ul></li><li>就算是idempotence，還是有影響到狀態<ul><li>所以下面的client2到了最後沒辦法看到移除了client1 add的資料的狀態</li><li><img loading=lazy src=https://i.imgur.com/7u6ScII.png alt></li></ul></li></ul></li><li><p>Timestamps and tombstones (soft delete)</p><ul><li><p><img loading=lazy src=https://i.imgur.com/2x26pwz.png alt></p></li><li><p>資料會放一個flag，標有沒有被刪過</p><ul><li><img loading=lazy src=https://i.imgur.com/aajmr03.png alt></li></ul></li><li><p>Reconciling replicas</p><ul><li>還會放一個timestamp標什麼時候被寫入</li><li><img loading=lazy src=https://i.imgur.com/d9g7UZu.png alt></li></ul></li><li><p>Concurrent writes by different clients</p><ul><li><img loading=lazy src=https://i.imgur.com/VUk3uP8.png alt></li><li>兩種做法<ul><li>Last writer wins<ul><li>取timestamp最大的<ul><li>total order (e.g. Lamport clock)</li></ul></li><li>注意 data loss</li></ul></li><li>Multi-value register<ul><li>如果可以比，取最大；不能比，都存<ul><li>partial order (e.g. vector clock)</li></ul></li></ul></li></ul></li></ul></li></ul></li><li><p>Quorum (Byzantine problem)</p><ul><li>Read-after-write consistency<ul><li><img loading=lazy src=https://i.imgur.com/R1jU3Ii.png alt></li></ul></li><li>只要read(read quorum)/write(write quorum)的response有到達指定人數就取這個結果<ul><li><code>read quorum + write quorum > nodes</code><ul><li>一般取，<code>(nodes+1)/2</code></li><li>可以在write時忍受<code>nodes-write quorum</code>壞掉，read是<code>nodes-read quorum</code></li><li><img loading=lazy src=https://i.imgur.com/OhWk7O6.png alt></li></ul></li></ul></li></ul></li></ul></li><li>Use Total order broadcast (在每個process中msg的順序都是一樣的!!)<ul><li>State machine replication<ul><li>FIFO total order broadcast送update msg: 一定會到、順序一樣<ul><li>能不能用更弱的broadcast<ul><li>這樣就沒有順序的保證了!!<ul><li>但是update的順序如果不影響最後的結果的話<ul><li>commutative: <code>f(g(x)) = g(f(x))</code></li></ul></li></ul></li></ul></li></ul></li><li>所以可以把update msg當成map，Replica當成DFA<ul><li>same input, same output: deterministic</li></ul></li><li>限制<ul><li>不能馬上更新，要等msg傳遞</li><li>需要 fault-tolerant total order broadcast</li></ul></li></ul></li></ul></li><li><img loading=lazy src=https://i.imgur.com/lPMblfp.png alt></li></ul><h2 id=consensus>Consensus<a hidden class=anchor aria-hidden=true href=#consensus>#</a></h2><ul><li>Fault-tolerant total order broadcast<ul><li>total order broadcast一定要leader!!<ul><li>leader壞了怎麼辦?<ul><li>自己選一個<ul><li>用failure detector (timeout)看leader壞了沒<ul><li>壞了就選下一個</li><li>確保只有一個leader<ul><li>用term區份這個任期中誰是該區市民與leader<ul><li>需要定義Quorum (過 半+1)</li><li>每個process在每個任期中最多只能投一次票</li><li><img loading=lazy src=https://i.imgur.com/lJkuN6R.png alt></li></ul></li><li>這樣可以確保一個任期只有一個leader<ul><li><img loading=lazy src=https://i.imgur.com/19CVQAL.png alt></li><li>leader在傳msg之前都要ack，不然怕場面尷尬</li><li><img loading=lazy src=https://i.imgur.com/TbniB5b.png alt></li></ul></li></ul></li></ul></li></ul></li></ul></li></ul></li><li>Consensus and total order broadcast<ul><li>Consensus: 大家都同意某一個值<ul><li>total order broadcast: 大家都同意下一個msg要送什麼</li></ul></li><li>Consensus and total order broadcast are formally equivalent</li></ul></li><li>Common consensus algorithms: Paxos, Multi-Paxos, Raft</li></ul></li><li>Distributed mutual exclusion<ul><li>作法<ul><li>central lock server<ul><li>leader是bottleneck</li><li>單點失敗<ul><li>怎麼重選leader</li></ul></li></ul></li><li>token passing<ul><li>用一個token去傳(整個要連成一個ring)，拿到就當成拿到lock</li><li>單點失敗<ul><li>怎麼rebuild ring</li><li>怎麼重生token</li></ul></li></ul></li><li>Totally ordered multicast<ul><li>只有一個人held，所以讓所有人投票，要拿到N-1<ul><li>raft是(N+1)/2</li></ul></li><li>concurrent requests (兩個以上want lock)<ul><li><img loading=lazy src=https://i.imgur.com/zux8dtF.png alt></li><li>看pid比大小<ul><li>哲學家用餐問題!!</li></ul></li></ul></li></ul></li></ul></li></ul></li><li>Consensus system models<ul><li>假設system model是<ul><li>partially synchronous<ul><li>not asynchronous (FLP result)<ul><li>in an asynchronous crash-stop system model<ul><li>no deterministic consensus algorithm that is guaranteed to terminate</li></ul></li></ul></li><li>use clocks only used<ul><li>for timeouts/failure detector<ul><li>to ensure progress</li></ul></li><li>not Safety (correctness)</li></ul></li></ul></li><li>crash-recovery</li></ul></li></ul></li><li>Raft<ul><li>state(腳色) 變化<ul><li><img loading=lazy src=https://i.imgur.com/oY41Fk7.png alt></li></ul></li><li>元件<ul><li>log: leader傳過的msg 或是 follower收到的msg (array)<ul><li>msg</li><li>term: 傳msg當時的term</li></ul></li><li>term: 任期<ul><li>主要是看這個</li></ul></li><li>sentLength: leader傳了多長的log給follower</li><li>ackedLength: follower回報他們的log多長</li><li>commitLength: 真的有deliver的有多少</li></ul></li></ul></li></ul><pre tabindex=0><code class="language-python=" data-lang="python=">class Node:
    def __init__(self):
        self.init_runtime_state()
        self.Term = 0
        self.votedFor = None
        self.log = []
        self.commitLength = 0
        self.id = &#34;whatever&#34;
        
    def init_runtime_state(self):
        self.Role = &#34;follower&#34;
        self.Leader = None
        self.votesReceived = set()
        self.sentLength = [0]*len(nodes)
        self.ackedLength = [0]*len(nodes)

    def when_leader_fail_OR_election_timeout(self):
        self.Term += 1
        self.Role = &#34;candidate&#34;
        self.votedFor = self.id
        self.votesReceived.add(self.id)
        lastTerm = self.log[-1].term if len(self.log) &gt; 0 else 0
        for n in nodes:
            __send(n, (&#34;VoteRequest&#34;, self.id, self.Term, len(self.log), lastTerm))
        __startElectionTimer()
    
    def when_recv_VoteRequest(self, cId, cTerm, CLogLen, cLogTerm):
        myLogTerm = self.log[-1].term
        
        isLargeLogTerm = cLogTerm &gt; myLogTerm
        inSameLogTerm = myLogTerm == myLogTerm
        hasMoreLog = CLogLen &gt;= len(self.log)
        logOK = isLargeLogTerm or (inSameLogTerm and hasMoreLog)
        
        isLargeTerm = cTerm &gt; self.Term
        inSameTerm = cTerm == self.Term
        notVoted_OR_voteSame = self.votedFor in {cId, None}
        termOK = isLargeTerm or (inSameTerm and notVoted_OR_voteSame)
        
        if logOK and termOK:
            self.Term, self.Role, self.votedFor = cTerm, &#34;follower&#34;, cId
            __send(cId, (&#34;VoteResponse&#34;, self.id, self.Term, True))
        else:
            __send(cId, (&#34;VoteResponse&#34;, self.id, self.Term, False))
    
    def when_recv_VoteResponse(self, vId, vTerm, isAgree):
        if self.Role == &#34;candidate&#34; and self.Term == vTerm and isAgree:
            self.votesReceived.add(vId)
            if len(self.votesReceived) &gt;= (len(nodes)+1)//2:
                self.Role, self.Leader = &#34;leader&#34;, self.id
                __stopElectionTimer()
                for n in {n in nodes if n is not self}:
                    self.sentLength[n], self.ackedLength[n] = len(self.log), 0
                    self.copyLogTo(n)
        elif vTerm &gt; self.term:
            self.Role, self.Term, self.votedFor = &#34;follower&#34;, vTerm, None
            __stopElectionTimer()

    def broadcast(self, msg):
        if self.Role == &#34;leader&#34;:
            self.log.append((msg, self.Term))
            self.ackedLength[self.id] = len(self.log)
            for n in {n in nodes if n is not self}:
                self.copyLogTo(n)
        else:
             __forwarding_to_leader(msg)
    def periodically_do(self):
        if self.Role == &#34;leader&#34;:
            for n in {n in nodes if n is not self}:
                self.copyLogTo(n)
    def copyLogTo(self, n):
        i = self.sentLength[n]
        diffLogs = self.log[i:]
        prevFollowerTerm = self.log[max(0, i-1)]
        __send(n, (&#34;LogRequest&#34;, self.id, self.Term, i, prevFollowerTerm, self.commitLength, diffLogs))
    
    def when_recv_LogRequest(self, lId, lTerm, followerLogStart, followerLogTerm, lCommitLen, diffLogs):
        isLargeTerm = lTerm &gt; self.Term
        isCandidate = lTerm == self.Term and self.Role == &#34;candidate&#34;
        if isLargeTerm or isCandidate:
            self.Role, self.Leader = &#34;follower&#34;, lId
            if isCandidate:
                self.Term, self.votedFor = lTerm, None
        
        largerLog = len(self.log) &gt;= followerLogStart # follower不能比較大，不然就沒有更新的意義了
        isFreshStart = followerLogStart == 0
        isLogStartInSameTerm = followerLogTerm == self.log[followerLogStart-1].term
        logOK = largerLog and (isFreshStart or isLogStartInSameTerm)
        
        if self.Term == lTerm and logOK:
            self.patchDiff(followerLogStart, lCommitLen, diffLogs)
            ack = len(diffLogs) + followerLogStart
            __send(lId, (&#34;LogResponse&#34;, self.id, self.Term, ack, True))
        else:
            __send(lId, (&#34;LogResponse&#34;, self.id, self.Term, 0, False))
    def patchDiff(self, start, lCommitLen, diff):
        #shrink log
        if diff and len(self.log) &gt; start and self.log[start].term != diff[0].term:
            self.log = self.log[:start]
        
        if start+len(diff) &gt; len(self.log):
            self.log += diff[len(self.log)-start:]
        
        if lCommitLen &gt; self.commitLength:
            for msg,_ in self.log[self.commitLength:lCommitLen]:
                __deliver(msg)
            self.commitLength = lCommitLen
    
    def when_recv_LogResponse(self, fId, fTerm, ack, good):
        if self.Term == fTerm and self.Role == &#34;leader&#34;:
            if good and ack &gt;= self.ackedLength[fId]:
                self.sentLength[fId] = self.ackedLength[Utils, CId] = ack
                self.commitLogEntries()
            elif self.sentLength[fId] &gt; 0:
                # 太長啦
                self.sentLength[fId] -= 1
                self.copyLogTo(fId)
        elif fTerm &gt; self.Term:
            self.Term, self.Role, self.votedFor = fTerm, &#34;follower&#34;, None
    
    def commitLogEntries(self):
        acks = lambda l: len(n for n in nodes if self.ackedLength[n] &gt;= l)
        newCommitLen = max({l for l in range(1,len(self.log+1)) if acks(l) &gt;= (len(nodes)+1)//2}, default=-1)
        
        if newCommitLen &gt; self.commitLength and self.log[newCommitLen-1].term == self.Term:
            for msg,_ in self.log[self.commitLength:newCommitLen]:
                __deliver(msg)
            self.commitLength = newCommitLen
</code></pre><h2 id=replica-consistency>Replica consistency<a hidden class=anchor aria-hidden=true href=#replica-consistency>#</a></h2><ul><li>各種情境下的Consistency<ul><li>ACID<ul><li>DB在跑完transaction後會從consistent state到另一個consistent state<ul><li>consistent: satisfying application-specific invariants</li></ul></li></ul></li><li>Read-after-write consistency</li><li>Replication: 每個replica都要consistent<ul><li>同樣狀態? 從什麼時候開始算</li><li>read都要return一樣的結果</li></ul></li></ul></li><li>Atomic commit<ul><li>ACID的transaction是<ul><li>either <strong>commits</strong> or <strong>aborts</strong><ul><li>commit: 是持久的(後面都看的到)</li><li>abort: 沒有可見的side-effect</li></ul></li><li>所以如果很多DB，也是either <strong>commits</strong> or <strong>aborts</strong></li></ul></li><li><img loading=lazy src=https://i.imgur.com/11w62Ts.png alt></li><li>Two-phase commit<ul><li><img loading=lazy src=https://i.imgur.com/Z35Dk17.png alt><ul><li>如果process在等coordinator回commit或是abort之前掛了?<ul><li>就是等coordinator回來</li></ul></li></ul></li><li>Fault-tolerant two-phase commit<ul><li>就是傳commit時會帶所有有關的replica<ul><li>這樣只要有人發現在有關的replica掛了就可以發abort<ul><li>total order broadcast</li></ul></li></ul></li><li>之後就是等return ok<ul><li>都ok，commit</li><li>出事，abort</li></ul></li><li><img loading=lazy src=https://i.imgur.com/XQcTkSw.png alt></li><li><img loading=lazy src=https://i.imgur.com/iYdUDV0.png alt></li></ul></li></ul></li></ul></li><li>Linearizability (strong consistency)<ul><li>多node的atomic operation<ul><li>每個operation的return都是最新的結果</li></ul></li><li>not happens-before<ul><li><img loading=lazy src=https://i.imgur.com/CpAjww1.png alt></li><li>set之後的get都要能看到set的結果 (Linearizability)</li><li>client1與client2沒有send/recv (not happens-before)<ul><li>所以不能用Lamport clocks!!</li><li>只能用phy clock!!</li></ul></li><li>Operations overlapping in time<ul><li>order沒差，這裡的重點是看的到</li></ul></li></ul></li><li>Serializability & Linearizability<ul><li>Linearizability: 都拿到最新的結果 (cache coherense)</li><li>Serializability: 多個transaction同時跑就像是transaction按照某個順序去跑 (mem model)</li></ul></li><li><img loading=lazy src=https://i.imgur.com/OeBQ63c.png alt><ul><li>client2與client3拿到的結果不對 (not Linearizability)!!</li></ul></li><li><img loading=lazy src=https://i.imgur.com/Lj5OqBj.png alt><ul><li>手法<ul><li>get的linearizability<ul><li>quorum read</li></ul></li><li>set的linearizability<ul><li>blind write to quorum</li></ul></li></ul></li></ul></li><li>Linearizable compare-and-swap (CAS)<ul><li>total order broadcast</li><li><img loading=lazy src=https://i.imgur.com/Gbnhidk.png alt></li></ul></li><li>advantages<ul><li>分散式不像分散式</li><li>使用上就變簡單了</li></ul></li><li>Downsides<ul><li>Performance: 很多msg與一直在等</li><li>Scalability: 需要leader</li><li>Availability: 連不到quorum什麼事都不用做</li></ul></li></ul></li><li>Eventual consistency<ul><li>The CAP theorem<ul><li>在網路會gg的情況下 (network <strong>P</strong>artition)，只能保證一個<ul><li><strong>C</strong>onsistent (linearizable)<ul><li>等</li></ul></li><li><strong>A</strong>vailable<ul><li>不等，直接傳自己的舊資料</li></ul></li></ul></li></ul></li><li>只要沒有進一步的update，所有replica都會變成一樣的state</li><li>Strong eventual consistency<ul><li>Convergence: 只要是同一個state跑同一集合的update(order沒差)，最後會是一樣的狀態</li><li>Eventual delivery: 只要有人被update到，最後所有人都會被update到</li></ul></li><li>Properties<ul><li>不用等</li><li>只要Causal broadcast或以下就可以update</li><li>Concurrent updates => 只要能處理conflict就沒事</li></ul></li></ul></li></ul><p><img loading=lazy src=https://i.imgur.com/U8744mn.png alt></p><h2 id=concurrency-control-in-applications-152>Concurrency control in applications (152)<a hidden class=anchor aria-hidden=true href=#concurrency-control-in-applications-152>#</a></h2><ul><li><p>Conflicts due to concurrent updates</p><ul><li><img loading=lazy src=https://i.imgur.com/pbm0oxw.png alt></li><li>解法<ul><li><p>Conflict-free Replicated Data Types (CRDTs)</p><ul><li>就是有timestamp的dict<ul><li><img loading=lazy src=https://i.imgur.com/oeUDKYv.png alt></li></ul></li><li>作法<ul><li>Operation-based<ul><li>傳的是action (set)<ul><li>reliable broadcast (一定要到，但可以是任何順序)<ul><li>action(set)一定要commutative</li></ul></li></ul></li><li>typically has smaller messages</li><li><img loading=lazy src=https://i.imgur.com/CZpjVYN.png alt></li><li>例子: Operation-based text CRDT<ul><li><img loading=lazy src=https://i.imgur.com/8WnwAfe.png alt></li><li>init & read<ul><li><img loading=lazy src=https://i.imgur.com/hHiFnJE.png alt><ul><li>elementAt就是<code>array[i]</code>，實作在set上</li></ul></li></ul></li><li>有一段區間，insert就是二分<ul><li><img loading=lazy src=https://i.imgur.com/wElkybh.png alt></li><li><img loading=lazy src=https://i.imgur.com/T3DpsNF.png alt></li></ul></li><li>delete就是把tuple從state中去掉<ul><li><img loading=lazy src=https://i.imgur.com/0TRFMd4.png alt></li></ul></li><li>causal broadcast<ul><li>insert要在delete之前先到</li></ul></li></ul></li></ul></li><li>State-based<ul><li>傳的是state (整個values)<ul><li>best-effort broadcast (不到沒關係)<ul><li>原本reliable確保一定會到，但現在沒有<ul><li>Idempotent</li></ul></li><li>剩下就是原本reliable的事<ul><li>Commutative</li><li>不過現在一次多個 (opration based是一次兩個)<ul><li>Associative</li></ul></li></ul></li></ul></li></ul></li><li>can tolerate message loss/duplication</li><li><img loading=lazy src=https://i.imgur.com/Wwrf5YJ.png alt><ul><li><img loading=lazy src=https://i.imgur.com/11XKqz2.png alt></li></ul></li></ul></li></ul></li></ul></li><li><p>Operational Transformation (OT)</p><ul><li>把operation記錄下來，之後需要重組可以重新組合<ul><li><img loading=lazy src=https://i.imgur.com/tVoFVQi.png alt></li></ul></li></ul></li></ul></li></ul></li><li><p>Consistent snapshots</p><ul><li>前面做的事(包含transcation)，後面(包含transcation)看的到<ul><li>consistent with causality<ul><li>transcation都要consistent with causality<ul><li>linearizability depends on real-time order<ul><li><img loading=lazy src=https://i.imgur.com/4euc5IX.png alt><ul><li>把誤差補上</li></ul></li></ul></li></ul></li></ul></li></ul></li><li>作法: multi-version concurrency control (MVCC)<ul><li>每一次write都會產生新的版本與相對應的timestamp</li><li>read-only transcation就是一個時間<ul><li>read時就是取比自己早且最靠近自己的資料</li></ul></li></ul></li></ul></li><li><p>snapshot of system-wide state</p><ul><li>Consistent cuts<ul><li>區分event的cut<ul><li>在cut中的event都是happens-before<ul><li>除了起點<code>:-)</code>，所以是consistent</li></ul></li></ul></li><li>也就是，在左手邊的event只有兩種<ul><li>send</li><li>recv，同時他的send也在cut(左手邊)中</li></ul></li><li><img loading=lazy src=https://i.imgur.com/cgC72Fo.png alt><ul><li>f只有recv，沒有send在cut中，就不是consistent</li></ul></li></ul></li><li>收集snapshot of system-wide state<ul><li>把Consistent cut推到的地方才收集local state<ul><li>local state之後可以集合起來變成global state</li></ul></li><li>怎麼代表cut?<ul><li>用marker msg以causal order去發<ul><li>收到marker就收集state<ul><li>但是當初收到marker的channel不用<ul><li>因為前一個process已經收集過了</li></ul></li></ul></li></ul></li></ul></li></ul></li></ul></li></ul><h2 id=ref>Ref<a hidden class=anchor aria-hidden=true href=#ref>#</a></h2><p><a href=https://www.jianshu.com/p/76ff7718d381>Linearizability versus Serializability</a>
<a href=https://www.cl.cam.ac.uk/teaching/2021/ConcDisSys/dist-sys-handout.pdf>Distributed Systems (好地方，如果slide不懂還有lecture note)</a>
<a href=https://www.cl.cam.ac.uk/teaching/1819/ConcDisSys/2018-CDS-1B-L13-rmm.pdf>Lecture 13: Vector clocks, consistent cuts, process groups, and distributed mutual exclusion</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://littlebees.github.io/tags/distributed/>Distributed</a></li></ul><nav class=paginav><a class=prev href=https://littlebees.github.io/2022/01/database-system/><span class=title>« Prev</span><br><span>database system</span>
</a><a class=next href=https://littlebees.github.io/2022/01/k8s%E6%A6%82%E5%BF%B5/><span class=title>Next »</span><br><span>k8s概念</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://littlebees.github.io/>記事本</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>