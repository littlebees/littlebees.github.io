<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>queuing thoery的tutorial | 記事本</title>
<meta name=keywords content="Performance"><meta name=description content="動機
補完當年的queuing thoery"><meta name=author content="zhengcf"><link rel=canonical href=https://littlebees.github.io/2022/01/queuing-thoery%E7%9A%84tutorial/><link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=https://littlebees.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://littlebees.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://littlebees.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://littlebees.github.io/apple-touch-icon.png><link rel=mask-icon href=https://littlebees.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://littlebees.github.io/2022/01/queuing-thoery%E7%9A%84tutorial/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="queuing thoery的tutorial"><meta property="og:description" content="動機
補完當年的queuing thoery"><meta property="og:type" content="article"><meta property="og:url" content="https://littlebees.github.io/2022/01/queuing-thoery%E7%9A%84tutorial/"><meta property="og:image" content="https://littlebees.github.io/images/papermod-cover.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-01-17T10:26:57+00:00"><meta property="article:modified_time" content="2022-01-17T10:26:57+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://littlebees.github.io/images/papermod-cover.png"><meta name=twitter:title content="queuing thoery的tutorial"><meta name=twitter:description content="動機
補完當年的queuing thoery"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://littlebees.github.io/posts/"},{"@type":"ListItem","position":2,"name":"queuing thoery的tutorial","item":"https://littlebees.github.io/2022/01/queuing-thoery%E7%9A%84tutorial/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"queuing thoery的tutorial","name":"queuing thoery的tutorial","description":"動機 補完當年的queuing thoery\n","keywords":["Performance"],"articleBody":"動機 補完當年的queuing thoery\n一言以蔽之，這到底在幹嘛 替queue建模 用random variable說話(設計每個queue的部分) 其實queuing thoery應該要另外開一堂專門介紹怎麼操作random variable才對 複習機率 Random variable到底是什麼 高級的random，可以生出對應的output。 可以把Random variable當成疊加態，等到去用時才會確定一個值，所以可以對這個RV算機率、期望值等等性質\n連續與離散 連續與離散就是看這個高級的random丟出來的值的range是可數還是不可數\n同時連續與離散的RV也有不同的術語\n連續 Cumulative Distribution Function pdf的積分，可以得到小於等於某個值的機率 Probability density function cdf的微分，就是圖上的那條線 (不是機率!!) 因為pdf的output可能超過1 PDF is not a probability. 離散 probability mass function 就是機率 Cumulative Distribution Function pmf的總和，可以得到小於等於某個值的機率 期望數 \u0026 變異數 期望數: output與pdf/pmf的加權總合 E(aY+bX) = aE(Y) + bE(X) E(g(X)) = sum(g(x)*pmf_or_pdf(x)) 這個很重要，之後會對X上一堆函數，需要這個才知道發生什麼事 如果有獨立的話，E(X \u0026 Y) = E(X) * E(Y) 這個也很重要 變異數: output與期望數的差距平方 (就是懶得算sqrt) 標準差: 給變異數上sqrt conditional probability 就是重新取scope，在Y中X的機率，所以把分母改成Y，分子改成X\u0026Y P(X | Y) = P(X \u0026 Y) / P(Y)\n配合獨立，P(X | Y) = P(X)\n貝氏: P(X | Y) = P(Y | X) * (P(X) / P(Y)) X與Y的and可以透過條件機率換出來，P(X | Y) * P(Y) = P(X \u0026 Y) = P(Y | X) * P(X)\nConditional Independence 就是在這個scope中兩RV是獨立的，但沒這個scope，它們不一定獨立 P(X \u0026 Y | Z) = P(X | Z) * P(Y | Z)\nConditional Independence — The Backbone of Bayesian Networks\n新Random variable的推導 Poisson Distribution 詳細推導可以看這邊Poisson Distribution — Intuition, Examples, and Derivation\n如果想知道\ntry n次 時 某事件 成功/失敗 的機率 可以用二項式分布，但是\n我們要知道次數 只能看 成功/失敗 的機率 (可以把成功當成事件發生的機率) 然而更多時候我們只知道某一段時間中的事件發生的平均值而已 所以需要把二項式分布改造一下\n把事件發生的機率還原成事件發生的平均值 / 總次數 接著把n拉到無限大，當成做很多次實驗\n之後處理前面那一坨 所以Poisson Distribution就是n拉到無限大的結果，這樣也讓他有一些使用上的假設\n每個單位時間的事件發生的機率都是一樣的 事件是獨立的 Exponential Distribution 詳細推導可以看這邊Exponential Distribution — Intuition, Derivation, and Applications\n現在想要看從一個事件發生後，要多久才會有下一個事件\n如果沿用Poisson Distribution就是看分布之間的機率\nhow? 事件發生後，要多久才會有下一個事件，也就是沒事件的時間有多久，所以把Poisson Distribution帶0 但是Poisson Distribution看的是每個單位時間，但我們需要的是一段時間 一次不夠，可以乘好幾次(獨立)，e^(-λt)\n這樣是在t後開始出現下一次事件的機率，in math P(T \u003e t) = e^(-λt) 注意到這裡的時間默默變成RV了\n之後就是用機率的終極定理，機率加總等於1 只要用1去減，就可以得出cdf，之後微分就是pdf\nmemoryless P(T \u003e a + b | T \u003e a) = P(T \u003e b)\nPoisson Process process其實就是RV多加一個time的參數，所以公式與exp dist很像，但是可以指定某時間區段中發生k次 怎麼推導新的Random variable 詳細推導可以看這邊Sum of Exponential Random Variables\n利用獨立與Marginal distribution (就像是對其中一個RV做微分)\n下面是兩個exp dist相加的cdf 之後就可以求pdf 這就是Erlang distribution\nqueuing model 怎麼叫queue queue長這樣 有這些元素 (會給的與要算的) n是數量，分成在queue與在server w是waiting time s是service time r是residual time (sojourn time, response time)，在整個系統中的逗留時間 λ(速度) µ是service的rate(速度)\n把queue的特性寫下來 接下去? 做下面3件事\n對每個部位用不同的Random variable去描述 把queue的東西換一換 組合1與2 之後看怎麼把他們組合在一起\n通常有兩個觀察者\nglobal (time) user Random variable有自己的特性，每個queue model也有自己的特性 不要搞混，不論何時都要想到現在探討的東西與queue之間的意義\nclassic queue model 適用於所有queue的law G/G/m Stability Condition: λ \u003c m*µ Little’s law: n_queue = λ * 平均waiting_time n = λ * 平均response_time m/m/1 Poisson arrivals (λ) Exponential service times (µ) 1 server 無限buffer FIFO 在這個時候system有多少人 從N(t) → N(t + ∆t)會有3種case\n1 arrival λ∆t 1 service µ∆t no arrival, no service 1 − (λ + µ)∆t in math Steady-state analysis: 如果把t設成無限大(微掉t)? 把Chapman Kolmogorov equation調成微掉t 解偏微分 (列遞迴加總等於1) stable λ \u003c m*µ m帶1\n所以λ/µ(這邊叫ρ, traffic intensity or load)要小於1 一些元素 Markov Chains Discrete-time Markov Chains Markov Chains的重點是只依據目前的狀態轉到下一個狀態\n寫成matrix 在這個時候system有多少人 兩個版本，一個算式版，一個matrix版 Steady-state analysis : 把n去掉 先是算式版 把上螢光的部分丟到左邊去 另一個matrix版 總input == 總output ergodicity: a unique positive steady-state distribution ergodicity 中文叫遍歷性，不管系統的初始狀態如何，在經歷了一段時間以後，系統就會處於統計平衡狀態\n有三個條件\nirreducible: From any state, it is possible to get to any state\n整個圖是個SC，不管從哪邊開始，都可以走到graph的任意一點 沒有的話 lose positivity, and possibly uniqueness, of the steady-state distribution positive recurrent: The expected return time to any state is finite\n應該叫deterministic graoh的node的量是有限的 沒有負環 沒有的話 這樣有些state無法到達穩定 (就是unstable) Aperiodicity: There is no T ≥ 2 such that paths from a state to itself can only take multiples of T steps\n可以把no T ≥ 2換成T \u003c 2，也就是最多一步 從自己到自己最多只有1步 如果有period在，這樣在算這一點的機率時 在自己的點，可以產生無限多條從自己開始的path，這樣就不用算了 沒有的話 這樣在算state時會看到 P_ii * P_ii的詭異畫面 Continuous-time Markov Chains 定義 大概長這樣 同樣的事件轉移機率，同樣的從system中i人，變成j人 不同的是現在是看arrival與service done(departure)誰先發生 寫成matrix 這裡定義了新東西: Transition rates 為了符合當初的機率加總等於1，所以把自己到自己設成-λ 在這個時候，system在什麼狀態 Steady-state analysis : 把t去掉 總input == 總output 特化的例子: Birth-death processes 只有兩個方向，往前或往後 Local balance == global balance equations Special matrix: Triangular system Stationary distribution 例子: M/M/1 queue with balking 看到有人可能client就直接走人 Stationary distribution 例子: M/M/1/K Stationary distribution Loss probability \u0026 Blocking probability \u0026 Ergodic Blocking probability: 是在t時刻整個system已經滿了 剛好等於整個system處於滿的時間總和對上觀察區間的比值 Loss probability: 是在client到system時整個system已經滿了 之後有一個特性是描述這兩個可以是等於的!!\nErgodic theorem 遍歷性 表現出來會是: 從時間上觀察，與從user的角度觀察，結果是一樣的 時間均值等於空間均值 例如要得出一個城市A、B兩座公園哪一個更受歡迎，有兩種辦法 第一種辦法是在一定的時間段考察兩個公園的人數，人數多的為更受歡迎公園 第二種辦法，隨機選擇一名市民，跟蹤足夠長的時間來統計他去兩個公園的次數，去得多的為更受歡迎公園 因為現在client是Poisson process，所以可以用Poisson Arrivals See Time Averages，其實就是遍歷性 這樣就可以用前面的Stationary distribution直接求Loss probability Multi-server systems M/M/C/C: Pure-Loss System 長這樣 user不會retry\n狀態圖 同時開C台server，所以service的RV是Exp(k*µ)，k是看現在有多少台正在跑\nSteady-State Distribution Blocking Probability a.k.a Erlang-B Steady-State Distribution帶C 與 PASTA\nM/M/C/∞: Waiting System 長這樣 現在有queue了!! Stability: ρ \u003c C\n狀態圖 Steady-State Distribution waiting probability a.k.a Erlang-C Steady-State Distribution帶C 與 PASTA 一些元素 Steady-State Distribution 與 little law導平均有多少人在system 之後，把service time與waiting time相加就是residual time 注意這裡的service time只有一台server multi-cpu 與 multi-core Ref Queuing Theory: from Markov Chains to Multi-Server Systems\n","wordCount":"596","inLanguage":"en","image":"https://littlebees.github.io/images/papermod-cover.png","datePublished":"2022-01-17T10:26:57Z","dateModified":"2022-01-17T10:26:57Z","author":{"@type":"Person","name":"zhengcf"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://littlebees.github.io/2022/01/queuing-thoery%E7%9A%84tutorial/"},"publisher":{"@type":"Organization","name":"記事本","logo":{"@type":"ImageObject","url":"https://littlebees.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://littlebees.github.io/ accesskey=h title="記事本 (Alt + H)">記事本</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://littlebees.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://littlebees.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://littlebees.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://littlebees.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://littlebees.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">queuing thoery的tutorial</h1><div class=post-meta><span title='2022-01-17 10:26:57 +0000 UTC'>January 17, 2022</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;zhengcf</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%e5%8b%95%e6%a9%9f aria-label=動機>動機</a></li><li><a href=#%e4%b8%80%e8%a8%80%e4%bb%a5%e8%94%bd%e4%b9%8b%e9%80%99%e5%88%b0%e5%ba%95%e5%9c%a8%e5%b9%b9%e5%98%9b aria-label=一言以蔽之，這到底在幹嘛>一言以蔽之，這到底在幹嘛</a></li><li><a href=#%e8%a4%87%e7%bf%92%e6%a9%9f%e7%8e%87 aria-label=複習機率>複習機率</a><ul><li><a href=#random-variable%e5%88%b0%e5%ba%95%e6%98%af%e4%bb%80%e9%ba%bc aria-label="Random variable到底是什麼">Random variable到底是什麼</a></li><li><a href=#%e9%80%a3%e7%ba%8c%e8%88%87%e9%9b%a2%e6%95%a3 aria-label=連續與離散>連續與離散</a></li><li><a href=#%e6%9c%9f%e6%9c%9b%e6%95%b8--%e8%ae%8a%e7%95%b0%e6%95%b8 aria-label="期望數 & 變異數">期望數 & 變異數</a></li><li><a href=#conditional-probability aria-label="conditional probability">conditional probability</a><ul><li><a href=#conditional-independence aria-label="Conditional Independence">Conditional Independence</a></li></ul></li></ul></li><li><a href=#%e6%96%b0random-variable%e7%9a%84%e6%8e%a8%e5%b0%8e aria-label="新Random variable的推導">新Random variable的推導</a><ul><li><a href=#poisson-distribution aria-label="Poisson Distribution">Poisson Distribution</a></li><li><a href=#exponential-distribution aria-label="Exponential Distribution">Exponential Distribution</a><ul><li><a href=#memoryless aria-label=memoryless>memoryless</a></li><li><a href=#poisson-process aria-label="Poisson Process">Poisson Process</a></li></ul></li><li><a href=#%e6%80%8e%e9%ba%bc%e6%8e%a8%e5%b0%8e%e6%96%b0%e7%9a%84random-variable aria-label="怎麼推導新的Random variable">怎麼推導新的Random variable</a></li></ul></li><li><a href=#queuing-model aria-label="queuing model">queuing model</a><ul><li><a href=#%e6%80%8e%e9%ba%bc%e5%8f%abqueue aria-label=怎麼叫queue>怎麼叫queue</a></li><li><a href=#%e6%8e%a5%e4%b8%8b%e5%8e%bb aria-label=接下去?>接下去?</a></li></ul></li><li><a href=#classic-queue-model aria-label="classic queue model">classic queue model</a><ul><li><a href=#%e9%81%a9%e7%94%a8%e6%96%bc%e6%89%80%e6%9c%89queue%e7%9a%84law-ggm aria-label="適用於所有queue的law G/G/m">適用於所有queue的law G/G/m</a></li><li><a href=#mm1 aria-label=m/m/1>m/m/1</a><ul><li><a href=#%e5%9c%a8%e9%80%99%e5%80%8b%e6%99%82%e5%80%99system%e6%9c%89%e5%a4%9a%e5%b0%91%e4%ba%ba aria-label=在這個時候system有多少人>在這個時候system有多少人</a></li><li><a href=#steady-state-analysis-%e5%a6%82%e6%9e%9c%e6%8a%8at%e8%a8%ad%e6%88%90%e7%84%a1%e9%99%90%e5%a4%a7%e5%be%ae%e6%8e%89t aria-label="Steady-state analysis: 如果把t設成無限大(微掉t)?">Steady-state analysis: 如果把t設成無限大(微掉t)?</a><ul><li><a href=#%e6%8a%8achapman-kolmogorov-equation%e8%aa%bf%e6%88%90%e5%be%ae%e6%8e%89t aria-label="把Chapman Kolmogorov equation調成微掉t">把Chapman Kolmogorov equation調成微掉t</a></li><li><a href=#%e8%a7%a3%e5%81%8f%e5%be%ae%e5%88%86-%e5%88%97%e9%81%9e%e8%bf%b4%e5%8a%a0%e7%b8%bd%e7%ad%89%e6%96%bc1 aria-label="解偏微分 (列遞迴加總等於1)">解偏微分 (列遞迴加總等於1)</a></li></ul></li><li><a href=#stable aria-label=stable>stable</a></li><li><a href=#%e4%b8%80%e4%ba%9b%e5%85%83%e7%b4%a0 aria-label=一些元素>一些元素</a></li></ul></li><li><a href=#markov-chains aria-label="Markov Chains">Markov Chains</a><ul><li><a href=#discrete-time-markov-chains aria-label="Discrete-time Markov Chains">Discrete-time Markov Chains</a><ul><li><a href=#%e5%af%ab%e6%88%90matrix aria-label=寫成matrix>寫成matrix</a></li><li><a href=#%e5%9c%a8%e9%80%99%e5%80%8b%e6%99%82%e5%80%99system%e6%9c%89%e5%a4%9a%e5%b0%91%e4%ba%ba-1 aria-label=在這個時候system有多少人>在這個時候system有多少人</a></li><li><a href=#steady-state-analysis--%e6%8a%8an%e5%8e%bb%e6%8e%89 aria-label="Steady-state analysis : 把n去掉">Steady-state analysis : 把n去掉</a><ul><li><a href=#%e7%b8%bdinput--%e7%b8%bdoutput aria-label="總input == 總output">總input == 總output</a></li></ul></li><li><a href=#ergodicity-a-unique-positive-steady-state-distribution aria-label="ergodicity: a unique positive steady-state distribution">ergodicity: a unique positive steady-state distribution</a></li></ul></li><li><a href=#continuous-time-markov-chains aria-label="Continuous-time Markov Chains">Continuous-time Markov Chains</a></li><li><a href=#%e5%af%ab%e6%88%90matrix-1 aria-label=寫成matrix>寫成matrix</a><ul><li><a href=#%e5%9c%a8%e9%80%99%e5%80%8b%e6%99%82%e5%80%99system%e5%9c%a8%e4%bb%80%e9%ba%bc%e7%8b%80%e6%85%8b aria-label=在這個時候，system在什麼狀態>在這個時候，system在什麼狀態</a></li><li><a href=#steady-state-analysis--%e6%8a%8at%e5%8e%bb%e6%8e%89 aria-label="Steady-state analysis : 把t去掉">Steady-state analysis : 把t去掉</a><ul><li><a href=#%e7%b8%bdinput--%e7%b8%bdoutput-1 aria-label="總input == 總output">總input == 總output</a></li></ul></li></ul></li><li><a href=#%e7%89%b9%e5%8c%96%e7%9a%84%e4%be%8b%e5%ad%90-birth-death-processes aria-label="特化的例子: Birth-death processes">特化的例子: Birth-death processes</a><ul><li><a href=#local-balance--global-balance-equations aria-label="Local balance == global balance equations">Local balance == global balance equations</a></li><li><a href=#special-matrix-triangular-system aria-label="Special matrix: Triangular system">Special matrix: Triangular system</a></li><li><a href=#stationary-distribution aria-label="Stationary distribution">Stationary distribution</a></li><li><a href=#%e4%be%8b%e5%ad%90-mm1-queue-with-balking aria-label="例子: M/M/1 queue with balking">例子: M/M/1 queue with balking</a><ul><li><a href=#stationary-distribution-1 aria-label="Stationary distribution">Stationary distribution</a></li></ul></li><li><a href=#%e4%be%8b%e5%ad%90-mm1k aria-label="例子: M/M/1/K">例子: M/M/1/K</a><ul><li><a href=#stationary-distribution-2 aria-label="Stationary distribution">Stationary distribution</a></li><li><a href=#loss-probability--blocking-probability--ergodic aria-label="Loss probability & Blocking probability & Ergodic">Loss probability & Blocking probability & Ergodic</a></li></ul></li></ul></li></ul></li><li><a href=#multi-server-systems aria-label="Multi-server systems">Multi-server systems</a><ul><li><a href=#mmcc-pure-loss-system aria-label="M/M/C/C: Pure-Loss System">M/M/C/C: Pure-Loss System</a><ul><li><a href=#steady-state-distribution aria-label="Steady-State Distribution">Steady-State Distribution</a></li><li><a href=#blocking-probability-aka-erlang-b aria-label="Blocking Probability a.k.a Erlang-B">Blocking Probability a.k.a Erlang-B</a></li></ul></li><li><a href=#mmc-waiting-system aria-label="M/M/C/∞: Waiting System">M/M/C/∞: Waiting System</a><ul><li><a href=#steady-state-distribution-1 aria-label="Steady-State Distribution">Steady-State Distribution</a></li><li><a href=#waiting-probability-aka-erlang-c aria-label="waiting probability a.k.a Erlang-C">waiting probability a.k.a Erlang-C</a></li><li><a href=#%e4%b8%80%e4%ba%9b%e5%85%83%e7%b4%a0-1 aria-label=一些元素>一些元素</a></li><li><a href=#multi-cpu-%e8%88%87-multi-core aria-label="multi-cpu 與 multi-core">multi-cpu 與 multi-core</a></li></ul></li></ul></li></ul></li><li><a href=#ref aria-label=Ref>Ref</a></li></ul></div></details></div><div class=post-content><h2 id=動機>動機<a hidden class=anchor aria-hidden=true href=#動機>#</a></h2><p>補完當年的queuing thoery</p><h2 id=一言以蔽之這到底在幹嘛>一言以蔽之，這到底在幹嘛<a hidden class=anchor aria-hidden=true href=#一言以蔽之這到底在幹嘛>#</a></h2><ol><li>替queue建模</li><li>用random variable說話(設計每個queue的部分)<ul><li>其實queuing thoery應該要另外開一堂專門介紹怎麼操作random variable才對</li></ul></li></ol><h2 id=複習機率>複習機率<a hidden class=anchor aria-hidden=true href=#複習機率>#</a></h2><h3 id=random-variable到底是什麼>Random variable到底是什麼<a hidden class=anchor aria-hidden=true href=#random-variable到底是什麼>#</a></h3><p>高級的random，可以生出對應的output。
可以把Random variable當成疊加態，等到去用時才會確定一個值，所以可以對這個RV算機率、期望值等等性質</p><h3 id=連續與離散>連續與離散<a hidden class=anchor aria-hidden=true href=#連續與離散>#</a></h3><p>連續與離散就是看這個高級的random丟出來的值的range是可數還是不可數</p><p>同時連續與離散的RV也有不同的術語</p><ul><li>連續<ul><li>Cumulative Distribution Function<ul><li>pdf的積分，可以得到小於等於某個值的機率</li></ul></li><li>Probability density function<ul><li>cdf的微分，就是圖上的那條線 (不是機率!!)<ul><li>因為pdf的output可能超過1<ul><li><a href=https://towardsdatascience.com/pdf-is-not-a-probability-5a4b8a5d9531>PDF is not a probability.</a></li></ul></li></ul></li></ul></li></ul></li><li>離散<ul><li>probability mass function<ul><li>就是機率</li></ul></li><li>Cumulative Distribution Function<ul><li>pmf的總和，可以得到小於等於某個值的機率</li></ul></li></ul></li></ul><h3 id=期望數--變異數>期望數 & 變異數<a hidden class=anchor aria-hidden=true href=#期望數--變異數>#</a></h3><ul><li>期望數: output與pdf/pmf的加權總合<ul><li><code>E(aY+bX) = aE(Y) + bE(X)</code></li><li><code>E(g(X)) = sum(g(x)*pmf_or_pdf(x))</code><ul><li>這個很重要，之後會對X上一堆函數，需要這個才知道發生什麼事</li></ul></li><li>如果有獨立的話，<code>E(X & Y) = E(X) * E(Y)</code><ul><li>這個也很重要</li></ul></li></ul></li><li>變異數: output與期望數的差距平方 (就是懶得算sqrt)<ul><li>標準差: 給變異數上sqrt</li></ul></li></ul><h3 id=conditional-probability>conditional probability<a hidden class=anchor aria-hidden=true href=#conditional-probability>#</a></h3><p>就是重新取scope，在Y中X的機率，所以把分母改成Y，分子改成X&amp;Y
<code>P(X | Y) = P(X & Y) / P(Y)</code></p><p>配合獨立，<code>P(X | Y) = P(X)</code></p><p>貝氏: <code>P(X | Y) = P(Y | X) * (P(X) / P(Y))</code>
X與Y的and可以透過條件機率換出來，<code>P(X | Y) * P(Y) = P(X & Y) = P(Y | X) * P(X)</code></p><h4 id=conditional-independence>Conditional Independence<a hidden class=anchor aria-hidden=true href=#conditional-independence>#</a></h4><p>就是在這個scope中兩RV是獨立的，但沒這個scope，它們不一定獨立
<code>P(X & Y | Z) = P(X | Z) * P(Y | Z)</code></p><p><a href=https://towardsdatascience.com/conditional-independence-the-backbone-of-bayesian-networks-85710f1b35b>Conditional Independence — The Backbone of Bayesian Networks</a></p><h2 id=新random-variable的推導>新Random variable的推導<a hidden class=anchor aria-hidden=true href=#新random-variable的推導>#</a></h2><h3 id=poisson-distribution>Poisson Distribution<a hidden class=anchor aria-hidden=true href=#poisson-distribution>#</a></h3><p>詳細推導可以看這邊<a href=https://towardsdatascience.com/sum-of-exponential-random-variables-b023b61f0c0f#9437>Poisson Distribution — Intuition, Examples, and Derivation</a></p><p>如果想知道</p><ul><li>try n次 時</li><li>某事件 成功/失敗 的機率</li></ul><p>可以用二項式分布，但是</p><ul><li>我們要知道<strong>次數</strong></li><li>只能看 <strong>成功/失敗</strong> 的機率 (可以把成功當成事件發生的機率)</li></ul><p>然而更多時候我們只知道某一段時間中的事件發生的平均值而已
所以需要把二項式分布改造一下</p><p>把事件發生的機率還原成<code>事件發生的平均值 / 總次數</code>
接著把n拉到無限大，當成做很多次實驗</p><p><img loading=lazy src=https://i.imgur.com/SDLCZ1m.png alt></p><p>之後處理前面那一坨
<img loading=lazy src=https://i.imgur.com/xD7fS0B.png alt></p><p>所以Poisson Distribution就是n拉到無限大的結果，這樣也讓他有一些使用上的假設</p><ol><li>每個單位時間的事件發生的機率都是一樣的</li><li>事件是獨立的</li></ol><h3 id=exponential-distribution>Exponential Distribution<a hidden class=anchor aria-hidden=true href=#exponential-distribution>#</a></h3><p>詳細推導可以看這邊<a href=https://towardsdatascience.com/what-is-exponential-distribution-7bdd08590e2a>Exponential Distribution — Intuition, Derivation, and Applications</a></p><p>現在想要看從一個事件發生後，要多久才會有下一個事件</p><p>如果沿用Poisson Distribution就是看分布之間的機率</p><p>how?
事件發生後，要多久才會有下一個事件，也就是沒事件的時間有多久，所以把Poisson Distribution帶0
<img loading=lazy src=https://i.imgur.com/BfuYw5b.png alt></p><p>但是Poisson Distribution看的是<strong>每個單位時間</strong>，但我們需要的是一段時間
一次不夠，可以乘好幾次(獨立)，<code>e^(-λt)</code></p><p>這樣是在t後開始出現下一次事件的機率，in math
<code>P(T > t) = e^(-λt)</code>
注意到這裡的時間默默變成RV了</p><p>之後就是用機率的終極定理，機率加總等於1
只要用1去減，就可以得出cdf，之後微分就是pdf</p><h4 id=memoryless>memoryless<a hidden class=anchor aria-hidden=true href=#memoryless>#</a></h4><p><code>P(T > a + b | T > a) = P(T > b)</code></p><p><img loading=lazy src=https://i.imgur.com/cZQo7zF.png alt>
<img loading=lazy src=https://i.imgur.com/7OwbevO.png alt></p><h4 id=poisson-process>Poisson Process<a hidden class=anchor aria-hidden=true href=#poisson-process>#</a></h4><p>process其實就是RV多加一個time的參數，所以公式與exp dist很像，但是可以指定某時間區段中發生k次
<img loading=lazy src=https://i.imgur.com/whbN34L.png alt></p><h3 id=怎麼推導新的random-variable>怎麼推導新的Random variable<a hidden class=anchor aria-hidden=true href=#怎麼推導新的random-variable>#</a></h3><p>詳細推導可以看這邊<a href=https://towardsdatascience.com/sum-of-exponential-random-variables-b023b61f0c0f#9437>Sum of Exponential Random Variables</a></p><p>利用獨立與<a href=https://en.wikipedia.org/wiki/Marginal_distribution>Marginal distribution</a> (就像是對其中一個RV做微分)</p><p>下面是兩個exp dist相加的cdf
<img loading=lazy src=https://i.imgur.com/bghMRA6.png alt></p><p>之後就可以求pdf
<img loading=lazy src=https://i.imgur.com/5jqZz2A.png alt></p><p>這就是Erlang distribution</p><h2 id=queuing-model>queuing model<a hidden class=anchor aria-hidden=true href=#queuing-model>#</a></h2><h3 id=怎麼叫queue>怎麼叫queue<a hidden class=anchor aria-hidden=true href=#怎麼叫queue>#</a></h3><p>queue長這樣
<img loading=lazy src=https://i.imgur.com/8rnbY7i.png alt></p><p>有這些元素 (會給的與要算的)
<img loading=lazy src=https://i.imgur.com/8Ng0PYs.png alt></p><p>n是數量，分成在queue與在server
w是waiting time
s是service time
r是residual time (sojourn time, response time)，在整個系統中的逗留時間
λ(速度)
µ是service的rate(速度)</p><p>把queue的特性寫下來
<img loading=lazy src=https://i.imgur.com/c5ombC1.png alt></p><h3 id=接下去>接下去?<a hidden class=anchor aria-hidden=true href=#接下去>#</a></h3><p>做下面3件事</p><ol><li>對每個部位用不同的Random variable去描述</li><li>把queue的東西換一換</li><li>組合1與2</li></ol><p>之後看怎麼把他們組合在一起</p><p>通常有兩個觀察者</p><ol><li>global (time)</li><li>user</li></ol><p>Random variable有自己的特性，每個queue model也有自己的特性
不要搞混，不論何時都要想到現在探討的東西與queue之間的意義</p><h2 id=classic-queue-model>classic queue model<a hidden class=anchor aria-hidden=true href=#classic-queue-model>#</a></h2><h3 id=適用於所有queue的law-ggm>適用於所有queue的law G/G/m<a hidden class=anchor aria-hidden=true href=#適用於所有queue的law-ggm>#</a></h3><ul><li>Stability Condition: <code>λ &lt; m*µ</code></li><li>Little&rsquo;s law: <code>n_queue = λ * 平均waiting_time</code><ul><li><code>n = λ * 平均response_time</code></li></ul></li></ul><h3 id=mm1>m/m/1<a hidden class=anchor aria-hidden=true href=#mm1>#</a></h3><p><img loading=lazy src=https://i.imgur.com/IavQDU8.png alt></p><ul><li>Poisson arrivals (λ)</li><li>Exponential service times (µ)</li><li>1 server</li><li>無限buffer</li><li>FIFO</li></ul><h4 id=在這個時候system有多少人>在這個時候system有多少人<a hidden class=anchor aria-hidden=true href=#在這個時候system有多少人>#</a></h4><p>從N(t) → N(t + ∆t)會有3種case</p><ul><li>1 arrival<ul><li>λ∆t</li></ul></li><li>1 service<ul><li>µ∆t</li></ul></li><li>no arrival, no service<ul><li>1 − (λ + µ)∆t</li></ul></li></ul><p>in math
<img loading=lazy src=https://i.imgur.com/2ZstiOK.png alt></p><p><img loading=lazy src=https://i.imgur.com/nmeBryY.png alt>
<img loading=lazy src=https://i.imgur.com/TAH0K5k.png alt>
<img loading=lazy src=https://i.imgur.com/U49PiuJ.png alt>
<img loading=lazy src=https://i.imgur.com/j6VI6bM.png alt>
<img loading=lazy src=https://i.imgur.com/bqAANwH.png alt>
<img loading=lazy src=https://i.imgur.com/XcH5Th8.png alt></p><h4 id=steady-state-analysis-如果把t設成無限大微掉t>Steady-state analysis: 如果把t設成無限大(微掉t)?<a hidden class=anchor aria-hidden=true href=#steady-state-analysis-如果把t設成無限大微掉t>#</a></h4><p><img loading=lazy src=https://i.imgur.com/v5uz5kc.png alt></p><h5 id=把chapman-kolmogorov-equation調成微掉t>把Chapman Kolmogorov equation調成微掉t<a hidden class=anchor aria-hidden=true href=#把chapman-kolmogorov-equation調成微掉t>#</a></h5><p><img loading=lazy src=https://i.imgur.com/E1BZuom.png alt>
<img loading=lazy src=https://i.imgur.com/fbBz5Vn.png alt></p><h5 id=解偏微分-列遞迴加總等於1>解偏微分 (列遞迴加總等於1)<a hidden class=anchor aria-hidden=true href=#解偏微分-列遞迴加總等於1>#</a></h5><p><img loading=lazy src=https://i.imgur.com/ryJVgD8.png alt>
<img loading=lazy src=https://i.imgur.com/vrMtcFU.png alt></p><h4 id=stable>stable<a hidden class=anchor aria-hidden=true href=#stable>#</a></h4><p><code>λ &lt; m*µ</code> m帶1</p><p>所以λ/µ(這邊叫ρ, traffic intensity or load)要小於1
<img loading=lazy src=https://i.imgur.com/exFq9V5.png alt></p><h4 id=一些元素>一些元素<a hidden class=anchor aria-hidden=true href=#一些元素>#</a></h4><p><img loading=lazy src=https://i.imgur.com/zwwvwHH.png alt>
<img loading=lazy src=https://i.imgur.com/w7QXjaY.png alt>
<img loading=lazy src=https://i.imgur.com/FpEixN0.png alt>
<img loading=lazy src=https://i.imgur.com/nLbTHEW.png alt></p><h3 id=markov-chains>Markov Chains<a hidden class=anchor aria-hidden=true href=#markov-chains>#</a></h3><h4 id=discrete-time-markov-chains>Discrete-time Markov Chains<a hidden class=anchor aria-hidden=true href=#discrete-time-markov-chains>#</a></h4><p>Markov Chains的重點是只依據目前的狀態轉到下一個狀態</p><p><img loading=lazy src=https://i.imgur.com/kFAdHNQ.png alt></p><h5 id=寫成matrix>寫成matrix<a hidden class=anchor aria-hidden=true href=#寫成matrix>#</a></h5><p><img loading=lazy src=https://i.imgur.com/jCP7mPI.png alt></p><h5 id=在這個時候system有多少人-1>在這個時候system有多少人<a hidden class=anchor aria-hidden=true href=#在這個時候system有多少人-1>#</a></h5><p>兩個版本，一個算式版，一個matrix版
<img loading=lazy src=https://i.imgur.com/qvjOLlj.png alt>
<img loading=lazy src=https://i.imgur.com/oRDRRss.png alt></p><h5 id=steady-state-analysis--把n去掉>Steady-state analysis : 把n去掉<a hidden class=anchor aria-hidden=true href=#steady-state-analysis--把n去掉>#</a></h5><p>先是算式版
<img loading=lazy src=https://i.imgur.com/T2D0Wuy.png alt>
把上螢光的部分丟到左邊去
<img loading=lazy src=https://i.imgur.com/lBeGP2t.png alt></p><p>另一個matrix版
<img loading=lazy src=https://i.imgur.com/ZIkttxl.png alt></p><h6 id=總input--總output>總input == 總output<a hidden class=anchor aria-hidden=true href=#總input--總output>#</a></h6><p><img loading=lazy src=https://i.imgur.com/vTsbF2G.png alt>
<img loading=lazy src=https://i.imgur.com/7PqW6se.png alt></p><h5 id=ergodicity-a-unique-positive-steady-state-distribution>ergodicity: a unique positive steady-state distribution<a hidden class=anchor aria-hidden=true href=#ergodicity-a-unique-positive-steady-state-distribution>#</a></h5><p>ergodicity 中文叫遍歷性，不管系統的初始狀態如何，在經歷了一段時間以後，系統就會處於統計平衡狀態</p><p>有三個條件</p><ul><li><p>irreducible: From any state, it is possible to get to any state</p><ul><li>整個圖是個SC，不管從哪邊開始，都可以走到graph的任意一點</li><li><img loading=lazy src=https://i.imgur.com/SWiXiAH.png alt></li><li>沒有的話<ul><li>lose positivity, and possibly uniqueness, of the steady-state distribution</li></ul></li></ul></li><li><p>positive recurrent: The expected return time to any state is finite</p><ul><li>應該叫deterministic<ul><li>graoh的node的量是有限的</li><li>沒有負環</li></ul></li><li><img loading=lazy src=https://i.imgur.com/4hzK4hE.png alt></li><li>沒有的話<ul><li>這樣有些state無法到達穩定 (就是unstable)</li></ul></li></ul></li><li><p>Aperiodicity: There is no T ≥ 2 such that paths from a state to itself can only take multiples of T steps</p><ul><li>可以把no T ≥ 2換成T &lt; 2，也就是最多一步<ul><li>從自己到自己最多只有1步</li><li>如果有period在，這樣在算這一點的機率時<ul><li>在自己的點，可以產生無限多條從自己開始的path，這樣就不用算了</li></ul></li></ul></li><li><img loading=lazy src=https://i.imgur.com/LHp02u0.png alt></li><li>沒有的話<ul><li>這樣在算state時會看到<ul><li>P_ii * P_ii的詭異畫面</li></ul></li></ul></li></ul></li></ul><h4 id=continuous-time-markov-chains>Continuous-time Markov Chains<a hidden class=anchor aria-hidden=true href=#continuous-time-markov-chains>#</a></h4><p>定義
<img loading=lazy src=https://i.imgur.com/HQ5fhdl.png alt></p><p>大概長這樣
<img loading=lazy src=https://i.imgur.com/NjLKX3U.png alt></p><p>同樣的事件轉移機率，同樣的從system中i人，變成j人
不同的是現在是看arrival與service done(departure)誰先發生
<img loading=lazy src=https://i.imgur.com/gp1oeK5.png alt></p><h4 id=寫成matrix-1>寫成matrix<a hidden class=anchor aria-hidden=true href=#寫成matrix-1>#</a></h4><p>這裡定義了新東西: Transition rates
為了符合當初的機率加總等於1，所以把自己到自己設成-λ
<img loading=lazy src=https://i.imgur.com/9Nzs0FB.png alt>
<img loading=lazy src=https://i.imgur.com/4xrEw9J.png alt></p><h5 id=在這個時候system在什麼狀態>在這個時候，system在什麼狀態<a hidden class=anchor aria-hidden=true href=#在這個時候system在什麼狀態>#</a></h5><p><img loading=lazy src=https://i.imgur.com/SOF3Hfx.png alt></p><h5 id=steady-state-analysis--把t去掉>Steady-state analysis : 把t去掉<a hidden class=anchor aria-hidden=true href=#steady-state-analysis--把t去掉>#</a></h5><p><img loading=lazy src=https://i.imgur.com/XprStHI.png alt></p><h6 id=總input--總output-1>總input == 總output<a hidden class=anchor aria-hidden=true href=#總input--總output-1>#</a></h6><p><img loading=lazy src=https://i.imgur.com/zYYWHDF.png alt></p><h4 id=特化的例子-birth-death-processes>特化的例子: Birth-death processes<a hidden class=anchor aria-hidden=true href=#特化的例子-birth-death-processes>#</a></h4><p>只有兩個方向，往前或往後
<img loading=lazy src=https://i.imgur.com/fxa6yyb.png alt></p><h5 id=local-balance--global-balance-equations>Local balance == global balance equations<a hidden class=anchor aria-hidden=true href=#local-balance--global-balance-equations>#</a></h5><p><img loading=lazy src=https://i.imgur.com/70A2Vpg.png alt></p><p><img loading=lazy src=https://i.imgur.com/hroGEYU.png alt></p><h5 id=special-matrix-triangular-system>Special matrix: Triangular system<a hidden class=anchor aria-hidden=true href=#special-matrix-triangular-system>#</a></h5><p><img loading=lazy src=https://i.imgur.com/GHtbXce.png alt></p><h5 id=stationary-distribution>Stationary distribution<a hidden class=anchor aria-hidden=true href=#stationary-distribution>#</a></h5><p><img loading=lazy src=https://i.imgur.com/id00wqM.png alt></p><h5 id=例子-mm1-queue-with-balking>例子: M/M/1 queue with balking<a hidden class=anchor aria-hidden=true href=#例子-mm1-queue-with-balking>#</a></h5><p>看到有人可能client就直接走人
<img loading=lazy src=https://i.imgur.com/sFs0tqv.png alt></p><h6 id=stationary-distribution-1>Stationary distribution<a hidden class=anchor aria-hidden=true href=#stationary-distribution-1>#</a></h6><p><img loading=lazy src=https://i.imgur.com/ggae7HX.png alt></p><h5 id=例子-mm1k>例子: M/M/1/K<a hidden class=anchor aria-hidden=true href=#例子-mm1k>#</a></h5><p><img loading=lazy src=https://i.imgur.com/mCO8ru3.png alt>
<img loading=lazy src=https://i.imgur.com/vZ2dvzB.png alt></p><h6 id=stationary-distribution-2>Stationary distribution<a hidden class=anchor aria-hidden=true href=#stationary-distribution-2>#</a></h6><p><img loading=lazy src=https://i.imgur.com/eAPLVGf.png alt></p><h6 id=loss-probability--blocking-probability--ergodic>Loss probability & Blocking probability & Ergodic<a hidden class=anchor aria-hidden=true href=#loss-probability--blocking-probability--ergodic>#</a></h6><ul><li>Blocking probability: 是在<strong>t時刻</strong>整個system已經滿了<ul><li>剛好等於整個system處於滿的時間總和對上觀察區間的比值<ul><li><img loading=lazy src=https://i.imgur.com/JohXJLu.png alt></li></ul></li></ul></li><li>Loss probability: 是在<strong>client到</strong>system時整個system已經滿了</li></ul><p>之後有一個特性是描述這兩個可以是等於的!!</p><ul><li>Ergodic theorem<ul><li>遍歷性<ul><li>表現出來會是: 從時間上觀察，與從user的角度觀察，結果是一樣的<ul><li>時間均值等於空間均值</li><li>例如要得出一個城市A、B兩座公園哪一個更受歡迎，有兩種辦法<ul><li>第一種辦法是在一定的時間段考察兩個公園的人數，人數多的為更受歡迎公園</li><li>第二種辦法，隨機選擇一名市民，跟蹤足夠長的時間來統計他去兩個公園的次數，去得多的為更受歡迎公園</li></ul></li></ul></li></ul></li><li><img loading=lazy src=https://i.imgur.com/hxNaw1l.png alt></li></ul></li></ul><p>因為現在client是Poisson process，所以可以用Poisson Arrivals See Time Averages，其實就是遍歷性
<img loading=lazy src=https://i.imgur.com/f4v9p2V.png alt></p><p>這樣就可以用前面的Stationary distribution直接求Loss probability
<img loading=lazy src=https://i.imgur.com/oi3XleH.png alt></p><h3 id=multi-server-systems>Multi-server systems<a hidden class=anchor aria-hidden=true href=#multi-server-systems>#</a></h3><h4 id=mmcc-pure-loss-system>M/M/C/C: Pure-Loss System<a hidden class=anchor aria-hidden=true href=#mmcc-pure-loss-system>#</a></h4><p>長這樣
<img loading=lazy src=https://i.imgur.com/GkRjgn4.png alt></p><p>user不會retry</p><p>狀態圖
<img loading=lazy src=https://i.imgur.com/QVgBhOQ.png alt></p><p>同時開C台server，所以service的RV是Exp(k*µ)，k是看現在有多少台正在跑</p><h5 id=steady-state-distribution>Steady-State Distribution<a hidden class=anchor aria-hidden=true href=#steady-state-distribution>#</a></h5><p><img loading=lazy src=https://i.imgur.com/HKKOID5.png alt></p><h5 id=blocking-probability-aka-erlang-b>Blocking Probability a.k.a Erlang-B<a hidden class=anchor aria-hidden=true href=#blocking-probability-aka-erlang-b>#</a></h5><p>Steady-State Distribution帶C 與 PASTA</p><p><img loading=lazy src=https://i.imgur.com/gZTyVcr.png alt></p><h4 id=mmc-waiting-system>M/M/C/∞: Waiting System<a hidden class=anchor aria-hidden=true href=#mmc-waiting-system>#</a></h4><p>長這樣
<img loading=lazy src=https://i.imgur.com/QulH6Bi.png alt></p><p>現在有queue了!!
Stability: <code>ρ &lt; C</code></p><p>狀態圖
<img loading=lazy src=https://i.imgur.com/hcmGYuU.png alt></p><h5 id=steady-state-distribution-1>Steady-State Distribution<a hidden class=anchor aria-hidden=true href=#steady-state-distribution-1>#</a></h5><p><img loading=lazy src=https://i.imgur.com/YSAdpwo.png alt></p><h5 id=waiting-probability-aka-erlang-c>waiting probability a.k.a Erlang-C<a hidden class=anchor aria-hidden=true href=#waiting-probability-aka-erlang-c>#</a></h5><p>Steady-State Distribution帶C 與 PASTA
<img loading=lazy src=https://i.imgur.com/oK7Bwvw.png alt></p><h5 id=一些元素-1>一些元素<a hidden class=anchor aria-hidden=true href=#一些元素-1>#</a></h5><p>Steady-State Distribution 與 little law導平均有多少人在system
<img loading=lazy src=https://i.imgur.com/jywGzya.png alt></p><p>之後，把service time與waiting time相加就是residual time
注意這裡的service time只有一台server
<img loading=lazy src=https://i.imgur.com/0LO2H73.png alt></p><h5 id=multi-cpu-與-multi-core>multi-cpu 與 multi-core<a hidden class=anchor aria-hidden=true href=#multi-cpu-與-multi-core>#</a></h5><p><img loading=lazy src=https://i.imgur.com/mqXwtRP.png alt></p><h2 id=ref>Ref<a hidden class=anchor aria-hidden=true href=#ref>#</a></h2><p><a href=https://learning.edx.org/course/course-v1:IMTx+CS101+2T2021/home>Queuing Theory: from Markov Chains to Multi-Server Systems</a></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://littlebees.github.io/tags/performance/>Performance</a></li></ul><nav class=paginav><a class=prev href=https://littlebees.github.io/2022/01/%E5%B7%A8%E5%9E%8B%E6%9C%8D%E5%8B%99%E6%9E%B6%E6%A7%8B%E5%88%86%E5%B8%83%E5%BC%8F/%E8%B3%87%E6%96%99%E5%BA%AB%E5%84%AA%E5%8C%96/%E8%A8%98%E6%86%B6%E9%AB%94%E5%BF%AB%E5%8F%96%E8%A8%AD%E8%A8%88/io%E6%A8%A1%E5%9E%8B/><span class=title>« Prev</span><br><span>巨型服務架構：分布式/資料庫優化/記憶體快取設計/IO模型</span>
</a><a class=next href=https://littlebees.github.io/2021/11/z-algorithm/><span class=title>Next »</span><br><span>Z Algorithm</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://littlebees.github.io/>記事本</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>